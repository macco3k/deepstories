{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the followings:\n",
    "* http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "* http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/farizrahman4u/seq2seq\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "## TODO\n",
    "* ~~look into categorical representation~~\n",
    "* ~~look into the number of missing words over the total~~\n",
    "* look into different training data generators (e.g. simple sentence2sentence)\n",
    "* look into different models (attention, hierachical, etc.)\n",
    "* look into character-level representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "# import seq2seq\n",
    "\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "from nltk import corpus, stem\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Masking, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "OUTPUT_PATH = 'output'\n",
    "\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, w2v):\n",
    "    cont = Contractions(w2v_model=w2v)\n",
    "    return cont.expand_texts(text=text, precise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim = w2v.vector_size\n",
    "eos_vector = np.ones((vocab_dim))\n",
    "unk_vector = np.zeros((vocab_dim))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(repl=' ', string=text, pattern='-')\n",
    "    return re.sub(repl='', string=text, pattern='[{}\\n\\t\\\\\\\\]'.format(''.join(punctuation)))\n",
    "\n",
    "def build_vocabulary(file_list, w2v):\n",
    "    idx2word = set() # only count unique words\n",
    "    missing_words = set()\n",
    "    for file in file_list:\n",
    "        print('build_vocabulary: processing [{}]'.format(file))\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for i,line in enumerate(f):\n",
    "                line = preprocess(line)\n",
    "                if len(line) == 0:\n",
    "                    print('Line {} is empty. Skipping it.'.format(i+1))\n",
    "                    continue\n",
    "                \n",
    "                for word in line.split(' '):\n",
    "                    # skip words without embeddings. They'll be assigned the <UNK> token\n",
    "                    if len(word) > 0:\n",
    "                        if word in w2v:\n",
    "                            idx2word.add(word)\n",
    "                        else:\n",
    "                            missing_words.add(word)\n",
    "                        \n",
    "    missing_words = sorted(list(missing_words))\n",
    "    idx2word = sorted(list(idx2word))\n",
    "    idx2word.insert(0, '<EOS>')\n",
    "    idx2word.insert(1, '<UNK>')\n",
    "    word2idx = {w:i for i,w in enumerate(idx2word)}\n",
    "    # skip EOS and UNK when looking up word embeddings\n",
    "    word2embeddings = {**{'<EOS>': eos_vector, '<UNK>': unk_vector}, **{w:w2v[w] for w in idx2word[2:]}}\n",
    "    return idx2word, word2idx, word2embeddings, missing_words\n",
    "\n",
    "def prepare_data(file_list, word2idx):\n",
    "    vocab_size = len(word2idx)    \n",
    "    data = []\n",
    "    for file in file_list:\n",
    "        print('prepare_data: processing [{}]'.format(file))\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            file_data = []\n",
    "            for i,line in enumerate(f):\n",
    "                line = preprocess(line)\n",
    "                if len(line) == 0:\n",
    "                    print('Line {} is empty. Skipping it.'.format(i+1))\n",
    "                    continue\n",
    "                # return the integer representation of the sentence\n",
    "                file_data.append([word2idx[w] if w in word2idx else word2idx['<UNK>'] for w in line.split(' ')])\n",
    "        data.append(file_data)\n",
    "    return data\n",
    "                                 \n",
    "def get_embedding_matrix(word2embeddings):\n",
    "    embedding_dim = len(list(word2embeddings.values())[0])\n",
    "    embedding_matrix = np.zeros(shape=(len(word2embeddings), embedding_dim))\n",
    "    for i, w in enumerate(word2embeddings):\n",
    "        embedding_matrix[i] = word2embeddings[w]\n",
    "    return embedding_matrix\n",
    "                                \n",
    "def prepare_input(input_text, word2embeddings):\n",
    "    return [word2embeddings[word] if word in word2embeddings else unk_vector for word in preprocess(input_text).split(' ') if len(word) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocabulary: processing [data/parsed\\parsed-12heads.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-1893.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-20160221-thesueno-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-20160221-thesueno.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-20160327-unrealcity-lifeonmars.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-69krakatoa.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-905-shrapnel.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-abno.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-acg-crossbow.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-acitw.txt]\n",
      "Line 171 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-actofmurder.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-adverbum.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-afdfr.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-afflicted.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-allthingsdevours.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-aotearoa.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-awakening.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-beingandrewplotkin.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bellwater.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bestman.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-blindhouse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bonaventure.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bookvol.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-broadsides.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bryant.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-buddha.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cacophony.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cc-fangvclaw-flooby.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-chefjanitor.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-childsplay.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-chineseroom.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-clipperbeta.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cokeandspeed.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cove.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-crescent.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-csbb.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cull.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-death.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-defra.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-degeneracy.txt]\n",
      "Line 248 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-demoparty.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dialcforcupcakes-103014-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dialcforcupcakes-103014.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-divis.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-djinni.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dramaqueen.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dualtransform.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eas.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eas2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eatme.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-edifice.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-electric.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-elysium.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-envcomp.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-escapade.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eurydice.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-everybodydies.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-everybodylovesaparade.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fdb-tin-folkar.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fear.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fifteenminutes-100214.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-finalexam20160124.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-finetuned.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-firebird.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fish.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-floatpoint.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-foofoo.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-forachange.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-foth.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fragileshells.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ft-n-awe.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-galatea.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-gdc09.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-glowgrass.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-goldilocks.txt]\n",
      "Line 842 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-groovebillygoat.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ground.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-guesstheverb.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-halothane.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hamper.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-heroes.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hoosegow.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-houseofdreamofmoon.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hunterindarkness.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1701.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1702.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1703.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1704.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic2010-1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-indigo.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-inls.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp08a.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp11.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-invisargo.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jabberwocky.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jacket4.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jacqissick.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jfw.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ka.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-laidoff-1May2016.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-laidoff-subrosa-1May2016.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-lethe.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-littlebluemen.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-lmwh.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-loose.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-lostpig.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-luminous.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-maincourse-iamthelaw.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-marika.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-measure.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-metamorphoses.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-mingsheng.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-mite.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-monkfish.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-moonlittower.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-mugglestudies.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocabulary: processing [data/parsed\\parsed-newernewyear.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed-jan16a.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed-jan16b.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed-jan9.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed08.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-nightfall.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-nightfall2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-nordandbert.txt]\n",
      "Line 980 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-oad.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-oneeyeopen.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-onehalf.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-orevore.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-park.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-partyfoul.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pathway.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pax.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pax2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pax2011.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pepper.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-photograph.txr.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-photograph.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-plan6-waker.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-plunderedhearts.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pnnsi1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pnnsi2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-primrose-edited.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-progressive1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-punkpoints.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-rameses.txt]\n",
      "Line 98 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-recluse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-represso.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-revolution-buny.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-robot.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-rogue.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-roofed-alien.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-rover.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-samfortune.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-santaland.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-saugusnet-a.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-saugusnet-b.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-saugusnet-c.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-scaryhouseamulet.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-scavenger.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sequitur.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-shadowsonthemirror.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-shelter.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sherbet.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-simplethefts.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sinsagainstmimesis.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-six.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-smittenkittens.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-snacktime.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-softfood.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sorcerer.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-spring.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-spur.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ssi.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ssos.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-starborn.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-statue.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-stewie-escapade.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-stf.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-subrosa-1and8may2016.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-suspended.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-suvehnux.txt]\n",
      "Line 128 is empty. Skipping it.\n",
      "Line 152 is empty. Skipping it.\n",
      "Line 166 is empty. Skipping it.\n",
      "Line 212 is empty. Skipping it.\n",
      "Line 246 is empty. Skipping it.\n",
      "Line 275 is empty. Skipping it.\n",
      "Line 320 is empty. Skipping it.\n",
      "Line 398 is empty. Skipping it.\n",
      "Line 506 is empty. Skipping it.\n",
      "Line 542 is empty. Skipping it.\n",
      "Line 551 is empty. Skipping it.\n",
      "Line 645 is empty. Skipping it.\n",
      "Line 770 is empty. Skipping it.\n",
      "Line 939 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-swigian.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tacofiction.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tangle.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tangle2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tapestry.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tdmamoom.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thanksgiving.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-themultidimensionalthief.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-theone.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-theoracle.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-theplay.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thohc1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thohc2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thread.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tokyo-mouse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-toonesiabandit.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-transparent-100914.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tryst.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-turkeyspeeds.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-unclezeb.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-undertow.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-unipool.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-unscientific.txt]\n",
      "Line 48 is empty. Skipping it.\n",
      "Line 50 is empty. Skipping it.\n",
      "Line 97 is empty. Skipping it.\n",
      "Line 3161 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-vagueness.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-varkana.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-violet.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wand.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-weapon.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wedding.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-weishaupt.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-windjack.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-winterwonderland.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wishbringer.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wizard.txt]\n",
      "Line 366 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-wof-sa.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ww-jingo-madrigals.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-xyzzy2011.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-yakshaving.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-yetifail.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-zork-i-2016-04-0310.txt]\n",
      "Line 2013 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-zork1+troll-2016Ap0310.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-zorkII.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-12heads.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-1893.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-20160221-thesueno-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-20160221-thesueno.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-20160327-unrealcity-lifeonmars.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-69krakatoa.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-905-shrapnel.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-abno.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-acg-crossbow.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-acitw.txt]\n",
      "Line 171 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-actofmurder.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-adverbum.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-afdfr.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-afflicted.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-allthingsdevours.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-aotearoa.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed\\parsed-awakening.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-beingandrewplotkin.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bellwater.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bestman.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-blindhouse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bonaventure.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bookvol.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-broadsides.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bryant.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-buddha.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cacophony.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cc-fangvclaw-flooby.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-chefjanitor.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-childsplay.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-chineseroom.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-clipperbeta.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cokeandspeed.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cove.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-crescent.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-csbb.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cull.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-death.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-defra.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-degeneracy.txt]\n",
      "Line 248 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-demoparty.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dialcforcupcakes-103014-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dialcforcupcakes-103014.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-divis.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-djinni.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dramaqueen.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dualtransform.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eas.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eas2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eatme.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-edifice.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-electric.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-elysium.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-envcomp.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-escapade.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eurydice.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-everybodydies.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-everybodylovesaparade.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fdb-tin-folkar.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fear.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fifteenminutes-100214.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-finalexam20160124.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-finetuned.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-firebird.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fish.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-floatpoint.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-foofoo.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-forachange.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-foth.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fragileshells.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ft-n-awe.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-galatea.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-gdc09.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-glowgrass.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-goldilocks.txt]\n",
      "Line 842 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-groovebillygoat.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ground.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-guesstheverb.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-halothane.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hamper.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-heroes.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hoosegow.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-houseofdreamofmoon.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hunterindarkness.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1701.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1702.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1703.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1704.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic2010-1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-indigo.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-inls.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp08a.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp11.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-invisargo.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jabberwocky.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jacket4.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jacqissick.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jfw.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ka.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-laidoff-1May2016.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-laidoff-subrosa-1May2016.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-lethe.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-littlebluemen.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-lmwh.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-loose.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-lostpig.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-luminous.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-maincourse-iamthelaw.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-marika.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-measure.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-metamorphoses.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-mingsheng.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-mite.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-monkfish.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-moonlittower.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-mugglestudies.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newernewyear.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed-jan16a.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed-jan16b.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed-jan9.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed08.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-nightfall.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-nightfall2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-nordandbert.txt]\n",
      "Line 980 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-oad.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-oneeyeopen.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-onehalf.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-orevore.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-park.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-partyfoul.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pathway.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pax.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pax2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pax2011.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pepper.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-photograph.txr.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-photograph.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-plan6-waker.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-plunderedhearts.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pnnsi1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pnnsi2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-primrose-edited.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-progressive1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-punkpoints.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-rameses.txt]\n",
      "Line 98 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-recluse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-represso.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-revolution-buny.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-robot.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-rogue.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-roofed-alien.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-rover.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed\\parsed-samfortune.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-santaland.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-saugusnet-a.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-saugusnet-b.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-saugusnet-c.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-scaryhouseamulet.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-scavenger.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sequitur.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-shadowsonthemirror.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-shelter.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sherbet.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-simplethefts.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sinsagainstmimesis.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-six.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-smittenkittens.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-snacktime.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-softfood.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sorcerer.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-spring.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-spur.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ssi.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ssos.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-starborn.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-statue.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-stewie-escapade.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-stf.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-subrosa-1and8may2016.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-suspended.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-suvehnux.txt]\n",
      "Line 128 is empty. Skipping it.\n",
      "Line 152 is empty. Skipping it.\n",
      "Line 166 is empty. Skipping it.\n",
      "Line 212 is empty. Skipping it.\n",
      "Line 246 is empty. Skipping it.\n",
      "Line 275 is empty. Skipping it.\n",
      "Line 320 is empty. Skipping it.\n",
      "Line 398 is empty. Skipping it.\n",
      "Line 506 is empty. Skipping it.\n",
      "Line 542 is empty. Skipping it.\n",
      "Line 551 is empty. Skipping it.\n",
      "Line 645 is empty. Skipping it.\n",
      "Line 770 is empty. Skipping it.\n",
      "Line 939 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-swigian.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tacofiction.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tangle.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tangle2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tapestry.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tdmamoom.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thanksgiving.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-themultidimensionalthief.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-theone.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-theoracle.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-theplay.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thohc1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thohc2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thread.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tokyo-mouse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-toonesiabandit.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-transparent-100914.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tryst.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-turkeyspeeds.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-unclezeb.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-undertow.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-unipool.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-unscientific.txt]\n",
      "Line 48 is empty. Skipping it.\n",
      "Line 50 is empty. Skipping it.\n",
      "Line 97 is empty. Skipping it.\n",
      "Line 3161 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-vagueness.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-varkana.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-violet.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wand.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-weapon.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wedding.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-weishaupt.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-windjack.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-winterwonderland.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wishbringer.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wizard.txt]\n",
      "Line 366 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-wof-sa.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ww-jingo-madrigals.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-xyzzy2011.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-yakshaving.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-yetifail.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-zork-i-2016-04-0310.txt]\n",
      "Line 2013 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-zork1+troll-2016Ap0310.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob('data/parsed/*.txt')\n",
    "idx2word, word2idx, word2embeddings, missing_words = build_vocabulary(file_list, w2v)\n",
    "data = prepare_data(file_list, word2idx)\n",
    "vocab_size = len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words found (<UNK>, <EOS> + embeddings): 34160\n",
      "Missing words (no embeddings): 9120\n"
     ]
    }
   ],
   "source": [
    "print('Unique words found (<UNK>, <EOS> + embeddings):', len(word2idx))\n",
    "print('Missing words (no embeddings):', len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix(word2embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch generator\n",
    "class BatchGenerator(object):            \n",
    "    def __init__(self, data, batch_size=1):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.UNK = word2idx['<UNK>']\n",
    "        self.EOS = word2idx['<EOS>']\n",
    "        self.PAD = 0\n",
    "        self.eye = np.eye(len(word2idx))\n",
    "        \n",
    "    def generate_batch(self): \n",
    "        def window(seq, n=3, step=1):\n",
    "            \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "            \"   s -> (s[0],...s[n-1]), (s[0+skip_n],...,s[n-1+skip_n]), ...   \"\n",
    "            it = iter(seq)\n",
    "            result = tuple(islice(it, n))\n",
    "            if len(result) == n:\n",
    "                yield result    \n",
    "\n",
    "            result = result[step:]\n",
    "            for elem in it:\n",
    "                result = result + (elem,)\n",
    "                if len(result) == n:\n",
    "                    yield result\n",
    "                    result = result[step:]\n",
    "                    \n",
    "        def to_categorical(sentence):\n",
    "            return [self.eye[wordidx] for wordidx in sentence]\n",
    "                    \n",
    "        # every three lines comprise a sample sequence where the first two items\n",
    "        # are the input and the last one is the output\n",
    "        i  = 1 # batch counter        \n",
    "        x_enc = []\n",
    "        x_dec = []\n",
    "        y  = []\n",
    "        while True:\n",
    "            for play in self.data:\n",
    "                j  = 1 # sample counter\n",
    "                for scene, command, reply in window(play, n=3, step=2):\n",
    "                    scene_command = scene + command\n",
    "                    \n",
    "                    encoder_input  = np.array(scene_command + [self.EOS])\n",
    "                    decoder_input  = np.array(reply)\n",
    "                    decoder_output = np.array(to_categorical(reply[1:] + [self.EOS]))\n",
    "                    \n",
    "                    print(encoder_input.shape, decoder_input.shape, decoder_output.shape)\n",
    "                \n",
    "                    x_enc.append(encoder_input)\n",
    "                    x_dec.append(decoder_input)\n",
    "                    y.append(decoder_output)\n",
    "                    if i == self.batch_size or j == len(play):\n",
    "                        if self.batch_size > 1:\n",
    "                            # pad and return the batch\n",
    "                            x_enc = sequence.pad_sequences(x_enc, padding='post', value=self.PAD)\n",
    "                            x_dec = sequence.pad_sequences(x_dec, padding='post', value=self.PAD)\n",
    "                            y     = sequence.pad_sequences(y,     padding='post', value=self.PAD) \n",
    "\n",
    "                        x_out, y_out = [np.array(x_enc.copy()), np.array(x_dec.copy())], np.array(y.copy())\n",
    "\n",
    "                        i  = 1\n",
    "                        x_enc = []\n",
    "                        x_dec = []\n",
    "                        y  = []\n",
    "\n",
    "                        yield (x_out, y_out)\n",
    "                    else:\n",
    "                        i += 1 # next sample per batch\n",
    "                    j += 1 # next sample\n",
    "                    \n",
    "            # no more data, just stop the generator\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233,) (57,) (57, 34160)\n",
      "(60,) (6,) (6, 34160)\n",
      "(11,) (28,) (28, 34160)\n",
      "(31,) (32,) (32, 34160)\n",
      "(35,) (34,) (34, 34160)\n",
      "(37,) (6,) (6, 34160)\n",
      "(9,) (6,) (6, 34160)\n",
      "(8,) (67,) (67, 34160)\n",
      "(70,) (13,) (13, 34160)\n",
      "(16,) (12,) (12, 34160)\n",
      "(15,) (6,) (6, 34160)\n",
      "(9,) (35,) (35, 34160)\n",
      "(38,) (21,) (21, 34160)\n",
      "(24,) (49,) (49, 34160)\n",
      "(52,) (21,) (21, 34160)\n",
      "(23,) (38,) (38, 34160)\n"
     ]
    }
   ],
   "source": [
    "generator = BatchGenerator(data, batch_size=16)\n",
    "sample = next(generator.generate_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(src_vocab_dim, dst_vocab_dim=None, latent_dim=300, mask_value=0, embedding_matrix=None):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_dim is None:\n",
    "        dst_vocab_dim = src_vocab_dim\n",
    "        \n",
    "    encoder_inputs = Input(shape=(None,)) # timesteps, features (one-hot encoding)\n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        encoder_masking = Embedding(input_dim=src_vocab_dim, output_dim=latent_dim, weights=[embedding_matrix], \n",
    "                                   trainable=False)(encoder_masking)\n",
    "        \n",
    "    encoder = LSTM(units=latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_masking)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        decoder_masking = Embedding(input_dim=src_vocab_dim, output_dim=latent_dim, weights=[embedding_matrix], \n",
    "                                   trainable=False)(decoder_masking)\n",
    "    \n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(dst_vocab_dim, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_33: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-fe748bc89ac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencinf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecinf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_vocab_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-180be7fdbf8b>\u001b[0m in \u001b[0;36mdefine_models\u001b[1;34m(src_vocab_dim, dst_vocab_dim, latent_dim, mask_value, embedding_matrix)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mdecoder_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mdecoder_dense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst_vocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdecoder_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[0moriginal_input_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_33: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# import keras.backend as K\n",
    "\n",
    "# def cos_distance(y_true, y_pred):\n",
    "#     y_true = K.l2_normalize(y_true, axis=-1)\n",
    "#     y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "#     return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\n",
    "\n",
    "model, encinf, decinf = define_models(src_vocab_dim=vocab_size, embedding_matrix=embedding_matrix)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "(233,) (57,) (57, 34160)\n",
      "(60,) (6,) (6, 34160)\n",
      "(11,) (28,) (28, 34160)\n",
      "(31,) (32,) (32, 34160)\n",
      "(35,) (34,) (34, 34160)\n",
      "(37,) (6,) (6, 34160)\n",
      "(9,) (6,) (6, 34160)\n",
      "(8,) (67,) (67, 34160)\n",
      "(70,) (13,) (13, 34160)\n",
      "(16,) (12,) (12, 34160)\n",
      "(15,) (6,) (6, 34160)\n",
      "(9,) (35,) (35, 34160)\n",
      "(38,) (21,) (21, 34160)\n",
      "(24,) (49,) (49, 34160)\n",
      "(52,) (21,) (21, 34160)\n",
      "(23,) (38,) (38, 34160)\n",
      "(41,) (6,) (6, 34160)\n",
      "(9,) (15,) (15, 34160)\n",
      "(18,) (1,) (1, 34160)\n",
      "(3,) (62,) (62, 34160)\n",
      "(65,) (4,) (4, 34160)\n",
      "(6,) (31,) (31, 34160)\n",
      "(33,) (67,) (67, 34160)\n",
      "(69,) (2,) (2, 34160)\n",
      "(4,) (76,) (76, 34160)\n",
      "(79,) (12,) (12, 34160)\n",
      "(15,) (12,) (12, 34160)\n",
      "(20,) (46,) (46, 34160)\n",
      "(49,) (46,) (46, 34160)\n",
      "(49,) (38,) (38, 34160)\n",
      "(41,) (43,) (43, 34160)\n",
      "(46,) (27,) (27, 34160)\n",
      "(30,) (28,) (28, 34160)\n",
      "(31,) (21,) (21, 34160)\n",
      "(24,) (6,) (6, 34160)\n",
      "(8,) (41,) (41, 34160)\n",
      "(43,) (15,) (15, 34160)\n",
      "(17,) (6,) (6, 34160)\n",
      "(9,) (6,) (6, 34160)\n",
      "(9,) (6,) (6, 34160)\n",
      "(8,) (41,) (41, 34160)\n",
      "(43,) (15,) (15, 34160)\n",
      "(18,) (1,) (1, 34160)\n",
      "(3,) (3,) (3, 34160)\n",
      "(5,) (27,) (27, 34160)\n",
      "(30,) (6,) (6, 34160)\n",
      "(8,) (41,) (41, 34160)\n",
      "(44,) (32,) (32, 34160)\n",
      "(35,) (40,) (40, 34160)\n",
      "(45,) (22,) (22, 34160)\n",
      "(27,) (23,) (23, 34160)\n",
      "(29,) (24,) (24, 34160)\n",
      "(29,) (43,) (43, 34160)\n",
      "(45,) (42,) (42, 34160)\n",
      "(45,) (6,) (6, 34160)\n",
      "(8,) (62,) (62, 34160)\n",
      "(65,) (38,) (38, 34160)\n",
      "(41,) (34,) (34, 34160)\n",
      "(37,) (46,) (46, 34160)\n",
      "(49,) (49,) (49, 34160)\n",
      "(52,) (21,) (21, 34160)\n",
      "(24,) (28,) (28, 34160)\n",
      "(31,) (27,) (27, 34160)\n",
      "(30,) (1,) (1, 34160)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_41 to have 3 dimensions, but got array with shape (32, 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-b76612af5364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1875\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1877\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1878\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_41 to have 3 dimensions, but got array with shape (32, 76)"
     ]
    }
   ],
   "source": [
    "batch_generator = BatchGenerator(data, batch_size=32)\n",
    "model.fit_generator(batch_generator.generate_batch(), steps_per_epoch=1000, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def decode_sequence(input_seq, vocab_dim, eos_vector, tol=1e-2, max_output_len=200):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, vocab_dim))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = eos_vector\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 0 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output_embedding, h, c = decinf.predict([target_seq] + states_value)\n",
    "        output_embedding = output_embedding[0,0,:]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        eos_distance = distance.cosine(output_embedding, eos_vector)\n",
    "        if eos_distance < tol or i > max_output_len:\n",
    "            print(eos_distance, tol)\n",
    "            stop_condition = True\n",
    "            \n",
    "        # Sample a token\n",
    "        if distance.cosine(output_embedding, unk_vector) < tol:\n",
    "            sampled_word = unk_vector\n",
    "        else:\n",
    "            sampled_word = w2v.most_similar(positive=[output_embedding], topn=1)\n",
    "        decoded_sentence += sampled_word[0][0] + ' '     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, vocab_dim))\n",
    "        target_seq[0, 0] = output_embedding\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = 'every muscle in your body strains, and you feel the grinding of faraway pulleys as the portcullis slowly lifts open. at last the heavy machinery catches, and you relax. east'\n",
    "input_seq = np.array(prepare_input(test_line, word2embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 300)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = np.reshape(input_seq, (1, 30, 300))\n",
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\scipy\\spatial\\distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2657773540215731 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Perrine_Bridge butterflyer butterflyer butterflyer UniCredit UniCredit UniCredit UniCredit UniCredit UniCredit Woodbourne_Correctional_Facility Woodbourne_Correctional_Facility Woodbourne_Correctional_Facility Woodbourne_Correctional_Facility unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett '"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(input_seq, w2v.vector_size, eos_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml (3.6)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
