{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the followings:\n",
    "* http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "* http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/farizrahman4u/seq2seq\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "## TODO\n",
    "* ~~look into categorical representation~~\n",
    "* ~~look into the number of missing words over the total~~\n",
    "* ~~look into different models (attention, hierachical, etc.)~~\n",
    "* look into attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniele\\appdata\\local\\conda\\conda\\envs\\ml\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\daniele\\appdata\\local\\conda\\conda\\envs\\ml\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, LSTM, GRU, Dense, Masking, Embedding, Activation, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "OUTPUT_PATH = 'output'\n",
    "punct = set(punctuation)\n",
    "file_list = sorted(glob.glob('data/parsed/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'GoogleNews-vectors-negative300.bin.gz'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load params\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'rb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'rb') as params_file:\n",
    "    data = pkl.load(data_file)\n",
    "    params = pkl.load(params_file)\n",
    "    tokenizer = params['tokenizer']\n",
    "    index_word = params['index_word']\n",
    "    embedding_matrix = params['W']\n",
    "    missing_words = params['missing_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300#w2v.vector_size\n",
    "eos_token = 'EOS'\n",
    "unk_token = 'UNK'\n",
    "eos_vector = np.ones((embedding_dim))\n",
    "unk_vector = np.zeros((embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(repl='', string=text, pattern='^> ') # remove starting caret, if any\n",
    "    text = re.sub(repl='\\g<1> \\g<2>', string=text, pattern='(\\w+)-(\\w+)') # compound words    \n",
    "    text = re.sub(repl=' ', string=text, pattern='-{2,}|\\s{2,}|[%s\\t\\n/]' % (''.join(punctuation)))\n",
    "#     text = re.sub(repl=' digits ', string=text, pattern='^\\d+$| \\d+| \\d+ ') # replace digits with a standard 'digits' word\n",
    "    return text\n",
    "\n",
    "def read_corpus(file_list):\n",
    "    corpus = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            print('read_corpus: processing [{}]'.format(file))\n",
    "            corpus.append(f.read())\n",
    "            \n",
    "    return corpus\n",
    "            \n",
    "def build_vocabulary(corpus, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words=num_words+1, oov_token=oov_token) # +1 for the oov token\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Fix keras' nasty behaviour. See https://github.com/keras-team/keras/issues/8092\n",
    "    # Only include words found in w2v\n",
    "    tokenizer.word_index = {w:i for  w,i in tokenizer.word_index.items() \n",
    "                                if   i <= num_words} # <= because tokenizer is 1 indexed (this will leave out UNK)\n",
    "    tokenizer.num_words = num_words + 2  # UNK + EOS\n",
    "    tokenizer.word_index[oov_token] = len(tokenizer.word_index) + 1\n",
    "    tokenizer.word_index[eos_token] = len(tokenizer.word_index) + 1\n",
    "    index_word = [None for i in range(len(tokenizer.word_index) + 1)]  # index is 1-based\n",
    "    for w,i in tokenizer.word_index.items():\n",
    "        index_word[i] = w\n",
    "    \n",
    "    return tokenizer, index_word\n",
    "\n",
    "def prepare_data(corpus, tokenizer):\n",
    "    # Still go through the files line by line, as we want to predict the next scene, \n",
    "    # not just the next sentence\n",
    "    data = []\n",
    "    for i, doc in enumerate(corpus):\n",
    "        doc_data = []\n",
    "        print('prepare_data: processing [{}]'.format(file_list[i]))\n",
    "        \n",
    "        for j, line in enumerate(doc.split('\\n')):\n",
    "            if len(line) == 0:\n",
    "                print('Line {} is empty. Replacing with \"empty line\".'.format(j+1))\n",
    "                line = 'empty line'\n",
    "\n",
    "            tokenized_line = tokenizer.texts_to_sequences([line])[0]\n",
    "            if len(tokenized_line) == 0:\n",
    "                tokenized_line = tokenizer.texts_to_sequences(['empty line'])[0]\n",
    "                \n",
    "            doc_data.append(tokenized_line)\n",
    "\n",
    "        if len(doc_data) == 0:\n",
    "            print('File {} has no data'.format(file_list[i]))\n",
    "        else:\n",
    "            data.append(doc_data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def get_embeddings(word_index, w2v, unk_vector):\n",
    "    embedding_matrix=np.zeros(shape=(len(word_index)+1, w2v.vector_size))  # +1 as keras' tokenizer is 1-based\n",
    "    missing_words = []\n",
    "    for word,i in word_index.items():\n",
    "        if word not in w2v:\n",
    "            # Try to capitalize it\n",
    "            if word.capitalize() not in w2v:\n",
    "                missing_words.append(word)\n",
    "                embedding_matrix[i] = unk_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = w2v[word.capitalize()]\n",
    "        else:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "    \n",
    "    # add EOS token and oov_tokens\n",
    "    embedding_matrix[-1] = eos_vector # keras' index the vocab starting from 1\n",
    "    embedding_matrix[-2] = unk_vector\n",
    "    return embedding_matrix, missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_corpus: processing [data/parsed/parsed-12heads.txt]\n",
      "read_corpus: processing [data/parsed/parsed-1893.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "read_corpus: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "read_corpus: processing [data/parsed/parsed-abno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acitw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-actofmurder.txt]\n",
      "read_corpus: processing [data/parsed/parsed-adverbum.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afdfr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afflicted.txt]\n",
      "read_corpus: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "read_corpus: processing [data/parsed/parsed-aotearoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-awakening.txt]\n",
      "read_corpus: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bellwater.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bestman.txt]\n",
      "read_corpus: processing [data/parsed/parsed-blindhouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bonaventure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bookvol.txt]\n",
      "read_corpus: processing [data/parsed/parsed-broadsides.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bryant.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-buddha.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cacophony.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "read_corpus: processing [data/parsed/parsed-childsplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chineseroom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cove.txt]\n",
      "read_corpus: processing [data/parsed/parsed-crescent.txt]\n",
      "read_corpus: processing [data/parsed/parsed-csbb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cull.txt]\n",
      "read_corpus: processing [data/parsed/parsed-death.txt]\n",
      "read_corpus: processing [data/parsed/parsed-defra.txt]\n",
      "read_corpus: processing [data/parsed/parsed-degeneracy.txt]\n",
      "read_corpus: processing [data/parsed/parsed-demoparty.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "read_corpus: processing [data/parsed/parsed-divis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-djinni.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dualtransform.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eatme.txt]\n",
      "read_corpus: processing [data/parsed/parsed-edifice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-electric.txt]\n",
      "read_corpus: processing [data/parsed/parsed-elysium.txt]\n",
      "read_corpus: processing [data/parsed/parsed-envcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eurydice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodydies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finetuned.txt]\n",
      "read_corpus: processing [data/parsed/parsed-firebird.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-floatpoint.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foofoo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-forachange.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foth.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fragileshells.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-galatea.txt]\n",
      "read_corpus: processing [data/parsed/parsed-gdc09.txt]\n",
      "read_corpus: processing [data/parsed/parsed-glowgrass.txt]\n",
      "read_corpus: processing [data/parsed/parsed-goldilocks.txt]\n",
      "read_corpus: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ground.txt]\n",
      "read_corpus: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-halothane.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hamper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-heroes.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hoosegow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1701.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1702.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1703.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1704.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-indigo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-inls.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp11.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-invisargo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacket4.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacqissick.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jfw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ka.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lethe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lmwh.txt]\n",
      "read_corpus: processing [data/parsed/parsed-loose.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lostpig.txt]\n",
      "read_corpus: processing [data/parsed/parsed-luminous.txt]\n",
      "read_corpus: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-marika.txt]\n",
      "read_corpus: processing [data/parsed/parsed-measure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mingsheng.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mite.txt]\n",
      "read_corpus: processing [data/parsed/parsed-monkfish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-moonlittower.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newernewyear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nordandbert.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oad.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-onehalf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-orevore.txt]\n",
      "read_corpus: processing [data/parsed/parsed-park.txt]\n",
      "read_corpus: processing [data/parsed/parsed-partyfoul.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pathway.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pepper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi2.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_corpus: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "read_corpus: processing [data/parsed/parsed-progressive1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-punkpoints.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rameses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-recluse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-represso.txt]\n",
      "read_corpus: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "read_corpus: processing [data/parsed/parsed-robot.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rogue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rover.txt]\n",
      "read_corpus: processing [data/parsed/parsed-samfortune.txt]\n",
      "read_corpus: processing [data/parsed/parsed-santaland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scavenger.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sequitur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shelter.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sherbet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-simplethefts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-six.txt]\n",
      "read_corpus: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "read_corpus: processing [data/parsed/parsed-snacktime.txt]\n",
      "read_corpus: processing [data/parsed/parsed-softfood.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sorcerer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spring.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssi.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssos.txt]\n",
      "read_corpus: processing [data/parsed/parsed-starborn.txt]\n",
      "read_corpus: processing [data/parsed/parsed-statue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suspended.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suvehnux.txt]\n",
      "read_corpus: processing [data/parsed/parsed-swigian.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tacofiction.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tapestry.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-test.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theone.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theoracle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thread.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "read_corpus: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tryst.txt]\n",
      "read_corpus: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unclezeb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-undertow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unipool.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unscientific.txt]\n",
      "read_corpus: processing [data/parsed/parsed-vagueness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-varkana.txt]\n",
      "read_corpus: processing [data/parsed/parsed-violet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wand.txt]\n",
      "read_corpus: processing [data/parsed/parsed-weapon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wedding.txt]\n",
      "read_corpus: processing [data/parsed/parsed-windjack.txt]\n",
      "read_corpus: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wishbringer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wizard.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wof-sa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "read_corpus: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yakshaving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yetifail.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(file_list)\n",
    "tokenizer, index_word = build_vocabulary(corpus, num_words=20000, oov_token=unk_token)\n",
    "embedding_matrix, missing_words = get_embeddings(tokenizer.word_index, w2v, unk_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "OOV token index: 20001\n",
      "EOS token index: 20002\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', tokenizer.num_words)\n",
    "print('OOV token index:', tokenizer.word_index[unk_token])\n",
    "print('EOS token index:', tokenizer.word_index[eos_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size: (20003, 300)\n",
      "Unique words found (UNK, EOS + vocab): 20002\n",
      "Of which missing words (no embeddings): 1562\n"
     ]
    }
   ],
   "source": [
    "# text = 'Sample sentence with a possible balabiut token and some 1984 plus sentry'\n",
    "# print(preprocess(text))\n",
    "# print(prepare_input(text, tokenizer))\n",
    "vocab_size = len(embedding_matrix)\n",
    "unk_index = tokenizer.word_index[unk_token]\n",
    "eos_index = unk_index+1\n",
    "print('Embedding matrix size:', embedding_matrix.shape)\n",
    "print('Unique words found (UNK, EOS + vocab):', len(tokenizer.word_index))\n",
    "print('Of which missing words (no embeddings):', len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-12heads.txt]\n",
      "prepare_data: processing [data/parsed/parsed-1893.txt]\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "Line 243 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "prepare_data: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "prepare_data: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "prepare_data: processing [data/parsed/parsed-abno.txt]\n",
      "prepare_data: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "Line 1650 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acitw.txt]\n",
      "prepare_data: processing [data/parsed/parsed-actofmurder.txt]\n",
      "prepare_data: processing [data/parsed/parsed-adverbum.txt]\n",
      "prepare_data: processing [data/parsed/parsed-afdfr.txt]\n",
      "prepare_data: processing [data/parsed/parsed-afflicted.txt]\n",
      "prepare_data: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "Line 962 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-aotearoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-awakening.txt]\n",
      "prepare_data: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bellwater.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bestman.txt]\n",
      "prepare_data: processing [data/parsed/parsed-blindhouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bonaventure.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bookvol.txt]\n",
      "prepare_data: processing [data/parsed/parsed-broadsides.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bryant.txt]\n",
      "Line 5 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-buddha.txt]\n",
      "Line 666 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-cacophony.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "Line 544 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "prepare_data: processing [data/parsed/parsed-childsplay.txt]\n",
      "Line 2218 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-chineseroom.txt]\n",
      "prepare_data: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cove.txt]\n",
      "prepare_data: processing [data/parsed/parsed-crescent.txt]\n",
      "prepare_data: processing [data/parsed/parsed-csbb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cull.txt]\n",
      "prepare_data: processing [data/parsed/parsed-death.txt]\n",
      "prepare_data: processing [data/parsed/parsed-defra.txt]\n",
      "prepare_data: processing [data/parsed/parsed-degeneracy.txt]\n",
      "Line 927 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-demoparty.txt]\n",
      "prepare_data: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "prepare_data: processing [data/parsed/parsed-divis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-djinni.txt]\n",
      "prepare_data: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-dualtransform.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eas.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eas2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eatme.txt]\n",
      "prepare_data: processing [data/parsed/parsed-edifice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-electric.txt]\n",
      "prepare_data: processing [data/parsed/parsed-elysium.txt]\n",
      "prepare_data: processing [data/parsed/parsed-envcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-escapade.txt]\n",
      "Line 310 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-eurydice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-everybodydies.txt]\n",
      "prepare_data: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "Line 686 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fear.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "prepare_data: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "prepare_data: processing [data/parsed/parsed-finetuned.txt]\n",
      "prepare_data: processing [data/parsed/parsed-firebird.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fish.txt]\n",
      "Line 7 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-floatpoint.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foofoo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-forachange.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foth.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fragileshells.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "prepare_data: processing [data/parsed/parsed-galatea.txt]\n",
      "prepare_data: processing [data/parsed/parsed-gdc09.txt]\n",
      "Line 565 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-glowgrass.txt]\n",
      "prepare_data: processing [data/parsed/parsed-goldilocks.txt]\n",
      "Line 2489 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ground.txt]\n",
      "prepare_data: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-halothane.txt]\n",
      "Line 12 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-hamper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-heroes.txt]\n",
      "Line 2227 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hoosegow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1701.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1702.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1703.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1704.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-indigo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-inls.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp11.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-invisargo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacket4.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacqissick.txt]\n",
      "Line 43 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-jfw.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ka.txt]\n",
      "Line 803 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "Line 421 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lethe.txt]\n",
      "prepare_data: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-lmwh.txt]\n",
      "prepare_data: processing [data/parsed/parsed-loose.txt]\n",
      "prepare_data: processing [data/parsed/parsed-lostpig.txt]\n",
      "Line 303 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-luminous.txt]\n",
      "prepare_data: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "prepare_data: processing [data/parsed/parsed-marika.txt]\n",
      "prepare_data: processing [data/parsed/parsed-measure.txt]\n",
      "prepare_data: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mingsheng.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mite.txt]\n",
      "Line 5 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-monkfish.txt]\n",
      "prepare_data: processing [data/parsed/parsed-moonlittower.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mugglestudies.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-newernewyear.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "Line 27 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nordandbert.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oad.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "Line 36 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-onehalf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-orevore.txt]\n",
      "prepare_data: processing [data/parsed/parsed-park.txt]\n",
      "prepare_data: processing [data/parsed/parsed-partyfoul.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pathway.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2.txt]\n",
      "Line 119 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pax2011.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pepper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "prepare_data: processing [data/parsed/parsed-progressive1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-punkpoints.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rameses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-recluse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-represso.txt]\n",
      "prepare_data: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "prepare_data: processing [data/parsed/parsed-robot.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rogue.txt]\n",
      "Line 1018 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rover.txt]\n",
      "prepare_data: processing [data/parsed/parsed-samfortune.txt]\n",
      "prepare_data: processing [data/parsed/parsed-santaland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scavenger.txt]\n",
      "Line 1348 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sequitur.txt]\n",
      "prepare_data: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "prepare_data: processing [data/parsed/parsed-shelter.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sherbet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-simplethefts.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "Line 864 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-six.txt]\n",
      "prepare_data: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "prepare_data: processing [data/parsed/parsed-snacktime.txt]\n",
      "prepare_data: processing [data/parsed/parsed-softfood.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sorcerer.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spring.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spur.txt]\n",
      "Line 2652 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ssi.txt]\n",
      "Line 89 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ssos.txt]\n",
      "prepare_data: processing [data/parsed/parsed-starborn.txt]\n",
      "prepare_data: processing [data/parsed/parsed-statue.txt]\n",
      "prepare_data: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-stf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suspended.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suvehnux.txt]\n",
      "Line 1183 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-swigian.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tacofiction.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tangle.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tangle2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tapestry.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "prepare_data: processing [data/parsed/parsed-test.txt]\n",
      "Line 962 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "prepare_data: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theone.txt]\n",
      "Line 554 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-theoracle.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theplay.txt]\n",
      "Line 1 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thohc1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thohc2.txt]\n",
      "Line 5 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thread.txt]\n",
      "Line 1163 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "Line 171 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "Line 470 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tryst.txt]\n",
      "Line 4182 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unclezeb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-undertow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unipool.txt]\n",
      "Line 790 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-unscientific.txt]\n",
      "Line 3721 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-vagueness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-varkana.txt]\n",
      "prepare_data: processing [data/parsed/parsed-violet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wand.txt]\n",
      "prepare_data: processing [data/parsed/parsed-weapon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wedding.txt]\n",
      "Line 4585 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-windjack.txt]\n",
      "prepare_data: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wishbringer.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wizard.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wof-sa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "prepare_data: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yakshaving.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yetifail.txt]\n",
      "Line 58 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zorkII.txt]\n",
      "Line 4092 is empty. Replacing with \"empty line\".\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save various objects for later reuse\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'wb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'wb') as params_file:\n",
    "    params = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'index_word': index_word,\n",
    "        'W': embedding_matrix,\n",
    "        'missing_words': missing_words\n",
    "    }\n",
    "    pkl.dump(data, data_file)\n",
    "    pkl.dump(params, params_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(seq, n=3, step=1):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s[0],...s[n-1]), (s[0+skip_n],...,s[n-1+skip_n]), ...   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "\n",
    "    result = result[step:]\n",
    "    for elem in it:\n",
    "        result = result + (elem,)\n",
    "        if len(result) == n:\n",
    "            yield result\n",
    "            result = result[step:]\n",
    "\n",
    "def create_samples(data, test_split=0.1, shuffle=False, max_seq_length=None):    \n",
    "    samples = []\n",
    "    for i, play in enumerate(data):\n",
    "        if max_seq_length is not None:\n",
    "            chunks = [line[offset:offset+max_seq_length] \n",
    "                      for line in play \n",
    "                      for offset in range(0, len(line), max_seq_length)]\n",
    "        else:\n",
    "            chunks = play\n",
    "            \n",
    "        for scene, command, reply in window(chunks, n=3, step=2):\n",
    "            samples.append((scene, command, reply))\n",
    "#             if max_seq_length is not None:\n",
    "#                 sub_scenes  = [scene[offset:offset+max_seq_length]   \n",
    "#                                for offset in range(0, len(scene), max_seq_length)]\n",
    "#                 sub_replies = [reply[offset:offset+max_seq_length]   \n",
    "#                                for offset in range(0, len(reply), max_seq_length)]\n",
    "                \n",
    "#                 # sample as many triples as possible\n",
    "# #                 scenes   = sub_scenes[np.random.choice(0, range(len(sub_scenes)), len(sub_scenes)//max_seq_length)]\n",
    "# #                 commands = sub_cmds[np.random.choice(range(len(sub_cmds)), len(sub_cmds)//max_seq_length)]\n",
    "# #                 replies   = sub_replies[np.random.choice(range(len(sub_replies)), len(sub_replies)//max_seq_length)]\n",
    "                \n",
    "#                 for s in sub_scenes:\n",
    "#                     for r in sub_replies:\n",
    "#                         samples.append((s, command, r))\n",
    "                \n",
    "# #             if len(command) > 10:\n",
    "# #                 command_line = ' '.join([index_word[idx] for idx in command])\n",
    "# #                 print('Found anomalous command for play {} [{}] with length {}: [{}]'.format(\n",
    "# #                     i, os.path.basename(file_list[i]), len(command), command_line))\n",
    "#             else:\n",
    "#                 samples.append((scene, command, reply))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(samples)\n",
    "        \n",
    "    if test_split is not None:\n",
    "        split = int((1-test_split) * len(samples))\n",
    "        train_samples = samples[:split]\n",
    "        test_samples = samples[split:]\n",
    "        return train_samples, test_samples\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch generator\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, vocab_size, batch_size=1, reverse_input=False, shuffle=True, max_seq_length=None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.vocab_size = vocab_size\n",
    "        self.reverse_input = reverse_input\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.UNK = unk_index\n",
    "        self.EOS = eos_index\n",
    "        self.PAD = 0\n",
    "        \n",
    "    def generate_batch(self): \n",
    "        # every three lines comprise a sample sequence where the first two items\n",
    "        # are the input and the last one is the output\n",
    "        i  = 1 # batch counter        \n",
    "        x_enc = []\n",
    "        x_dec = []\n",
    "        y  = []\n",
    "            \n",
    "        while True:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.data)\n",
    "            \n",
    "            for j, (scene, command, reply) in enumerate(self.data):\n",
    "                if self.reverse_input:\n",
    "                    scene = scene[::-1]\n",
    "                    \n",
    "                encoder_input  = np.array(scene + command)\n",
    "                decoder_input  = np.array([self.EOS] + reply)\n",
    "                decoder_output = np.array(to_categorical(reply + [self.EOS], self.vocab_size))\n",
    "                    \n",
    "                x_enc.append(encoder_input)\n",
    "                x_dec.append(decoder_input)\n",
    "                y.append(decoder_output)\n",
    "                \n",
    "                if i == self.batch_size or j == len(data):\n",
    "                    if self.batch_size > 1:\n",
    "                        # pad and return the batch\n",
    "                        x_enc = sequence.pad_sequences(x_enc, padding='post', value=self.PAD, maxlen=self.max_seq_length)\n",
    "                        x_dec = sequence.pad_sequences(x_dec, padding='post', value=self.PAD, maxlen=self.max_seq_length)    \n",
    "                        y     = sequence.pad_sequences(y, padding='post', value=self.PAD, maxlen=self.max_seq_length)\n",
    "\n",
    "                    x_out, y_out = [np.array(x_enc), np.array(x_dec)], np.array(y)\n",
    "                    \n",
    "                    i = 1\n",
    "                    x_enc = []\n",
    "                    x_dec = []\n",
    "                    y = []\n",
    "\n",
    "                    yield (x_out, y_out)\n",
    "                else:\n",
    "                    i += 1 # next sample per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, val_loss, color=None, fname=None, legend=False):\n",
    "        N = len(loss)\n",
    "        train_loss_plt, = plt.plot(range(0, N), loss)\n",
    "        val_loss_plt, = plt.plot(range(0, N), val_loss)\n",
    "        \n",
    "        if color is not None:\n",
    "            plt.setp(train_loss_plt, color=color, linestyle='-')\n",
    "            plt.setp(val_loss_plt, color=color, linestyle='--')\n",
    "            \n",
    "        if legend:\n",
    "            plt.legend((train_loss_plt, val_loss_plt), ('train loss', 'val loss'))\n",
    "        \n",
    "        if fname is not None:\n",
    "            plt.savefig(fname)\n",
    "        \n",
    "        return [train_loss_plt, val_loss_plt]\n",
    "\n",
    "def plot(losses, fname=None):        \n",
    "    lines = []\n",
    "    names = []\n",
    "    colors = [plt.cm.gist_ncar(i) for i in np.linspace(0, 1, len(losses))]\n",
    "    for i, (loss, val_loss) in enumerate(losses):\n",
    "        lines.extend(plot_loss(loss, val_loss, colors[i]))\n",
    "        names.extend(['{} loss'.format(i+1), '{} val loss'.format(i+1)])\n",
    "    \n",
    "    plt.legend(lines, names)\n",
    "    \n",
    "    if fname is not None:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class InferenceModelsCheckpoint(Callback):\n",
    "    def __init__(self, models, filepath, monitor='val_loss', verbose=0):\n",
    "        self.encoder, self.decoder = models\n",
    "        self.monitor = monitor\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.best = np.Inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current < self.best:\n",
    "            filepath = self.filepath\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
    "                \n",
    "            self.encoder.save(filepath + '-encinf.h5')\n",
    "            self.decoder.save(filepath + '-decinf.h5')                \n",
    "            self.best = current\n",
    "\n",
    "def train_model(models, train_samples, batch_size, epochs=10, shuffle=True, n_folds=None, train_split=None, \n",
    "                model_name=None, max_seq_length=None, steps_per_epoch=None):\n",
    "    assert not (n_folds is not None and train_split is not None), ValueError('Either n_folds or train_split should be specified, but not both.')\n",
    "    assert not (n_folds is None and train_split is None), ValueError('Either n_folds or train_split must be specified.')   \n",
    "    \n",
    "    def _run_model(train, val, steps_per_epoch):\n",
    "        train_generator = BatchGenerator(train, batch_size=batch_size, vocab_size=vocab_size, reverse_input=True, \n",
    "                                         shuffle=shuffle, max_seq_length=max_seq_length)\n",
    "        val_generator = BatchGenerator(val, batch_size=batch_size, vocab_size=vocab_size, reverse_input=True, \n",
    "                                       shuffle=shuffle, max_seq_length=max_seq_length)\n",
    "        \n",
    "        # utils callbacks\n",
    "        checkpointer = ModelCheckpoint(filepath=model_name + '.h5', verbose=1, save_best_only=True)\n",
    "        seq2seq_cp = InferenceModelsCheckpoint(filepath=model_name, verbose=1, models=(encoder, decoder))\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, mode='auto', \n",
    "                                      min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "        early_stop = EarlyStopping(patience=1, min_delta=0.0001, verbose=1)\n",
    "        callbacks = [checkpointer, seq2seq_cp, reduce_lr, early_stop]\n",
    "        \n",
    "        # actual train\n",
    "        if steps_per_epoch is None:\n",
    "            steps_per_epoch = len(train)//batch_size\n",
    "            \n",
    "        history = model.fit_generator(train_generator.generate_batch(), steps_per_epoch=steps_per_epoch, epochs=epochs, \n",
    "                            validation_data=val_generator.generate_batch(), validation_steps=len(val)//batch_size,\n",
    "                            callbacks=callbacks)\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    model, encoder, decoder = models\n",
    "    train_samples = np.array(train_samples)\n",
    "    losses = []  # keep track of train and val loss for each fold\n",
    "    \n",
    "    if n_folds is None:\n",
    "        train, val = train_test_split(train_samples, train_size=train_split, shuffle=shuffle)\n",
    "        \n",
    "        history = _run_model(train, val, steps_per_epoch)\n",
    "        # plot current losses\n",
    "        plot_loss(history.history['loss'], history.history['val_loss'], fname=model_name + '.png', legend=True)\n",
    "    else:  \n",
    "        kfold = KFold(n_folds, shuffle=shuffle)\n",
    "        for i, (train, val) in enumerate(kfold.split(train_samples)):\n",
    "            print(\"Running fold {}/{}\".format(i+1, n_folds))\n",
    "\n",
    "            model_file = model_name + '-fold-{}'.format(i+1)\n",
    "            history = _run_model(train_samples[train], train_samples[val], steps_per_epoch)\n",
    "\n",
    "            # record losses for the final plot\n",
    "            losses.append((history.history['loss'], history.history['val_loss']))\n",
    "\n",
    "        # plot losses for all folds\n",
    "        plot(losses, model_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import dot, concatenate\n",
    "\n",
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models_lstm(src_vocab_size, embedding_matrix, input_shape, dst_vocab_size=None, embedding_dim=300, latent_dim=128, \n",
    "                       mask_value=0, trainable_embeddings=False, encoder_depth=1, decoder_depth=1, attention=False):    \n",
    "    \n",
    "    def get_attention_context(decoder_outputs, encoder_outputs):\n",
    "        # this gives weights in the form [[<dec1,enc1>, ..., <dec1,encN>], ..., [<decM,enc1>, ..., <decM,encN>]]\n",
    "        weights = dot([decoder_outputs, encoder_outputs], axes=[2, 2], name='att_weights_dot')\n",
    "        weights = Activation('softmax', name='att_weights_act')(weights)\n",
    "        \n",
    "        # this computes the actual context. We need to move along the last dimension for the weights (i.e.)\n",
    "        # timesteps), and the first dimension for the encoder outputs (i.e. timesteps) so as to compute a\n",
    "        # linear combination of encoder outputs. The results contain as many elements as timesteps, and the \n",
    "        # i-th component of such elements is the linear combination of all encoder outputs' i-th component\n",
    "        context = dot([weights, encoder_outputs], axes=[2, 1], name='att_context_dot')\n",
    "        context = concatenate([context, decoder_outputs])\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_size is None:\n",
    "        dst_vocab_size = src_vocab_size\n",
    "        \n",
    "    encoder_inputs = Input(shape=input_shape) # timesteps, features (integer)\n",
    "    decoder_inputs = Input(shape=input_shape)\n",
    "    inputs = [encoder_inputs, decoder_inputs]\n",
    "    \n",
    "    masking = Masking(mask_value=mask_value)\n",
    "    embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)\n",
    "    \n",
    "    encoder_embedding = embedding(masking(encoder_inputs))\n",
    "    decoder_embedding = embedding(masking(decoder_inputs))\n",
    "    \n",
    "    ######## ENCODER ########\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder_0')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    for i in range(encoder_depth - 1):  # DEPTH (the encoder need not be shared, so we can just instantiate a new LSTM object)\n",
    "        encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_sequences=True, return_state=True, \n",
    "                                                 name='encoder_{}'.format(i+1))(encoder_outputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    \n",
    "    ######## DECODER ########\n",
    "    decoder_layers = []  # keep track of deep layers\n",
    "    \n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_lstm = LSTM(units=latent_dim, return_sequences=True, return_state=True, name='decoder_0')    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    for i in range(decoder_depth - 1):  # DEPTH\n",
    "        lstm = LSTM(units=latent_dim, return_sequences=True, return_state=True, name = 'decoder_{}'.format(i+1))\n",
    "        decoder_layers.append(lstm)\n",
    "        decoder_outputs, _, _ = lstm(decoder_outputs)\n",
    "        \n",
    "    decoder_dense = Dense(dst_vocab_size, activation='softmax')\n",
    "    \n",
    "    if attention:\n",
    "        context_dense = Dense(latent_dim, activation='tanh', name='att_context_act')\n",
    "        context = get_attention_context(decoder_outputs, encoder_outputs)             \n",
    "        decoder_outputs = decoder_dense(context_dense(context))\n",
    "    else:\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "    train_model = Model(inputs, decoder_outputs, name='train_model')\n",
    "    \n",
    "    ####### INFERENCE ENCODER #######\n",
    "    inference_encoder = Model(encoder_inputs, encoder_states + [encoder_outputs], name='inf_encoder')\n",
    "    \n",
    "    ####### INFERENCE DECODER #######\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='inference_h')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='inference_c')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]    \n",
    "\n",
    "    for d in range(decoder_depth - 1):  # DEPTH\n",
    "        decoder_outputs, state_h, state_c = decoder_layers[d](decoder_outputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "    \n",
    "    if attention:\n",
    "        attention_inputs = Input(shape=input_shape + (latent_dim,), name='attention_inputs')\n",
    "        context = get_attention_context(decoder_outputs, attention_inputs)\n",
    "        decoder_outputs = decoder_dense(context_dense(context))\n",
    "        \n",
    "        inference_decoder = Model([decoder_inputs, attention_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states, name='inf_decoder')\n",
    "    else:\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        inference_decoder = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states, name='inf_decoder')\n",
    "    \n",
    "    # return all models\n",
    "    return train_model, inference_encoder, inference_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic models\n",
    "Here, we train two basic (1-deep) models, parameterized by their sequence length:\n",
    "* 200-length (better but much slower to train)\n",
    "* 50-length (much faster to train but less performant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, None)         0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 300)    6000900     masking_3[0][0]                  \n",
      "                                                                 masking_3[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0 (LSTM)                [(None, None, 300),  721200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_0 (LSTM)                [(None, None, 300),  721200      embedding_3[1][0]                \n",
      "                                                                 encoder_0[0][1]                  \n",
      "                                                                 encoder_0[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 20003)  6020903     decoder_0[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,464,203\n",
      "Trainable params: 7,463,303\n",
      "Non-trainable params: 6,000,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 200\n",
    "model_name = 'basic_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(None,), latent_dim=300, \n",
    "                                           embedding_matrix=embedding_matrix, encoder_depth=1, decoder_depth=1, \n",
    "                                           trainable_embeddings=False, attention=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved data...\n",
      "Train samples: 101787\n",
      "Test samples: 0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('data/train_test_data_200.pkl'):\n",
    "    print('Loading saved data...')\n",
    "    with open('data/train_test_data_200.pkl', mode='rb') as f:\n",
    "        train_samples, test_samples = pkl.load(f)\n",
    "else:\n",
    "    print('Creatins samples...')\n",
    "    train_samples, test_samples = create_samples(data, max_seq_length=max_seq_length, test_split=0)\n",
    "    with open('data/train_test_data_200.pkl', mode='wb') as f:\n",
    "        pkl.dump((train_samples, test_samples), f)\n",
    "\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2244s 2s/step - loss: 0.6119 - categorical_accuracy: 0.0149 - val_loss: 0.5684 - val_categorical_accuracy: 0.0184\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56839, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_0_1/while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'encoder_0_1/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to basic_seq2seq_20k_200_300d_1-1_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'inference_h_1:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'inference_c_1:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 2234s 2s/step - loss: 0.5456 - categorical_accuracy: 0.0199 - val_loss: 0.5397 - val_categorical_accuracy: 0.0212\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56839 to 0.53966, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00002: saving model to basic_seq2seq_20k_200_300d_1-1_LSTM\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 2231s 2s/step - loss: 0.5221 - categorical_accuracy: 0.0218 - val_loss: 0.5205 - val_categorical_accuracy: 0.0228\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53966 to 0.52053, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00003: saving model to basic_seq2seq_20k_200_300d_1-1_LSTM\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 2227s 2s/step - loss: 0.5031 - categorical_accuracy: 0.0236 - val_loss: 0.5081 - val_categorical_accuracy: 0.0244\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52053 to 0.50809, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00004: saving model to basic_seq2seq_20k_200_300d_1-1_LSTM\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 2229s 2s/step - loss: 0.4971 - categorical_accuracy: 0.0251 - val_loss: 0.4988 - val_categorical_accuracy: 0.0256\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50809 to 0.49879, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00005: saving model to basic_seq2seq_20k_200_300d_1-1_LSTM\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2afacf782e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name, max_seq_length=max_seq_length, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 50\n",
    "model_name = 'basic_seq2seq_20k_50_300d_1-1_LSTM'\n",
    "model_file = model_name + '.h5'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(None,), latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=1, decoder_depth=1, trainable_embeddings=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 121867\n",
      "Test samples: 0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('data/train_test_data_50.pkl'):\n",
    "    print('Loading saved data...')\n",
    "    with open('data/train_test_data_50.pkl', mode='rb') as f:\n",
    "        train_samples, test_samples = pkl.load(f)\n",
    "else:\n",
    "    print('Creating samples')\n",
    "    train_samples, test_samples = create_samples(data, max_seq_length=50, test_split=0)\n",
    "    with open('data/train_test_data_50.pkl', mode='wb') as f:\n",
    "        pkl.dump((train_samples, test_samples), f)\n",
    "\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1499s 1s/step - loss: 1.9959 - categorical_accuracy: 0.0596 - val_loss: 1.8318 - val_categorical_accuracy: 0.0746\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83184, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_0_2/while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'encoder_0_2/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'inference_h_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'inference_c_2:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1493s 1s/step - loss: 1.7264 - categorical_accuracy: 0.0821 - val_loss: 1.6943 - val_categorical_accuracy: 0.0895\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.83184 to 1.69428, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00002: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1495s 1s/step - loss: 1.6045 - categorical_accuracy: 0.0954 - val_loss: 1.6025 - val_categorical_accuracy: 0.0996\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.69428 to 1.60253, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00003: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1495s 1s/step - loss: 1.5196 - categorical_accuracy: 0.1057 - val_loss: 1.5428 - val_categorical_accuracy: 0.1080\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.60253 to 1.54282, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00004: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1497s 1s/step - loss: 1.4515 - categorical_accuracy: 0.1141 - val_loss: 1.4967 - val_categorical_accuracy: 0.1149\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.54282 to 1.49673, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00005: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2afacf9fcf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name, max_seq_length=50, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-based models\n",
    "Here, we train two basic (1-deep) models as before, parameterized by their sequence length:\n",
    "* 200-length (better but much slower to train)\n",
    "* 50-length (much faster to train but less performant)\n",
    "\n",
    "In addition, they also implement a simple attention mechanism as described in https://arxiv.org/abs/1508.04025 (see also https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html for a nice tutorial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training model ===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None)         0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    6000900     masking_1[0][0]                  \n",
      "                                                                 masking_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0 (LSTM)                [(None, None, 300),  721200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_0 (LSTM)                [(None, None, 300),  721200      embedding_1[1][0]                \n",
      "                                                                 encoder_0[0][1]                  \n",
      "                                                                 encoder_0[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_dot (Dot)           (None, None, None)   0           decoder_0[0][0]                  \n",
      "                                                                 encoder_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_act (Activation)    (None, None, None)   0           att_weights_dot[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_context_dot (Dot)           (None, None, 300)    0           att_weights_act[0][0]            \n",
      "                                                                 encoder_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 600)    0           att_context_dot[0][0]            \n",
      "                                                                 decoder_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_context_act (Dense)         (None, None, 300)    180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 20003)  6020903     att_context_act[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 13,644,503\n",
      "Trainable params: 7,643,603\n",
      "Non-trainable params: 6,000,900\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "=== Inference encoder ===\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         6000900   \n",
      "_________________________________________________________________\n",
      "encoder_0 (LSTM)             [(None, None, 300), (None 721200    \n",
      "=================================================================\n",
      "Total params: 6,722,100\n",
      "Trainable params: 721,200\n",
      "Non-trainable params: 6,000,900\n",
      "_________________________________________________________________\n",
      "\n",
      "=== Inference decoder ===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    6000900     masking_1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_h (InputLayer)        (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inference_c (InputLayer)        (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_0 (LSTM)                [(None, None, 300),  721200      embedding_1[1][0]                \n",
      "                                                                 inference_h[0][0]                \n",
      "                                                                 inference_c[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_inputs (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_dot (Dot)           (None, None, None)   0           decoder_0[1][0]                  \n",
      "                                                                 attention_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_act (Activation)    (None, None, None)   0           att_weights_dot[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_context_dot (Dot)           (None, None, 300)    0           att_weights_act[0][0]            \n",
      "                                                                 attention_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 600)    0           att_context_dot[0][0]            \n",
      "                                                                 decoder_0[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_context_act (Dense)         (None, None, 300)    180300      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 20003)  6020903     att_context_act[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 12,923,303\n",
      "Trainable params: 6,922,403\n",
      "Non-trainable params: 6,000,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 200\n",
    "model_name = 'att_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(None,), latent_dim=300, \n",
    "                                           embedding_matrix=embedding_matrix, encoder_depth=1, decoder_depth=1, \n",
    "                                           trainable_embeddings=False, attention=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "\n",
    "print('=== Training model ===')\n",
    "model.summary()\n",
    "print('\\n=== Inference encoder ===')\n",
    "encinf.summary()\n",
    "print('\\n=== Inference decoder ===')\n",
    "decinf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('data/train_test_data_200.pkl'):\n",
    "    print('Loading saved data...')\n",
    "    with open('data/train_test_data_200.pkl', mode='rb') as f:\n",
    "        train_samples, test_samples = pkl.load(f)\n",
    "else:\n",
    "    print('Creatins samples...')\n",
    "    train_samples, test_samples = create_samples(data, max_seq_length=max_seq_length, test_split=0)\n",
    "    with open('data/train_test_data_200.pkl', mode='wb') as f:\n",
    "        pkl.dump((train_samples, test_samples), f)\n",
    "\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1593s 2s/step - loss: 0.8809 - categorical_accuracy: 0.0234 - val_loss: 0.8452 - val_categorical_accuracy: 0.0300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84519, saving model to att_seq2seq_20k_200_300d_1-1_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_0/while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'encoder_0/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to att_seq2seq_20k_200_300d_1-1_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'inference_h:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'inference_c:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1605s 2s/step - loss: 0.7943 - categorical_accuracy: 0.0320 - val_loss: 0.7659 - val_categorical_accuracy: 0.0334\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84519 to 0.76594, saving model to att_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00002: saving model to att_seq2seq_20k_200_300d_1-1_LSTM\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1596s 2s/step - loss: 0.7655 - categorical_accuracy: 0.0357 - val_loss: 0.7586 - val_categorical_accuracy: 0.0375\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76594 to 0.75862, saving model to att_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00003: saving model to att_seq2seq_20k_200_300d_1-1_LSTM\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1601s 2s/step - loss: 0.7365 - categorical_accuracy: 0.0387 - val_loss: 0.7181 - val_categorical_accuracy: 0.0380\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.75862 to 0.71809, saving model to att_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00004: saving model to att_seq2seq_20k_200_300d_1-1_LSTM\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1609s 2s/step - loss: 0.7125 - categorical_accuracy: 0.0407 - val_loss: 0.6908 - val_categorical_accuracy: 0.0396\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.71809 to 0.69080, saving model to att_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00005: saving model to att_seq2seq_20k_200_300d_1-1_LSTM\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ae46b41cb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training model ===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_48 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_24 (Masking)            (None, None)         0           input_47[0][0]                   \n",
      "                                                                 input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, None, 300)    6000900     masking_24[0][0]                 \n",
      "                                                                 masking_24[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0 (LSTM)                [(None, None, 300),  721200      embedding_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_0 (LSTM)                [(None, None, 300),  721200      embedding_24[1][0]               \n",
      "                                                                 encoder_0[0][1]                  \n",
      "                                                                 encoder_0[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_dot (Dot)           (None, None, None)   0           decoder_0[0][0]                  \n",
      "                                                                 encoder_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_act (Activation)    (None, None, None)   0           att_weights_dot[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_context_dot (Dot)           (None, None, 300)    0           att_weights_act[0][0]            \n",
      "                                                                 encoder_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, None, 600)    0           att_context_dot[0][0]            \n",
      "                                                                 decoder_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_context_act (Dense)         (None, None, 300)    180300      concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, None, 20003)  6020903     att_context_act[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 13,644,503\n",
      "Trainable params: 7,643,603\n",
      "Non-trainable params: 6,000,900\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "=== Inference encoder ===\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "masking_24 (Masking)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_24 (Embedding)     (None, None, 300)         6000900   \n",
      "_________________________________________________________________\n",
      "encoder_0 (LSTM)             [(None, None, 300), (None 721200    \n",
      "=================================================================\n",
      "Total params: 6,722,100\n",
      "Trainable params: 721,200\n",
      "Non-trainable params: 6,000,900\n",
      "_________________________________________________________________\n",
      "\n",
      "=== Inference decoder ===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_48 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_24 (Masking)            (None, None)         0           input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, None, 300)    6000900     masking_24[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "inference_h (InputLayer)        (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inference_c (InputLayer)        (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_0 (LSTM)                [(None, None, 300),  721200      embedding_24[1][0]               \n",
      "                                                                 inference_h[0][0]                \n",
      "                                                                 inference_c[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_inputs (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_dot (Dot)           (None, None, None)   0           decoder_0[1][0]                  \n",
      "                                                                 attention_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "att_weights_act (Activation)    (None, None, None)   0           att_weights_dot[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att_context_dot (Dot)           (None, None, 300)    0           att_weights_act[0][0]            \n",
      "                                                                 attention_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, None, 600)    0           att_context_dot[0][0]            \n",
      "                                                                 decoder_0[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "att_context_act (Dense)         (None, None, 300)    180300      concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, None, 20003)  6020903     att_context_act[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 12,923,303\n",
      "Trainable params: 6,922,403\n",
      "Non-trainable params: 6,000,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 50\n",
    "model_name = 'att_seq2seq_20k_50_300d_1-1_LSTM'\n",
    "\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(None,), latent_dim=300, \n",
    "                                           embedding_matrix=embedding_matrix, encoder_depth=1, decoder_depth=1, \n",
    "                                           trainable_embeddings=False, attention=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "\n",
    "print('=== Training model ===')\n",
    "model.summary()\n",
    "print('\\n=== Inference encoder ===')\n",
    "encinf.summary()\n",
    "print('\\n=== Inference decoder ===')\n",
    "decinf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved data...\n",
      "Train samples: 121867\n",
      "Test samples: 0\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('data/train_test_data_50.pkl'):\n",
    "    print('Loading saved data...')\n",
    "    with open('data/train_test_data_50.pkl', mode='rb') as f:\n",
    "        train_samples, test_samples = pkl.load(f)\n",
    "else:\n",
    "    print('Creating samples')\n",
    "    train_samples, test_samples = create_samples(data, max_seq_length=50, test_split=0)\n",
    "    with open('data/train_test_data_50.pkl', mode='wb') as f:\n",
    "        pkl.dump((train_samples, test_samples), f)\n",
    "\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniele\\appdata\\local\\conda\\conda\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 791s 791ms/step - loss: 2.0664 - categorical_accuracy: 0.0539 - val_loss: 1.9883 - val_categorical_accuracy: 0.0675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.98830, saving model to att_seq2seq_20k_50_300d_1-1_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniele\\appdata\\local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_0_23/while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'encoder_0_23/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to att_seq2seq_20k_50_300d_1-1_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniele\\appdata\\local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\network.py:888: UserWarning: Layer decoder_0 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'inference_h_23:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'inference_c_23:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 806s 806ms/step - loss: 1.8871 - categorical_accuracy: 0.0698 - val_loss: 1.8848 - val_categorical_accuracy: 0.0751\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.98830 to 1.88475, saving model to att_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00002: saving model to att_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 818s 818ms/step - loss: 1.8381 - categorical_accuracy: 0.0774 - val_loss: 1.8336 - val_categorical_accuracy: 0.0819\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.88475 to 1.83357, saving model to att_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00003: saving model to att_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 782s 782ms/step - loss: 1.7744 - categorical_accuracy: 0.0834 - val_loss: 1.7832 - val_categorical_accuracy: 0.0867\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.83357 to 1.78316, saving model to att_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00004: saving model to att_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 812s 812ms/step - loss: 1.7304 - categorical_accuracy: 0.0875 - val_loss: 1.7485 - val_categorical_accuracy: 0.0912\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.78316 to 1.74852, saving model to att_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00005: saving model to att_seq2seq_20k_50_300d_1-1_LSTM\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Test\n",
    "Here we test the models by producing output sentences from a sample input sentence. We provide two type of inference:\n",
    "* greedy inference (based on argmax sampling)\n",
    "* beam-search inference (slower but usually giving more realistic results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(seq):\n",
    "    return ' '.join([index_word[int(idx)] for idx in seq])\n",
    "\n",
    "def prepare_input(input_text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([preprocess(input_text)])[0]\n",
    "\n",
    "def decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50, attention=False):\n",
    "    # Encode the input as state vectors.\n",
    "    h, c, encoder_outputs = encinf.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0, 0] = eos_index\n",
    "    \n",
    "    if attention:\n",
    "        decoder_inputs = [target_seq, encoder_outputs, h, c]\n",
    "    else:\n",
    "        decoder_inputs = [target_seq, h, c]\n",
    "        \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 1 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output, h, c = decinf.predict(decoder_inputs)\n",
    "        sampled_word_index = np.argmax(output[0, -1, :])  # batch, time, features\n",
    "        sampled_word = index_word[sampled_word_index]\n",
    "\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if sampled_word == eos_token or i > max_output_len:\n",
    "            stop_condition = True     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_word_index\n",
    "        i += 1\n",
    "        \n",
    "        # Update states\n",
    "        if attention:\n",
    "            decoder_inputs = [target_seq, encoder_outputs, h, c]\n",
    "        else:\n",
    "            decoder_inputs = [target_seq, h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def beam_decoder_lstm(encinf, decinf, input_seq, k, vocab_size, max_output_len=50, attention=False, best_only=False):\n",
    "    # Encode the input as state vectors.\n",
    "    h, c, encoder_outputs = encinf.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index.\n",
    "    target_seq[0,0] = eos_index\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''    \n",
    "    decoded_sequences = []\n",
    "    \n",
    "    if attention:\n",
    "        decoder_inputs = [target_seq, encoder_outputs, h, c]\n",
    "    else:\n",
    "        decoder_inputs = [target_seq, h, c]\n",
    "    \n",
    "    # Init Beam Array (decoder inputs, candidate sequence, score)\n",
    "    alive_beams = [(target_seq, decoder_inputs, 1.0)]\n",
    "    dead_beams = []\n",
    "    \n",
    "    output_len = 1 # number of sampled words\n",
    "    while not stop_condition:        \n",
    "        all_candidates = []\n",
    "        for i in range(len(alive_beams)):  # for each beam, try to append the next predicted word\n",
    "            # Get the values of the beam\n",
    "            candidate_seq, decoder_inputs, score = alive_beams[i]\n",
    "#             print('Extending beam {} [{}]'.format(i+1, translate_sequence(candidate_seq[0])))\n",
    "            \n",
    "            # predict the next word and create candidate list\n",
    "            output, h, c = decinf.predict(decoder_inputs)\n",
    "            candidate_score_list = output[0][0]\n",
    "            \n",
    "            for j in range(len(candidate_score_list)):  # extend the beam one word at a time and compute the score               \n",
    "                # Add each candidate to the target sequence.\n",
    "                new_candidate_seq = np.append(candidate_seq, [[j]], axis=1)\n",
    "                \n",
    "                target_seq = np.zeros((1, 1))\n",
    "                target_seq[0, 0] = j\n",
    "                \n",
    "                if attention:\n",
    "                    decoder_inputs = [target_seq, encoder_outputs, h, c]\n",
    "                else:\n",
    "                    decoder_inputs = [target_seq, h, c]\n",
    "                    \n",
    "                candidate_beam = (new_candidate_seq, decoder_inputs, score*-np.log(candidate_score_list[j]))\n",
    "                all_candidates.append(candidate_beam)\n",
    "\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[-1])\n",
    "        # select k best\n",
    "        alive_beams = ordered[:k]\n",
    "        \n",
    "        # if a beam has the index of the eos, \n",
    "        # 1. Add the beam to decoded_sequences, \n",
    "        # 2. Remove it from beams \n",
    "        # 3. Lower number of beams (k)\n",
    "        dead_beams = []\n",
    "        for bindex, (candidate_seq, _, score) in enumerate(alive_beams):\n",
    "            candidate_seq = candidate_seq[0]\n",
    "            if candidate_seq[-1] == eos_index:\n",
    "#                 print('EOS reached for beam [{}]'.format(translate_sequence(candidate_seq)))\n",
    "                dead_beams.append(bindex)\n",
    "                k -= 1\n",
    "                decoded_sequences.append((candidate_seq, score))\n",
    "                stop_condition = (0 == k)  # no more beams\n",
    "        \n",
    "        alive_beams = [beam for bindex, beam in enumerate(alive_beams) if bindex not in dead_beams]\n",
    "        \n",
    "        # Exit condition: either each beam got eos or hit max length\n",
    "        if not stop_condition and output_len > max_output_len:\n",
    "            # Add the beams that did not have eos so far to the decoded_sequences\n",
    "            decoded_sequences += alive_beams\n",
    "            stop_condition = True\n",
    "\n",
    "        output_len += 1\n",
    "        \n",
    "    decoded_sequences = sorted(decoded_sequences, key=lambda beam: beam[1])\n",
    "        \n",
    "    decoded_sentences = [(score, [index_word[int(idx)] if idx > 0 else 'NONE' for idx in seq[1:]]) for seq, score in decoded_sequences]\n",
    "    decoded_sentences = [(score, ' '.join(sentence)) for score, sentence in decoded_sentences]\n",
    "\n",
    "    if best_only:\n",
    "        return decoded_sentences[0][1]\n",
    "    else:\n",
    "        return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lines = [\"\"\"You find yourself in a dimly lit room. Before you you can spot a table. \n",
    "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
    "             to be a large living room. examine table\"\"\",\n",
    "             \"\"\"You find yourself in a dimly lit room. Before you you can spot a table. \n",
    "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
    "to be a large living room. look\"\"\",\n",
    "              \"\"\"You find yourself in a dimly lit room. Before you you can spot a table. \n",
    "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
    "to be a large living room. east\"\"\",\n",
    "              \"\"\"The wind blows on your face. You can barely feel your right hand It has been three days\n",
    "              since you left the camp. Or was it four? While you struggle to remember, you spot what\n",
    "              appears to be the light of a bright beacon to the east. east\"\"\",\n",
    "              \"\"\"inventory\"\"\", \n",
    "              \"\"\"wait\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(None,), latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=1, decoder_depth=1, trainable_embeddings=False, attention=False)\n",
    "\n",
    "# load the 200-long model\n",
    "model_name = 'models/basic_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "if os.path.isfile(model_name + '.h5'):\n",
    "    print('Loading weights...')\n",
    "    encinf.load_weights(model_name + '-encinf.h5')\n",
    "    decinf.load_weights(model_name + '-decinf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: you are in a small cave a large room with been been been used to use a few feet of the room there is a small room to the north and a small room to the north is a small window leading into the room to the north is a small \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: you are in a small cave a large room with been been been used to use a few feet of the room there is a small room to the north and a small room to the north is a small window leading into the room to the north is a small \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: you are in a small cave a large room with been been been used to use a few feet of the room there is a small room to the north and a small room to the north is a small window leading into the room to the north is a small \n",
      "IN: The wind blows on your face. You can barely feel your right hand It has been three days\n",
      "              since you left the camp. Or was it four? While you struggle to remember, you spot what\n",
      "              appears to be the light of a bright beacon to the east. east\n",
      "OUT: the UNK is not a little more than than than the other than the other of the house EOS \n",
      "IN: inventory\n",
      "OUT: the UNK is not a little more than a few feet away EOS \n",
      "IN: wait\n",
      "OUT: you are in a small cave a large room with been been been used to use a few feet of the room there is a small room to the north and a small room to the north is a small window leading into the room to the north is a small \n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line))\n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: you cannot see any such thing you can hear the crashing and clanging of a sword fight coming from the tavern common room EOS\n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: you cannot see any such thing you can hear the crashing and clanging of a sword fight coming from the tavern common room EOS\n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: you cannot see any such thing you can hear the crashing and clanging of a sword fight coming from the tavern common room EOS\n",
      "IN: The wind blows on your face. You can barely feel your right hand It has been three days\n",
      "              since you left the camp. Or was it four? While you struggle to remember, you spot what\n",
      "              appears to be the light of a bright beacon to the east. east\n",
      "OUT: it is a small room in the corner of the room there is a small window to the north and a closed door to the south is open EOS\n",
      "IN: inventory\n",
      "OUT: you cannot see any such thing you can hear the crashing and clanging of a sword fight coming from the tavern common room EOS\n",
      "IN: wait\n",
      "OUT: you EOS\n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line))\n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(beam_decoder_lstm(encinf, decinf, input_seq, 5, vocab_size, max_output_len=50, best_only=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "# load the 50-long model\n",
    "model_name = 'models/basic_seq2seq_20k_50_300d_1-1_LSTM'\n",
    "if os.path.isfile(model_name + '.h5'):\n",
    "    print('Loading weights...')\n",
    "    encinf.load_weights(model_name + '-encinf.h5')\n",
    "    decinf.load_weights(model_name + '-decinf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: a UNK of UNK and a UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK tia UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: a UNK of UNK and a UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK tia UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: a UNK of UNK and a UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK tia UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK \n",
      "IN: inventory\n",
      "OUT: you are carrying a black rubber strap EOS \n",
      "IN: wait\n",
      "OUT: a UNK UNK EOS \n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line))\n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: you can hear the crashing and clanging of a sword fight coming from the tavern common EOS\n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: you can hear the crashing and clanging of a sword fight coming from the tavern common EOS\n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: you can hear the crashing and clanging of a sword fight coming from the tavern common EOS\n",
      "IN: inventory\n",
      "OUT: you EOS\n",
      "IN: wait\n",
      "OUT: you EOS\n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line))\n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(beam_decoder_lstm(encinf, decinf, input_seq, 5, vocab_size, max_output_len=50, best_only=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(None,), latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=1, decoder_depth=1, trainable_embeddings=False, attention=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "\n",
    "# load the 200-long model\n",
    "model_name = 'models/att_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "if os.path.isfile(model_name + '.h5'):\n",
    "    print('Loading weights...')\n",
    "    encinf.load_weights(model_name + '-encinf.h5')\n",
    "    decinf.load_weights(model_name + '-decinf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: examine me EOS \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: examine me EOS \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: examine me EOS \n",
      "IN: inventory\n",
      "OUT: you are not wearing the key EOS \n",
      "IN: wait\n",
      "OUT: wait EOS \n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line)) \n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50, attention=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: south EOS\n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: south EOS\n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: south EOS\n",
      "IN: inventory\n",
      "OUT: look EOS\n",
      "IN: wait\n",
      "OUT: wait EOS\n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line))\n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(beam_decoder_lstm(encinf, decinf, input_seq, 5, vocab_size, max_output_len=50, attention=True, best_only=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "# load the 50-long model\n",
    "model_name = 'models/att_seq2seq_20k_50_300d_1-1_LSTM'\n",
    "if os.path.isfile(model_name + '.h5'):\n",
    "    print('Loading weights...')\n",
    "    encinf.load_weights(model_name + '-encinf.h5')\n",
    "    decinf.load_weights(model_name + '-decinf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "             to be a large living room. examine table\n",
      "OUT: examine me EOS \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. look\n",
      "OUT: examine me EOS \n",
      "IN: You find yourself in a dimly lit room. Before you you can spot a table. \n",
      "On the other side of the room a poster hangs askew on the wall. To the east a door leads to what seems\n",
      "to be a large living room. east\n",
      "OUT: examine me EOS \n",
      "IN: inventory\n",
      "OUT: examine bag EOS \n",
      "IN: wait\n",
      "OUT: out EOS \n"
     ]
    }
   ],
   "source": [
    "for line in test_lines:\n",
    "    print('IN: {}'.format(line))\n",
    "    input_seq = prepare_input(line, tokenizer)\n",
    "    print('OUT: {}'.format(decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50, attention=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT: [(0.009560183054736715, 'inventory EOS'), (0.06043526970614366, 'east EOS'), (1.0097972336216878, 'examine me EOS'), (2.2212223625281133, 'get all EOS'), (3.144422467227716, 'take all EOS')]\n"
     ]
    }
   ],
   "source": [
    "line = test_lines[0]\n",
    "input_seq = prepare_input(line, tokenizer)\n",
    "print('OUT: {}'.format(beam_decoder_lstm(encinf, decinf, input_seq, 5, vocab_size, max_output_len=50, attention=True, best_only=False)))\n",
    "# for line in test_lines:\n",
    "#     print('IN: {}'.format(line))\n",
    "#     input_seq = prepare_input(line, tokenizer)\n",
    "#     print('OUT: {}'.format(beam_decoder_lstm(encinf, decinf, input_seq, 5, vocab_size, max_output_len=50, attention=True, best_only=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
