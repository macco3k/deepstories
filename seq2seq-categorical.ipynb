{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the followings:\n",
    "* http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "* http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/farizrahman4u/seq2seq\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "## TODO\n",
    "* ~~look into categorical representation~~\n",
    "* ~~look into the number of missing words over the total~~\n",
    "* ~~look into different models (attention, hierachical, etc.)~~\n",
    "* look into attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, LSTM, GRU, Dense, Masking, Embedding, Activation, TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "OUTPUT_PATH = 'output'\n",
    "punct = set(punctuation)\n",
    "file_list = sorted(glob.glob('data/parsed/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'GoogleNews-vectors-negative300.bin.gz'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load params\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'rb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'rb') as params_file:\n",
    "    data = pkl.load(data_file)\n",
    "    params = pkl.load(params_file)\n",
    "    tokenizer = params['tokenizer']\n",
    "    index_word = params['index_word']\n",
    "    word2embeddings = params['w2e']\n",
    "    embedding_matrix = params['W']\n",
    "    missing_words = params['missing_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300#w2v.vector_size\n",
    "eos_token = 'EOS'\n",
    "unk_token = 'UNK'\n",
    "eos_vector = np.ones((embedding_dim))\n",
    "unk_vector = np.zeros((embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(repl='', string=text, pattern='^> ') # remove starting caret, if any\n",
    "    text = re.sub(repl='\\g<1> \\g<2>', string=text, pattern='(\\w+)-(\\w+)') # compound words    \n",
    "    text = re.sub(repl=' ', string=text, pattern='-{2,}|\\s{2,}|[%s\\t\\n/]' % (''.join(punctuation)))\n",
    "#     text = re.sub(repl=' digits ', string=text, pattern='^\\d+$| \\d+| \\d+ ') # replace digits with a standard 'digits' word\n",
    "    return text\n",
    "\n",
    "def read_corpus(file_list):\n",
    "    corpus = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            print('read_corpus: processing [{}]'.format(file))\n",
    "            corpus.append(f.read())\n",
    "            \n",
    "    return corpus\n",
    "            \n",
    "def build_vocabulary(corpus, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words=num_words+1, oov_token=oov_token) # +1 for the oov token\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Fix keras' nasty behaviour. See https://github.com/keras-team/keras/issues/8092\n",
    "    # Only include words found in w2v\n",
    "    tokenizer.word_index = {w:i for  w,i in tokenizer.word_index.items() \n",
    "                                if   i <= num_words} # <= because tokenizer is 1 indexed (this will leave out UNK)\n",
    "    tokenizer.num_words = num_words + 2  # UNK + EOS\n",
    "    tokenizer.word_index[oov_token] = len(tokenizer.word_index) + 1\n",
    "    tokenizer.word_index[eos_token] = len(tokenizer.word_index) + 1\n",
    "    index_word = [None for i in range(len(tokenizer.word_index) + 1)]  # index is 1-based\n",
    "    for w,i in tokenizer.word_index.items():\n",
    "        index_word[i] = w\n",
    "    \n",
    "    return tokenizer, index_word\n",
    "\n",
    "def prepare_data(corpus, tokenizer):\n",
    "    # Still go through the files line by line, as we want to predict the next scene, \n",
    "    # not just the next sentence\n",
    "    data = []\n",
    "    for i, doc in enumerate(corpus):\n",
    "        doc_data = []\n",
    "        print('prepare_data: processing [{}]'.format(file_list[i]))\n",
    "        \n",
    "        for j, line in enumerate(doc.split('\\n')):\n",
    "            if len(line) == 0:\n",
    "                print('Line {} is empty. Replacing with \"empty line\".'.format(j+1))\n",
    "                line = 'empty line'\n",
    "\n",
    "            doc_data.append(tokenizer.texts_to_sequences([line])[0])\n",
    "\n",
    "        if len(doc_data) == 0:\n",
    "            print('File {} has no data'.format(file_list[i]))\n",
    "        else:\n",
    "            data.append(doc_data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def get_embeddings(word_index, w2v, unk_vector):\n",
    "    embedding_matrix=np.zeros(shape=(len(word_index)+2, w2v.vector_size))  # +2 as keras' tokenizer is 1-based\n",
    "    missing_words = []\n",
    "    for word,i in word_index.items():\n",
    "        if word not in w2v:\n",
    "            # Try to capitalize it\n",
    "            if word.capitalize() not in w2v:\n",
    "                missing_words.append(word)\n",
    "                embedding_matrix[i] = unk_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = w2v[word.capitalize()]\n",
    "        else:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "    \n",
    "    # add EOS token\n",
    "    embedding_matrix[-1] = eos_vector # keras' index the vocab starting from 1\n",
    "    embedding_matrix[-2] = unk_vector\n",
    "    return embedding_matrix, missing_words\n",
    "\n",
    "def get_embedding_matrix(word2embeddings):\n",
    "    embedding_dim = len(list(word2embeddings.values())[0])\n",
    "    embedding_matrix = np.zeros(shape=(len(word2embeddings)+2, embedding_dim)) # +2 as keras tokenizer is 1-based\n",
    "    for i, w in enumerate(word2embeddings): # keras' tokenizer index is 1-based\n",
    "        embedding_matrix[i+1] = word2embeddings[w]\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_corpus: processing [data/parsed/parsed-12heads.txt]\n",
      "read_corpus: processing [data/parsed/parsed-1893.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "read_corpus: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "read_corpus: processing [data/parsed/parsed-abno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acitw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-actofmurder.txt]\n",
      "read_corpus: processing [data/parsed/parsed-adverbum.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afdfr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afflicted.txt]\n",
      "read_corpus: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "read_corpus: processing [data/parsed/parsed-aotearoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-awakening.txt]\n",
      "read_corpus: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bellwater.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bestman.txt]\n",
      "read_corpus: processing [data/parsed/parsed-blindhouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bonaventure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bookvol.txt]\n",
      "read_corpus: processing [data/parsed/parsed-broadsides.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bryant.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-buddha.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cacophony.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "read_corpus: processing [data/parsed/parsed-childsplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chineseroom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cove.txt]\n",
      "read_corpus: processing [data/parsed/parsed-crescent.txt]\n",
      "read_corpus: processing [data/parsed/parsed-csbb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cull.txt]\n",
      "read_corpus: processing [data/parsed/parsed-death.txt]\n",
      "read_corpus: processing [data/parsed/parsed-defra.txt]\n",
      "read_corpus: processing [data/parsed/parsed-degeneracy.txt]\n",
      "read_corpus: processing [data/parsed/parsed-demoparty.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "read_corpus: processing [data/parsed/parsed-divis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-djinni.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dualtransform.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eatme.txt]\n",
      "read_corpus: processing [data/parsed/parsed-edifice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-electric.txt]\n",
      "read_corpus: processing [data/parsed/parsed-elysium.txt]\n",
      "read_corpus: processing [data/parsed/parsed-envcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eurydice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodydies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finetuned.txt]\n",
      "read_corpus: processing [data/parsed/parsed-firebird.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-floatpoint.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foofoo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-forachange.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foth.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fragileshells.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-galatea.txt]\n",
      "read_corpus: processing [data/parsed/parsed-gdc09.txt]\n",
      "read_corpus: processing [data/parsed/parsed-glowgrass.txt]\n",
      "read_corpus: processing [data/parsed/parsed-goldilocks.txt]\n",
      "read_corpus: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ground.txt]\n",
      "read_corpus: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-halothane.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hamper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-heroes.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hoosegow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1701.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1702.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1703.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1704.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-indigo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-inls.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp11.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-invisargo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacket4.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacqissick.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jfw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ka.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lethe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lmwh.txt]\n",
      "read_corpus: processing [data/parsed/parsed-loose.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lostpig.txt]\n",
      "read_corpus: processing [data/parsed/parsed-luminous.txt]\n",
      "read_corpus: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-marika.txt]\n",
      "read_corpus: processing [data/parsed/parsed-measure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mingsheng.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mite.txt]\n",
      "read_corpus: processing [data/parsed/parsed-monkfish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-moonlittower.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newernewyear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nordandbert.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oad.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-onehalf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-orevore.txt]\n",
      "read_corpus: processing [data/parsed/parsed-park.txt]\n",
      "read_corpus: processing [data/parsed/parsed-partyfoul.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pathway.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pepper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "read_corpus: processing [data/parsed/parsed-progressive1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-punkpoints.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rameses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-recluse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-represso.txt]\n",
      "read_corpus: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "read_corpus: processing [data/parsed/parsed-robot.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rogue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rover.txt]\n",
      "read_corpus: processing [data/parsed/parsed-samfortune.txt]\n",
      "read_corpus: processing [data/parsed/parsed-santaland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scavenger.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sequitur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shelter.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sherbet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-simplethefts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-six.txt]\n",
      "read_corpus: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "read_corpus: processing [data/parsed/parsed-snacktime.txt]\n",
      "read_corpus: processing [data/parsed/parsed-softfood.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sorcerer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spring.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssi.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssos.txt]\n",
      "read_corpus: processing [data/parsed/parsed-starborn.txt]\n",
      "read_corpus: processing [data/parsed/parsed-statue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suspended.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suvehnux.txt]\n",
      "read_corpus: processing [data/parsed/parsed-swigian.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tacofiction.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tapestry.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theone.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theoracle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thread.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "read_corpus: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tryst.txt]\n",
      "read_corpus: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unclezeb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-undertow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unipool.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unscientific.txt]\n",
      "read_corpus: processing [data/parsed/parsed-vagueness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-varkana.txt]\n",
      "read_corpus: processing [data/parsed/parsed-violet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wand.txt]\n",
      "read_corpus: processing [data/parsed/parsed-weapon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wedding.txt]\n",
      "read_corpus: processing [data/parsed/parsed-windjack.txt]\n",
      "read_corpus: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wishbringer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wizard.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wof-sa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "read_corpus: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yakshaving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yetifail.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0d4f9fd0bf0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(file_list)\n",
    "tokenizer, index_word = build_vocabulary(corpus, num_words=20000, oov_token=unk_token)\n",
    "embedding_matrix, missing_words = get_embeddings(tokenizer.word_index, w2v, unk_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "OOV token index: 20001\n",
      "EOS token index: 20002\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', tokenizer.num_words)\n",
    "print('OOV token index:', tokenizer.word_index[unk_token])\n",
    "print('EOS token index:', tokenizer.word_index[eos_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size: (20003, 300)\n",
      "Total triples: 131807\n",
      "Unique words found (UNK, EOS + vocab): 20002\n",
      "Of which missing words (no embeddings): 1557\n"
     ]
    }
   ],
   "source": [
    "# text = 'Sample sentence with a possible balabiut token and some 1984 plus sentry'\n",
    "# print(preprocess(text))\n",
    "# print(prepare_input(text, tokenizer))\n",
    "vocab_size = len(embedding_matrix)\n",
    "unk_index = tokenizer.word_index[unk_token]\n",
    "eos_index = unk_index+1\n",
    "print('Embedding matrix size:', embedding_matrix.shape)\n",
    "print('Total triples:', sum([(len(f)-3)//2 for f in data]))\n",
    "print('Unique words found (UNK, EOS + vocab):', len(tokenizer.word_index))\n",
    "print('Of which missing words (no embeddings):', len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-12heads.txt]\n",
      "prepare_data: processing [data/parsed/parsed-1893.txt]\n",
      "Line 1197 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "Line 1445 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "prepare_data: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "prepare_data: processing [data/parsed/parsed-abno.txt]\n",
      "Line 1217 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "Line 1825 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acitw.txt]\n",
      "prepare_data: processing [data/parsed/parsed-actofmurder.txt]\n",
      "Line 567 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-adverbum.txt]\n",
      "Line 1085 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-afdfr.txt]\n",
      "prepare_data: processing [data/parsed/parsed-afflicted.txt]\n",
      "prepare_data: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "prepare_data: processing [data/parsed/parsed-aotearoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-awakening.txt]\n",
      "prepare_data: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "Line 833 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bellwater.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bestman.txt]\n",
      "Line 1509 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-blindhouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bonaventure.txt]\n",
      "Line 759 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bookvol.txt]\n",
      "Line 393 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-broadsides.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bryant.txt]\n",
      "Line 995 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-buddha.txt]\n",
      "Line 661 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-cacophony.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "prepare_data: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "Line 871 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-childsplay.txt]\n",
      "Line 2191 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-chineseroom.txt]\n",
      "prepare_data: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cove.txt]\n",
      "prepare_data: processing [data/parsed/parsed-crescent.txt]\n",
      "prepare_data: processing [data/parsed/parsed-csbb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cull.txt]\n",
      "Line 2031 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-death.txt]\n",
      "prepare_data: processing [data/parsed/parsed-defra.txt]\n",
      "prepare_data: processing [data/parsed/parsed-degeneracy.txt]\n",
      "prepare_data: processing [data/parsed/parsed-demoparty.txt]\n",
      "Line 665 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "prepare_data: processing [data/parsed/parsed-divis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-djinni.txt]\n",
      "Line 891 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-dualtransform.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eas.txt]\n",
      "Line 363 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-eas2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eatme.txt]\n",
      "prepare_data: processing [data/parsed/parsed-edifice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-electric.txt]\n",
      "prepare_data: processing [data/parsed/parsed-elysium.txt]\n",
      "prepare_data: processing [data/parsed/parsed-envcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eurydice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-everybodydies.txt]\n",
      "Line 459 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fear.txt]\n",
      "Line 1897 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "prepare_data: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "Line 1723 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-finetuned.txt]\n",
      "prepare_data: processing [data/parsed/parsed-firebird.txt]\n",
      "Line 1877 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fish.txt]\n",
      "Line 2983 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-floatpoint.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foofoo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-forachange.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foth.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fragileshells.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "Line 1875 is empty. Replacing with \"empty line\".\n",
      "Line 2021 is empty. Replacing with \"empty line\".\n",
      "Line 2325 is empty. Replacing with \"empty line\".\n",
      "Line 2545 is empty. Replacing with \"empty line\".\n",
      "Line 3249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-galatea.txt]\n",
      "prepare_data: processing [data/parsed/parsed-gdc09.txt]\n",
      "prepare_data: processing [data/parsed/parsed-glowgrass.txt]\n",
      "prepare_data: processing [data/parsed/parsed-goldilocks.txt]\n",
      "prepare_data: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "Line 675 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ground.txt]\n",
      "prepare_data: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-halothane.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hamper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-heroes.txt]\n",
      "Line 2197 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hoosegow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1701.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1702.txt]\n",
      "Line 135 is empty. Replacing with \"empty line\".\n",
      "Line 139 is empty. Replacing with \"empty line\".\n",
      "Line 165 is empty. Replacing with \"empty line\".\n",
      "Line 201 is empty. Replacing with \"empty line\".\n",
      "Line 205 is empty. Replacing with \"empty line\".\n",
      "Line 209 is empty. Replacing with \"empty line\".\n",
      "Line 213 is empty. Replacing with \"empty line\".\n",
      "Line 217 is empty. Replacing with \"empty line\".\n",
      "Line 221 is empty. Replacing with \"empty line\".\n",
      "Line 225 is empty. Replacing with \"empty line\".\n",
      "Line 229 is empty. Replacing with \"empty line\".\n",
      "Line 233 is empty. Replacing with \"empty line\".\n",
      "Line 237 is empty. Replacing with \"empty line\".\n",
      "Line 241 is empty. Replacing with \"empty line\".\n",
      "Line 249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ic1703.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1704.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-indigo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-inls.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp11.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp2.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-invisargo.txt]\n",
      "Line 89 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacket4.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacqissick.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jfw.txt]\n",
      "Line 27 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ka.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "Line 539 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lethe.txt]\n",
      "prepare_data: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-lmwh.txt]\n",
      "prepare_data: processing [data/parsed/parsed-loose.txt]\n",
      "Line 675 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lostpig.txt]\n",
      "prepare_data: processing [data/parsed/parsed-luminous.txt]\n",
      "prepare_data: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "Line 843 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-marika.txt]\n",
      "prepare_data: processing [data/parsed/parsed-measure.txt]\n",
      "prepare_data: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mingsheng.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mite.txt]\n",
      "Line 495 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-monkfish.txt]\n",
      "Line 1249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-moonlittower.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "Line 249 is empty. Replacing with \"empty line\".\n",
      "Line 385 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-newernewyear.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nordandbert.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oad.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-onehalf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-orevore.txt]\n",
      "prepare_data: processing [data/parsed/parsed-park.txt]\n",
      "Line 791 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-partyfoul.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pathway.txt]\n",
      "Line 411 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pax.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2011.txt]\n",
      "Line 1563 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pepper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "Line 1057 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "Line 1727 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "Line 1091 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-progressive1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-punkpoints.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rameses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-recluse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-represso.txt]\n",
      "Line 1485 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "prepare_data: processing [data/parsed/parsed-robot.txt]\n",
      "Line 581 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-rogue.txt]\n",
      "Line 1007 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rover.txt]\n",
      "Line 1341 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-samfortune.txt]\n",
      "prepare_data: processing [data/parsed/parsed-santaland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scavenger.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sequitur.txt]\n",
      "prepare_data: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "Line 661 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-shelter.txt]\n",
      "Line 1353 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sherbet.txt]\n",
      "Line 4491 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-simplethefts.txt]\n",
      "Line 791 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-six.txt]\n",
      "Line 887 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "prepare_data: processing [data/parsed/parsed-snacktime.txt]\n",
      "Line 413 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-softfood.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sorcerer.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spring.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spur.txt]\n",
      "Line 2639 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ssi.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ssos.txt]\n",
      "prepare_data: processing [data/parsed/parsed-starborn.txt]\n",
      "Line 103 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-statue.txt]\n",
      "Line 503 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-stf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suspended.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suvehnux.txt]\n",
      "Line 1083 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-swigian.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tacofiction.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tangle.txt]\n",
      "Line 2267 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tangle2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tapestry.txt]\n",
      "Line 1267 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "Line 1235 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "Line 651 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theone.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theoracle.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theplay.txt]\n",
      "Line 1 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thohc1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thohc2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thread.txt]\n",
      "Line 1153 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "Line 939 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "Line 1421 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tryst.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "Line 1271 is empty. Replacing with \"empty line\".\n",
      "Line 1575 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-unclezeb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-undertow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unipool.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unscientific.txt]\n",
      "Line 4597 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-vagueness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-varkana.txt]\n",
      "prepare_data: processing [data/parsed/parsed-violet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wand.txt]\n",
      "prepare_data: processing [data/parsed/parsed-weapon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wedding.txt]\n",
      "Line 4547 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-windjack.txt]\n",
      "prepare_data: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wishbringer.txt]\n",
      "Line 651 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-wizard.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wof-sa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "prepare_data: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yakshaving.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yetifail.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save various objects for later reuse\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'wb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'wb') as params_file:\n",
    "    params = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'index_word': index_word,\n",
    "        'W': embedding_matrix,\n",
    "        'w2e': word2embeddings,\n",
    "        'missing_words': missing_words\n",
    "    }\n",
    "    pkl.dump(data, data_file)\n",
    "    pkl.dump(params, params_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(seq, n=3, step=1):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s[0],...s[n-1]), (s[0+skip_n],...,s[n-1+skip_n]), ...   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "\n",
    "    result = result[step:]\n",
    "    for elem in it:\n",
    "        result = result + (elem,)\n",
    "        if len(result) == n:\n",
    "            yield result\n",
    "            result = result[step:]\n",
    "\n",
    "def create_samples(data, test_split=0.1, shuffle=False, max_seq_length=None):    \n",
    "    samples = []\n",
    "    for i, play in enumerate(data):\n",
    "        if max_seq_length is not None:\n",
    "            chunks = [line[offset:offset+max_seq_length] \n",
    "                      for line in play \n",
    "                      for offset in range(0, len(line), max_seq_length)]\n",
    "        else:\n",
    "            chunks = play\n",
    "            \n",
    "        for scene, command, reply in window(chunks, n=3, step=2):\n",
    "#             if max_seq_length is not None:\n",
    "#                 sub_scenes  = [scene[offset:offset+max_seq_length]   for offset in range(0, len(scene),   max_seq_length)]\n",
    "#                 sub_cmds    = [command[offset:offset+max_seq_length] for offset in range(0, len(command), max_seq_length)]\n",
    "#                 sub_replies = [reply[offset:offset+max_seq_length]   for offset in range(0, len(reply),   max_seq_length)]\n",
    "                \n",
    "#                 nb_samples = \n",
    "#                 # sample a number of contextual sequences\n",
    "#                 scenes   = sub_scenes[np.random.choice(range(len(sub_scenes)), len(sub_scenes)//max_seq_length)]\n",
    "#                 commands = sub_cmds[np.random.choice(range(len(sub_cmds)), len(sub_cmds)//max_seq_length)]\n",
    "#                 replies   = sub_replies[np.random.choice(range(len(sub_replies)), len(sub_replies)//max_seq_length)]\n",
    "                \n",
    "                \n",
    "#             if len(command) > 10:\n",
    "#                 command_line = ' '.join([index_word[idx] for idx in command])\n",
    "#                 print('Found anomalous command for play {} [{}] with length {}: [{}]'.format(\n",
    "#                     i, os.path.basename(file_list[i]), len(command), command_line))\n",
    "                \n",
    "            samples.append((scene, command, reply))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(samples)\n",
    "        \n",
    "    if test_split is not None:\n",
    "        split = int((1-test_split) * len(samples))\n",
    "        train_samples = samples[:split]\n",
    "        test_samples = samples[split:]\n",
    "        return train_samples, test_samples\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch generator\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, vocab_size, batch_size=1, reverse_input=False, shuffle=True, max_seq_length=None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.vocab_size = vocab_size\n",
    "        self.reverse_input = reverse_input\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.UNK = unk_index\n",
    "        self.EOS = eos_index\n",
    "        self.PAD = 0\n",
    "        \n",
    "    def generate_batch(self): \n",
    "        # every three lines comprise a sample sequence where the first two items\n",
    "        # are the input and the last one is the output\n",
    "        i  = 1 # batch counter        \n",
    "        x_enc = []\n",
    "        x_dec = []\n",
    "        y  = []\n",
    "            \n",
    "        while True:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.data)\n",
    "            \n",
    "            for j, (scene, command, reply) in enumerate(self.data):\n",
    "                if self.reverse_input:\n",
    "                    scene = scene[::-1]\n",
    "                    \n",
    "                encoder_input  = np.array(scene + command)\n",
    "                decoder_input  = np.array([self.EOS] + reply)\n",
    "                decoder_output = np.array(to_categorical(reply + [self.EOS], self.vocab_size))\n",
    "                    \n",
    "                x_enc.append(encoder_input)\n",
    "                x_dec.append(decoder_input)\n",
    "                y.append(decoder_output)\n",
    "                \n",
    "                if i == self.batch_size or j == len(data):\n",
    "                    if self.batch_size > 1:\n",
    "                        # pad and return the batch\n",
    "                        x_enc = sequence.pad_sequences(x_enc, padding='post', value=self.PAD, maxlen=self.max_seq_length)\n",
    "                        x_dec = sequence.pad_sequences(x_dec, padding='post', value=self.PAD, maxlen=self.max_seq_length)    \n",
    "                        y     = sequence.pad_sequences(y, padding='post', value=self.PAD, maxlen=self.max_seq_length)\n",
    "\n",
    "                    x_out, y_out = [np.array(x_enc), np.array(x_dec)], np.array(y)\n",
    "                    \n",
    "                    i = 1\n",
    "                    x_enc = []\n",
    "                    x_dec = []\n",
    "                    y = []\n",
    "\n",
    "                    yield (x_out, y_out)\n",
    "                else:\n",
    "                    i += 1 # next sample per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "#         self._uxpb = TimeDistributed(Dense(units=self.units, weights=[self.U_a, self.b_a],\n",
    "#                                              input_dim=self.input_dim)(self.x_seq))\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models_lstm(src_vocab_size, embedding_matrix, input_shape, dst_vocab_size=None, embedding_dim=300, latent_dim=128, \n",
    "                       mask_value=0, trainable_embeddings=False, encoder_depth=1, decoder_depth=1, attention=False):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_size is None:\n",
    "        dst_vocab_size = src_vocab_size\n",
    "        \n",
    "    encoder_inputs = Input(shape=input_shape) # timesteps, features (integer)\n",
    "    decoder_inputs = Input(shape=input_shape)\n",
    "    inputs = [encoder_inputs, decoder_inputs]\n",
    "    \n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    encoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(encoder_masking)\n",
    "    decoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(decoder_masking)\n",
    "    \n",
    "    ######## ENCODER ########\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    for _ in range(encoder_depth-1):  # DEPTH (the encoder need not be shared, so we can just instantiate a new LSTM object)\n",
    "        encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_sequences=True, return_state=True)(encoder_outputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    \n",
    "    ######## DECODER ########\n",
    "    decoder_layers = []  # keep track of deep layers\n",
    "    \n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_lstm = LSTM(units=latent_dim, return_sequences=True, return_state=True)        \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    if attention:\n",
    "        decoder_outputs = AttentionDecoder(units=latent_dim, output_dim=embedding_dim)(decoder_outputs)\n",
    "        \n",
    "    for _ in range(decoder_depth-1):  # DEPTH\n",
    "        lstm = LSTM(units=latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_layers.append(lstm)\n",
    "        decoder_outputs, _, _ = lstm(decoder_outputs)\n",
    "        \n",
    "    decoder_dense = Dense(dst_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "    model = Model(inputs, decoder_outputs)\n",
    "    \n",
    "    ####### INFERENCE ENCODER #######\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    ####### INFERENCE DECODER #######\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    if attention:\n",
    "        decoder_outputs = AttentionDecoder(units=latent_dim, output_dim=embedding_dim)(decoder_outputs)\n",
    "        \n",
    "    decoder_states = [state_h, state_c]    \n",
    "    for d in range(decoder_depth - 1):  # DEPTH\n",
    "        decoder_outputs, state_h, state_c = decoder_layers[d](decoder_outputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, val_loss, color=None, fname=None, legend=False):\n",
    "        N = len(loss)\n",
    "        train_loss_plt, = plt.plot(range(0, N), loss)\n",
    "        val_loss_plt, = plt.plot(range(0, N), val_loss)\n",
    "        \n",
    "        if color is not None:\n",
    "            plt.setp(train_loss_plt, color=color, linestyle='-')\n",
    "            plt.setp(val_loss_plt, color=color, linestyle='--')\n",
    "            \n",
    "        if legend:\n",
    "            plt.legend((train_loss_plt, val_loss_plt), ('train loss', 'val loss'))\n",
    "        \n",
    "        if fname is not None:\n",
    "            plt.savefig(fname)\n",
    "        \n",
    "        return [train_loss_plt, val_loss_plt]\n",
    "\n",
    "def plot(losses, fname=None):        \n",
    "    lines = []\n",
    "    names = []\n",
    "    colors = [plt.cm.gist_ncar(i) for i in np.linspace(0, 1, len(losses))]\n",
    "    for i, (loss, val_loss) in enumerate(losses):\n",
    "        lines.extend(plot_loss(loss, val_loss, colors[i]))\n",
    "        names.extend(['{} loss'.format(i+1), '{} val loss'.format(i+1)])\n",
    "    \n",
    "    plt.legend(lines, names)\n",
    "    \n",
    "    if fname is not None:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class InferenceModelsCheckpoint(Callback):\n",
    "    def __init__(self, models, filepath, monitor='val_loss', verbose=0):\n",
    "        self.encoder, self.decoder = models\n",
    "        self.monitor = monitor\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.best = np.Inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current < self.best:\n",
    "            filepath = self.filepath\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
    "                \n",
    "            self.encoder.save(filepath + '-encinf.h5')\n",
    "            self.decoder.save(filepath + '-decinf.h5')                \n",
    "            self.best = current\n",
    "\n",
    "def train_model(models, train_samples, batch_size, epochs=10, shuffle=True, n_folds=None, train_split=None, \n",
    "                model_name=None, max_seq_length=None):\n",
    "    assert not (n_folds is not None and train_split is not None), ValueError('Either n_folds or train_split should be specified, but not both.')\n",
    "    assert not (n_folds is None and train_split is None), ValueError('Either n_folds or train_split must be specified.')   \n",
    "    \n",
    "    def _run_model(train, val):\n",
    "        train_generator = BatchGenerator(train, batch_size=batch_size, vocab_size=vocab_size, reverse_input=True, \n",
    "                                         shuffle=shuffle, max_seq_length=max_seq_length)\n",
    "        val_generator = BatchGenerator(val, batch_size=batch_size, vocab_size=vocab_size, reverse_input=True, \n",
    "                                       shuffle=shuffle, max_seq_length=max_seq_length)\n",
    "        \n",
    "        # utils callbacks\n",
    "        checkpointer = ModelCheckpoint(filepath=model_name + '.h5', verbose=1, save_best_only=True)\n",
    "        seq2seq_cp = InferenceModelsCheckpoint(filepath=model_name, verbose=1, models=(encoder, decoder))\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, mode='auto', \n",
    "                                      min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "        early_stop = EarlyStopping(patience=1, min_delta=0.0001, verbose=1)\n",
    "        callbacks = [checkpointer, seq2seq_cp, reduce_lr, early_stop]\n",
    "        \n",
    "        # actual train\n",
    "        history = model.fit_generator(train_generator.generate_batch(), steps_per_epoch=len(train)//batch_size, epochs=epochs, \n",
    "                            validation_data=val_generator.generate_batch(), validation_steps=len(val)//batch_size,\n",
    "                            callbacks=callbacks)\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    model, encoder, decoder = models\n",
    "    train_samples = np.array(train_samples)\n",
    "    losses = []  # keep track of train and val loss for each fold\n",
    "    \n",
    "    if n_folds is None:\n",
    "        train, val = train_test_split(train_samples, train_size=train_split, shuffle=shuffle)\n",
    "        \n",
    "        history = _run_model(train, val)\n",
    "        # plot current losses\n",
    "        plot_loss(history.history['loss'], history.history['val_loss'], fname=model_name + '.png')\n",
    "    else:  \n",
    "        kfold = KFold(n_folds, shuffle=shuffle)\n",
    "        for i, (train, val) in enumerate(kfold.split(train_samples)):\n",
    "            print(\"Running fold {}/{}\".format(i+1, n_folds))\n",
    "\n",
    "            model_file = model_name + '-fold-{}'.format(i+1)\n",
    "            history = _run_model(train_samples[train], train_samples[val])\n",
    "\n",
    "            # record losses for the final plot\n",
    "            losses.append((history.history['loss'], history.history['val_loss']))\n",
    "\n",
    "        # plot losses for all folds\n",
    "        plot(losses, model_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_55 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_56 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_39 (Masking)            (None, 200)          0           input_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_40 (Masking)            (None, 200)          0           input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)        (None, 200, 300)     6000900     masking_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)        (None, 200, 300)     6000900     masking_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  [(None, 200, 300), ( 721200      embedding_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  [(None, 200, 300), ( 721200      embedding_40[0][0]               \n",
      "                                                                 lstm_38[0][1]                    \n",
      "                                                                 lstm_38[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDecoder (AttentionDeco (None, 200, 300)     1351800     lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 200, 20003)   6020903     AttentionDecoder[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 20,816,903\n",
      "Trainable params: 8,815,103\n",
      "Non-trainable params: 12,001,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 200\n",
    "model_name = 'basic_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(max_seq_length,), latent_dim=300, \n",
    "                                           embedding_matrix=embedding_matrix, encoder_depth=1, decoder_depth=1, \n",
    "                                           trainable_embeddings=False, attention=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 132060\n",
      "Test samples: 1334\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = create_samples(data, max_seq_length=max_seq_length, test_split=0.01)\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 135/3920 [>.............................] - ETA: 3:26:16 - loss: 1.0626 - categorical_accuracy: 0.0047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-b643fe251dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n\u001b[0;32m----> 4\u001b[0;31m             train_split=0.95, model_name=model_name, max_seq_length=max_seq_length)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-a74f24a1df5d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(models, train_samples, batch_size, epochs, shuffle, n_folds, train_split, model_name, max_seq_length)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# plot current losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-a74f24a1df5d>\u001b[0m in \u001b[0;36m_run_model\u001b[0;34m(train, val)\u001b[0m\n\u001b[1;32m     42\u001b[0m         history = model.fit_generator(train_generator.generate_batch(), steps_per_epoch=len(train)//batch_size, epochs=epochs, \n\u001b[1;32m     43\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'basic_seq2seq_20k_100_300d_1-1_LSTM'\n",
    "model_file = model_name + '.h5'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=1, decoder_depth=1, trainable_embeddings=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, test_samples = create_samples(data, max_seq_length=100, test_split=0.01)\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 50\n",
    "model_name = 'basic_seq2seq_20k_50_300d_1-1_LSTM'\n",
    "model_file = model_name + '.h5'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, input_shape=(max_seq_length,), latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=1, decoder_depth=1, trainable_embeddings=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 157731\n",
      "Test samples: 1594\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = create_samples(data, max_seq_length=50, test_split=0.01)\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1170/1170 [==============================] - 1752s 1s/step - loss: 2.0053 - categorical_accuracy: 0.0594 - val_loss: 1.8167 - val_categorical_accuracy: 0.0740\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.81666, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_41 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_40/while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'lstm_40/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_41 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_61:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'input_62:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "1170/1170 [==============================] - 1748s 1s/step - loss: 1.7406 - categorical_accuracy: 0.0823 - val_loss: 1.6766 - val_categorical_accuracy: 0.0887\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.81666 to 1.67655, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00002: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 3/5\n",
      "1170/1170 [==============================] - 1744s 1s/step - loss: 1.6239 - categorical_accuracy: 0.0950 - val_loss: 1.5972 - val_categorical_accuracy: 0.0990\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.67655 to 1.59717, saving model to basic_seq2seq_20k_50_300d_1-1_LSTM.h5\n",
      "\n",
      "Epoch 00003: saving model to basic_seq2seq_20k_50_300d_1-1_LSTM\n",
      "Epoch 4/5\n",
      " 880/1170 [=====================>........] - ETA: 6:53 - loss: 1.5478 - categorical_accuracy: 0.1032"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "train_model((model, encinf, decinf), train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name, max_seq_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(input_text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([preprocess(input_text)])[0]\n",
    "\n",
    "def decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0, 0] = eos_index\n",
    "    \n",
    "    decoder_inputs = [target_seq] + states_value\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 1 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output, h, c = decinf.predict(decoder_inputs)\n",
    "        sampled_word_index = np.argmax(output[0, -1, :])  # batch, time, features\n",
    "        sampled_word = index_word[sampled_word_index]\n",
    "\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if sampled_word == eos_token or i > max_output_len:\n",
    "            stop_condition = True     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_word_index\n",
    "        i += 1\n",
    "        \n",
    "        # Update states\n",
    "        decoder_inputs = [target_seq, h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def prepare_input(input_text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([preprocess(input_text)])[0]\n",
    "\n",
    "def decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0, 0] = eos_index\n",
    "    \n",
    "    decoder_inputs = [target_seq] + states_value\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 1 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output, h, c = decinf.predict(decoder_inputs)\n",
    "        sampled_word_index = np.argmax(output[0, -1, :])  # batch, time, features\n",
    "        sampled_word = index_word[sampled_word_index]\n",
    "\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if sampled_word == eos_token or i > max_output_len:\n",
    "            stop_condition = True     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_word_index\n",
    "        i += 1\n",
    "        \n",
    "        # Update states\n",
    "        decoder_inputs = [target_seq, h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def beam_decoder_lstm(encinf, decinf, input_seq, k, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index.\n",
    "    target_seq[0,0] = eos_index\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''    \n",
    "    decoded_sequences = []\n",
    "    \n",
    "    # Init Beam Array\n",
    "    beams = [(target_seq, states_value, target_seq, 1.0)]\n",
    "    \n",
    "    output_len = 1 # number of sampled words\n",
    "    while not stop_condition:        \n",
    "        all_candidates = []\n",
    "        for i in range(len(beams)):  # for each beam, try to append the next predicted word\n",
    "            # Get the values of the beam\n",
    "            target_seq, states_value, candidate_seq, score = beams[i]\n",
    "            \n",
    "            # predict the next word and create candidate list\n",
    "            output, h, c = decinf.predict([target_seq] + states_value)\n",
    "            candidate_score_list = output[0][0]\n",
    "            \n",
    "#             argmax = np.argmax(candidate_score_list)\n",
    "#             print('argmax: {} with value {}'.format(index_word[int(argmax)], candidate_score_list[argmax]))\n",
    "            \n",
    "            for j in range(len(candidate_score_list)):  # for each possible word, compute the score               \n",
    "                # Add each candidate to the target sequence.\n",
    "                new_candidate_seq = np.append(candidate_seq, [[j]], axis=1)\n",
    "                candidate_beam = [np.array([[j]]), [h, c],  new_candidate_seq, score*-np.log(candidate_score_list[j])]\n",
    "                all_candidates.append(candidate_beam)\n",
    "\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[-1])\n",
    "        # select k best\n",
    "        beams = ordered[:k]\n",
    "        \n",
    "        # if a beam has the index of the eos, \n",
    "        # 1. Add the beam to decoded_sequences, \n",
    "        # 2. Remove it from beams \n",
    "        # 3. Lower number of beams (k)\n",
    "        for bindex, beam in enumerate(beams):\n",
    "            if beam[0][0][-1] == eos_index:\n",
    "                print('EOS reached for beam ', bindex)\n",
    "                decoded_sequences.append(beam)\n",
    "                del beams[bindex]\n",
    "                k -= 1\n",
    "                   \n",
    "        # Exit condition: either each beam got eos or hit max length\n",
    "        if len(beams) == 0 or output_len > max_output_len:\n",
    "            print('Max output length reached')\n",
    "            # Add the beams that did not have eos so far to the decoded_sequences\n",
    "            decoded_sequences += beams\n",
    "            stop_condition = True \n",
    "\n",
    "        output_len += 1\n",
    "        \n",
    "    decoded_sentences = [(seq[2], seq[-1]) for seq in decoded_sequences]\n",
    "    decoded_sentences = [(score, [index_word[int(idx)] if idx > 0 else 'NONE' for idx in seq[0]]) for seq, score in decoded_sentences]\n",
    "    decoded_sentences = [(score, ' '.join(sentence)) for score, sentence in decoded_sentences]\n",
    "\n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = \"\"\"You find yourself in a dimly lit room. Before you you can spot a table. On the other side of the room\n",
    "a poster hangs askew on the wall. examine poster.\"\"\"\n",
    "# test_line = \"\"\"inventory\"\"\"\n",
    "input_seq = prepare_input(test_line, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence encoded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'you are in a small clearing in a well marked forest path that extends to the east and south behind the house is a path leading down a path leads northwest and a path leads southeast and a path leads southeast and a passage leads southeast a small passage leads southeast '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "#                              encoder_depth=1, decoder_depth=1, trainable_embeddings=False)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "\n",
    "# # load the 200-long model\n",
    "# model_name = 'basic_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "# if os.path.isfile(model_name + '.h5'):\n",
    "#     model.load_weights(model_name + '.h5')\n",
    "#     encinf.load_weights(model_name + '-encinf.h5')\n",
    "#     decinf.load_weights(model_name + '-decinf.h5')\n",
    "decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence encoded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['UNK chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly sirens sirens sirens sirens chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly sirens sirens sirens sirens chilly chilly chilly chilly chilly',\n",
       " 'UNK chilly never chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly sirens sirens sirens sirens chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly chilly sirens sirens sirens sirens chilly chilly chilly chilly chilly']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_decoder_lstm(encinf, decinf, input_seq, 2, vocab_size, max_output_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
