{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the followings:\n",
    "* http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "* http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/farizrahman4u/seq2seq\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "## TODO\n",
    "* ~~look into categorical representation~~\n",
    "* ~~look into the number of missing words over the total~~\n",
    "* look into different training data generators (e.g. simple sentence2sentence)\n",
    "* look into different models (attention, hierachical, etc.)\n",
    "* look into character-level representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "# import seq2seq\n",
    "\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "from nltk import corpus, stem\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Masking, Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "OUTPUT_PATH = 'output'\n",
    "\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, w2v):\n",
    "    cont = Contractions(w2v_model=w2v)\n",
    "    return cont.expand_texts(text=text, precise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim = w2v.vector_size\n",
    "eos_vector = np.ones((vocab_dim))\n",
    "unk_vector = np.zeros((vocab_dim))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(repl=' ', string=text, pattern='-')\n",
    "    return re.sub(repl='', string=text, pattern='[{}\\n\\t\\\\\\\\]'.format(''.join(punctuation)))\n",
    "\n",
    "def build_vocabulary(file_list, w2v):\n",
    "    idx2word = set() # only count unique words\n",
    "    missing_words = set()\n",
    "    for file in file_list:\n",
    "        print('build_vocabulary: processing [{}]'.format(file))\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for i,line in enumerate(f):\n",
    "                line = preprocess(line)\n",
    "                if len(line) == 0:\n",
    "                    print('Line {} is empty. Skipping it.'.format(i+1))\n",
    "                    continue\n",
    "                \n",
    "                for word in line.split(' '):\n",
    "                    # skip words without embeddings. They'll be assigned the <UNK> token\n",
    "                    if len(word) > 0:\n",
    "                        if word in w2v:\n",
    "                            idx2word.add(word)\n",
    "                        else:\n",
    "                            missing_words.add(word)\n",
    "                        \n",
    "    missing_words = sorted(list(missing_words))\n",
    "    idx2word = sorted(list(idx2word))\n",
    "    idx2word.insert(0, '<EOS>')\n",
    "    idx2word.insert(1, '<UNK>')\n",
    "    word2idx = {w:i for i,w in enumerate(idx2word)}\n",
    "    # skip EOS and UNK when looking up word embeddings\n",
    "    word2embeddings = {**{'<EOS>': eos_vector, '<UNK>': unk_vector}, **{w:w2v[w] for w in idx2word[2:]}}\n",
    "    return idx2word, word2idx, word2embeddings, missing_words\n",
    "\n",
    "def prepare_data(file_list, word2idx):\n",
    "    vocab_size = len(word2idx)    \n",
    "    data = []\n",
    "    for file in file_list:\n",
    "        print('prepare_data: processing [{}]'.format(file))\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            file_data = []\n",
    "            for i,line in enumerate(f):\n",
    "                line = preprocess(line)\n",
    "                if len(line) == 0:\n",
    "                    print('Line {} is empty. Skipping it.'.format(i+1))\n",
    "                    continue\n",
    "                # return the integer representation of the sentence\n",
    "                file_data.append([word2idx[w] if w in word2idx else word2idx['<UNK>'] for w in line.split(' ')])\n",
    "        data.append(file_data)\n",
    "    return data\n",
    "                                 \n",
    "def get_embedding_matrix(word2embeddings):\n",
    "    embedding_dim = len(list(word2embeddings.values())[0])\n",
    "    embedding_matrix = np.zeros(shape=(len(word2embeddings), embedding_dim))\n",
    "    for i, w in enumerate(word2embeddings):\n",
    "        embedding_matrix[i] = word2embeddings[w]\n",
    "    return embedding_matrix\n",
    "                                \n",
    "def prepare_input(input_text, word2embeddings):\n",
    "    return [word2embeddings[word] if word in word2embeddings else unk_vector for word in preprocess(input_text).split(' ') if len(word) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocabulary: processing [data/parsed\\parsed-12heads.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-1893.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-20160221-thesueno-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-20160221-thesueno.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-20160327-unrealcity-lifeonmars.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-69krakatoa.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-905-shrapnel.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-abno.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-acg-crossbow.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-acitw.txt]\n",
      "Line 171 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-actofmurder.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-adverbum.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-afdfr.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-afflicted.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-allthingsdevours.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-aotearoa.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-awakening.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-beingandrewplotkin.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bellwater.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bestman.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-blindhouse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bonaventure.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bookvol.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-broadsides.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bryant.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-bse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-buddha.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cacophony.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cc-fangvclaw-flooby.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-chefjanitor.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-childsplay.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-chineseroom.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-clipperbeta.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cokeandspeed.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cove.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-crescent.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-csbb.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-cull.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-death.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-defra.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-degeneracy.txt]\n",
      "Line 248 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-demoparty.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dialcforcupcakes-103014-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dialcforcupcakes-103014.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-divis.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-djinni.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dramaqueen.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-dualtransform.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eas.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eas2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eatme.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-edifice.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-electric.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-elysium.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-envcomp.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-escapade.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-eurydice.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-everybodydies.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-everybodylovesaparade.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fdb-tin-folkar.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fear.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fifteenminutes-100214.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-finalexam20160124.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-finetuned.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-firebird.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fish.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-floatpoint.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-foofoo.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-forachange.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-foth.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-fragileshells.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ft-n-awe.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-galatea.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-gdc09.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-glowgrass.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-goldilocks.txt]\n",
      "Line 842 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-groovebillygoat.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ground.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-guesstheverb.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-halothane.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hamper.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-heroes.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hollywoodvisionary-part2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hoosegow.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-houseofdreamofmoon.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-hunterindarkness.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1701.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1702.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1703.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic1704.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ic2010-1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-indigo.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-inls.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp08a.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp11.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-introcomp2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-invisargo.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jabberwocky.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jacket4.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jacqissick.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-jfw.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ka.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-laidoff-1May2016.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-laidoff-subrosa-1May2016.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-lethe.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-littlebluemen.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-lmwh.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-loose.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-lostpig.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-luminous.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-maincourse-iamthelaw.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-marika.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-measure.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-metamorphoses.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-mingsheng.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-mite.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-monkfish.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-moonlittower.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-mugglestudies.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newernewyear.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed-jan16a.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed-jan16b.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed-jan9.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-newyearsspeed08.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-nightfall.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-nightfall2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-nordandbert.txt]\n",
      "Line 980 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-oad.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-oneeyeopen.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-onehalf.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocabulary: processing [data/parsed\\parsed-orevore.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-park.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-partyfoul.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pathway.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pax.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pax2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pax2011.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pepper.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-photograph.txr.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-photograph.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-plan6-waker.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-plunderedhearts.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pnnsi1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-pnnsi2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-primrose-edited.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-progressive1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-punkpoints.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-rameses.txt]\n",
      "Line 98 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-recluse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-represso.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-revolution-buny.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-robot.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-rogue.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-roofed-alien.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-rover.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-samfortune.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-santaland.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-saugusnet-a.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-saugusnet-b.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-saugusnet-c.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-scaryhouseamulet.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-scavenger.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sequitur.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-shadowsonthemirror.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-shelter.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sherbet.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-simplethefts.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sinsagainstmimesis.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-six.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-smittenkittens.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-snacktime.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-softfood.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-sorcerer.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-spring.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-spur.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ssi.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ssos.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-starborn.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-statue.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-stewie-escapade.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-stf.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-subrosa-1and8may2016.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-suspended.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-suvehnux.txt]\n",
      "Line 128 is empty. Skipping it.\n",
      "Line 152 is empty. Skipping it.\n",
      "Line 166 is empty. Skipping it.\n",
      "Line 212 is empty. Skipping it.\n",
      "Line 246 is empty. Skipping it.\n",
      "Line 275 is empty. Skipping it.\n",
      "Line 320 is empty. Skipping it.\n",
      "Line 398 is empty. Skipping it.\n",
      "Line 506 is empty. Skipping it.\n",
      "Line 542 is empty. Skipping it.\n",
      "Line 551 is empty. Skipping it.\n",
      "Line 645 is empty. Skipping it.\n",
      "Line 770 is empty. Skipping it.\n",
      "Line 939 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-swigian.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tacofiction.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tangle.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tangle2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tapestry.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tdmamoom.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thanksgiving.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-themultidimensionalthief.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-theone.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-theoracle.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-theplay.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thohc1.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thohc2.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-thread.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tokyo-mouse.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-toonesiabandit.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-transparent-100914.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-tryst.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-turkeyspeeds.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-unclezeb.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-undertow.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-unipool.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-unscientific.txt]\n",
      "Line 48 is empty. Skipping it.\n",
      "Line 50 is empty. Skipping it.\n",
      "Line 97 is empty. Skipping it.\n",
      "Line 3161 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-vagueness.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-varkana.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-violet.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wand.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-weapon.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wedding.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-weishaupt.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-windjack.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-winterwonderland.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wishbringer.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-wizard.txt]\n",
      "Line 366 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-wof-sa.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-ww-jingo-madrigals.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-xyzzy2011.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-yakshaving.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-yetifail.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-zork-i-2016-04-0310.txt]\n",
      "Line 2013 is empty. Skipping it.\n",
      "build_vocabulary: processing [data/parsed\\parsed-zork1+troll-2016Ap0310.txt]\n",
      "build_vocabulary: processing [data/parsed\\parsed-zorkII.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-12heads.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-1893.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-20160221-thesueno-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-20160221-thesueno.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-20160327-unrealcity-lifeonmars.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-69krakatoa.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-905-shrapnel.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-abno.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-acg-crossbow.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-acitw.txt]\n",
      "Line 171 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-actofmurder.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-adverbum.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-afdfr.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-afflicted.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-allthingsdevours.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-aotearoa.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-awakening.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-beingandrewplotkin.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bellwater.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bestman.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-blindhouse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bonaventure.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bookvol.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-broadsides.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bryant.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-bse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-buddha.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cacophony.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cc-fangvclaw-flooby.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-chefjanitor.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-childsplay.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-chineseroom.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed\\parsed-clipperbeta.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cokeandspeed.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cove.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-crescent.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-csbb.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-cull.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-death.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-defra.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-degeneracy.txt]\n",
      "Line 248 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-demoparty.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dialcforcupcakes-103014-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dialcforcupcakes-103014.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-divis.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-djinni.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dramaqueen.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-dualtransform.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eas.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eas2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eatme.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-edifice.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-electric.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-elysium.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-envcomp.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-escapade.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-eurydice.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-everybodydies.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-everybodylovesaparade.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fdb-tin-folkar.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fear.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fifteenminutes-100214.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-finalexam20160124.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-finetuned.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-firebird.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fish.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-floatpoint.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-foofoo.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-forachange.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-foth.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-fragileshells.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ft-n-awe.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-galatea.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-gdc09.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-glowgrass.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-goldilocks.txt]\n",
      "Line 842 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-groovebillygoat.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ground.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-guesstheverb.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-halothane.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hamper.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-heroes.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hollywoodvisionary-part2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hoosegow.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-houseofdreamofmoon.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-hunterindarkness.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1701.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1702.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1703.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic1704.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ic2010-1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-indigo.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-inls.txt]\n",
      "Line 88 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp08a.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp11.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-introcomp2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-invisargo.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jabberwocky.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jacket4.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jacqissick.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-jfw.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ka.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-laidoff-1May2016.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-laidoff-subrosa-1May2016.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-lethe.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-littlebluemen.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-lmwh.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-loose.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-lostpig.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-luminous.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-maincourse-iamthelaw.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-marika.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-measure.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-metamorphoses.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-mingsheng.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-mite.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-monkfish.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-moonlittower.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-mugglestudies.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newernewyear.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed-jan16a.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed-jan16b.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed-jan9.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-newyearsspeed08.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-nightfall.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-nightfall2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-nordandbert.txt]\n",
      "Line 980 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-oad.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-oneeyeopen.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-onehalf.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-orevore.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-park.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-partyfoul.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pathway.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pax.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pax2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pax2011.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pepper.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-photograph.txr.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-photograph.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-plan6-waker.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-plunderedhearts.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pnnsi1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-pnnsi2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-primrose-edited.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-progressive1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-punkpoints.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-rameses.txt]\n",
      "Line 98 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-recluse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-represso.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-revolution-buny.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-robot.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-rogue.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-roofed-alien.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-rover.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-samfortune.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-santaland.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-saugusnet-a.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-saugusnet-b.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-saugusnet-c.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-scaryhouseamulet.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-scavenger.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sequitur.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-shadowsonthemirror.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed\\parsed-shelter.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sherbet.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-simplethefts.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sinsagainstmimesis.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-six.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-smittenkittens.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-snacktime.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-softfood.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-sorcerer.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-spring.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-spur.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ssi.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ssos.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-starborn.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-statue.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-stewie-escapade.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-stf.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-subrosa-1and8may2016.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-suspended.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-suvehnux.txt]\n",
      "Line 128 is empty. Skipping it.\n",
      "Line 152 is empty. Skipping it.\n",
      "Line 166 is empty. Skipping it.\n",
      "Line 212 is empty. Skipping it.\n",
      "Line 246 is empty. Skipping it.\n",
      "Line 275 is empty. Skipping it.\n",
      "Line 320 is empty. Skipping it.\n",
      "Line 398 is empty. Skipping it.\n",
      "Line 506 is empty. Skipping it.\n",
      "Line 542 is empty. Skipping it.\n",
      "Line 551 is empty. Skipping it.\n",
      "Line 645 is empty. Skipping it.\n",
      "Line 770 is empty. Skipping it.\n",
      "Line 939 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-swigian.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tacofiction.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tangle.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tangle2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tapestry.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tdmamoom.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thanksgiving.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-themultidimensionalthief.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-theone.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-theoracle.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-theplay.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thohc1.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thohc2.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-thread.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tokyo-mouse.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-toonesiabandit.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-transparent-100914.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-tryst.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-turkeyspeeds.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-unclezeb.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-undertow.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-unipool.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-unscientific.txt]\n",
      "Line 48 is empty. Skipping it.\n",
      "Line 50 is empty. Skipping it.\n",
      "Line 97 is empty. Skipping it.\n",
      "Line 3161 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-vagueness.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-varkana.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-violet.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wand.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-weapon.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wedding.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-weishaupt.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-windjack.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-winterwonderland.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wishbringer.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-wizard.txt]\n",
      "Line 366 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-wof-sa.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-ww-jingo-madrigals.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-xyzzy2011.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-yakshaving.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-yetifail.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-zork-i-2016-04-0310.txt]\n",
      "Line 2013 is empty. Skipping it.\n",
      "prepare_data: processing [data/parsed\\parsed-zork1+troll-2016Ap0310.txt]\n",
      "prepare_data: processing [data/parsed\\parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob('data/parsed/*.txt')\n",
    "idx2word, word2idx, word2embeddings, missing_words = build_vocabulary(file_list, w2v)\n",
    "data = prepare_data(file_list, word2idx)\n",
    "vocab_size = len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words found (<UNK>, <EOS> + embeddings): 34160\n",
      "Missing words (no embeddings): 9120\n"
     ]
    }
   ],
   "source": [
    "print('Unique words found (<UNK>, <EOS> + embeddings):', len(word2idx))\n",
    "print('Missing words (no embeddings):', len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix(word2embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch generator\n",
    "class BatchGenerator(object):            \n",
    "    def __init__(self, data, batch_size=1):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.UNK = word2idx['<UNK>']\n",
    "        self.EOS = word2idx['<EOS>']\n",
    "        self.PAD = 0\n",
    "        self.eye = np.eye(len(word2idx))\n",
    "        \n",
    "    def generate_batch(self): \n",
    "        def window(seq, n=3, step=1):\n",
    "            \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "            \"   s -> (s[0],...s[n-1]), (s[0+skip_n],...,s[n-1+skip_n]), ...   \"\n",
    "            it = iter(seq)\n",
    "            result = tuple(islice(it, n))\n",
    "            if len(result) == n:\n",
    "                yield result    \n",
    "\n",
    "            result = result[step:]\n",
    "            for elem in it:\n",
    "                result = result + (elem,)\n",
    "                if len(result) == n:\n",
    "                    yield result\n",
    "                    result = result[step:]\n",
    "                    \n",
    "        def to_categorical(sentence):\n",
    "            return [self.eye[wordidx] for wordidx in sentence]\n",
    "                    \n",
    "        # every three lines comprise a sample sequence where the first two items\n",
    "        # are the input and the last one is the output\n",
    "        i  = 1 # batch counter        \n",
    "        x_enc = []\n",
    "        x_dec = []\n",
    "        y  = []\n",
    "        while True:\n",
    "            for play in self.data:\n",
    "                j  = 1 # sample counter\n",
    "                for scene, command, reply in window(play, n=3, step=2):\n",
    "                    scene_command = scene + command\n",
    "                    \n",
    "                    encoder_input  = np.array(scene_command + [self.EOS])\n",
    "                    decoder_input  = np.array(reply)\n",
    "                    decoder_output = np.array(to_categorical(reply[1:] + [self.EOS]))\n",
    "                \n",
    "                    x_enc.append(encoder_input)\n",
    "                    x_dec.append(decoder_input)\n",
    "                    y.append(decoder_output)\n",
    "                    if i == self.batch_size or j == len(play):\n",
    "                        if self.batch_size > 1:\n",
    "                            # pad and return the batch\n",
    "                            x_enc = sequence.pad_sequences(x_enc, padding='post', value=self.PAD)\n",
    "                            x_dec = sequence.pad_sequences(x_dec, padding='post', value=self.PAD)\n",
    "                            y     = sequence.pad_sequences(y,     padding='post', value=self.PAD) \n",
    "\n",
    "                        x_out, y_out = [np.array(x_enc.copy()), np.array(x_dec.copy())], np.array(y.copy())\n",
    "\n",
    "                        i  = 1\n",
    "                        x_enc = []\n",
    "                        x_dec = []\n",
    "                        y  = []\n",
    "\n",
    "                        yield (x_out, y_out)\n",
    "                    else:\n",
    "                        i += 1 # next sample per batch\n",
    "                    j += 1 # next sample\n",
    "                    \n",
    "            # no more data, just stop the generator\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(src_vocab_dim, dst_vocab_dim=None, latent_dim=300, mask_value=0, embedding_matrix=None):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_dim is None:\n",
    "        dst_vocab_dim = src_vocab_dim\n",
    "        \n",
    "    encoder_inputs = Input(shape=(None,)) # timesteps, features (one-hot encoding)\n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        encoder_masking = Embedding(input_dim=src_vocab_dim, output_dim=latent_dim, weights=[embedding_matrix], \n",
    "                                   trainable=False)(encoder_masking)\n",
    "        \n",
    "    encoder = LSTM(units=latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_masking)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    if embedding_matrix is not None:\n",
    "        decoder_masking = Embedding(input_dim=src_vocab_dim, output_dim=latent_dim, weights=[embedding_matrix], \n",
    "                                   trainable=False)(decoder_masking)\n",
    "    \n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_masking, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(dst_vocab_dim, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_masking, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________\n",
      "Layer (type)                        Output Shape            Param #      Connected to                         \n",
      "==============================================================================================================\n",
      "input_1 (InputLayer)                (None, None)            0                                                 \n",
      "______________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                (None, None)            0                                                 \n",
      "______________________________________________________________________________________________________________\n",
      "masking_1 (Masking)                 (None, None)            0            input_1[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "masking_2 (Masking)                 (None, None)            0            input_2[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)             (None, None, 300)       10248000     masking_1[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)             (None, None, 300)       10248000     masking_2[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                       [(None, 300), (None, 30 721200       embedding_1[0][0]                    \n",
      "______________________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                       [(None, None, 300), (No 721200       embedding_2[0][0]                    \n",
      "                                                                         lstm_1[0][1]                         \n",
      "                                                                         lstm_1[0][2]                         \n",
      "______________________________________________________________________________________________________________\n",
      "dense_1 (Dense)                     (None, None, 34160)     10282160     lstm_2[0][0]                         \n",
      "==============================================================================================================\n",
      "Total params: 32,220,560\n",
      "Trainable params: 11,724,560\n",
      "Non-trainable params: 20,496,000\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import keras.backend as K\n",
    "\n",
    "# def cos_distance(y_true, y_pred):\n",
    "#     y_true = K.l2_normalize(y_true, axis=-1)\n",
    "#     y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "#     return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\n",
    "\n",
    "model, encinf, decinf = define_models(src_vocab_dim=vocab_size, embedding_matrix=embedding_matrix)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "   4/1000 [..............................] - ETA: 39:50 - loss: 4.3611"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[16,237,34160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg = Neg[T=DT_FLOAT, _class=[\"loc:@loss/dense_1_loss/truediv\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/truediv)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_111 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3128_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg', defined at:\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-0f4ce427ae4c>\", line 2, in <module>\n    model.fit_generator(batch_generator.generate_batch(), steps_per_epoch=1000, epochs=25)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 2080, in fit_generator\n    self._make_train_function()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py\", line 244, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2515, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 611, in gradients\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 377, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 611, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 864, in _RealDivGrad\n    grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3123, in _neg\n    \"Neg\", x=x, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'loss/dense_1_loss/truediv', defined at:\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 7 identical lines from previous traceback]\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-fe748bc89ac9>\", line 9, in <module>\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 830, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 429, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 69, in categorical_crossentropy\n    return K.categorical_crossentropy(y_true, y_pred)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3004, in categorical_crossentropy\n    True)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 934, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1030, in _truediv_python3\n    return gen_math_ops._real_div(x, y, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3757, in _real_div\n    \"RealDiv\", x=x, y=y, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,237,34160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg = Neg[T=DT_FLOAT, _class=[\"loc:@loss/dense_1_loss/truediv\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/truediv)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_111 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3128_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,237,34160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg = Neg[T=DT_FLOAT, _class=[\"loc:@loss/dense_1_loss/truediv\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/truediv)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_111 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3128_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0f4ce427ae4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,237,34160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg = Neg[T=DT_FLOAT, _class=[\"loc:@loss/dense_1_loss/truediv\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/truediv)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_111 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3128_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg', defined at:\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-0f4ce427ae4c>\", line 2, in <module>\n    model.fit_generator(batch_generator.generate_batch(), steps_per_epoch=1000, epochs=25)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 2080, in fit_generator\n    self._make_train_function()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py\", line 244, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2515, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 611, in gradients\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 377, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 611, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 864, in _RealDivGrad\n    grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3123, in _neg\n    \"Neg\", x=x, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'loss/dense_1_loss/truediv', defined at:\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 7 identical lines from previous traceback]\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-fe748bc89ac9>\", line 9, in <module>\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 830, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\", line 429, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\losses.py\", line 69, in categorical_crossentropy\n    return K.categorical_crossentropy(y_true, y_pred)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3004, in categorical_crossentropy\n    True)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 934, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1030, in _truediv_python3\n    return gen_math_ops._real_div(x, y, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3757, in _real_div\n    \"RealDiv\", x=x, y=y, name=name)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,237,34160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training/RMSprop/gradients/loss/dense_1_loss/truediv_grad/Neg = Neg[T=DT_FLOAT, _class=[\"loc:@loss/dense_1_loss/truediv\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/truediv)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_111 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3128_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "batch_generator = BatchGenerator(data, batch_size=16)\n",
    "model.fit_generator(batch_generator.generate_batch(), steps_per_epoch=1000, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def decode_sequence(input_seq, vocab_dim, eos_vector, tol=1e-2, max_output_len=200):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, vocab_dim))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = eos_vector\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 0 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output_embedding, h, c = decinf.predict([target_seq] + states_value)\n",
    "        output_embedding = output_embedding[0,0,:]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        eos_distance = distance.cosine(output_embedding, eos_vector)\n",
    "        if eos_distance < tol or i > max_output_len:\n",
    "            print(eos_distance, tol)\n",
    "            stop_condition = True\n",
    "            \n",
    "        # Sample a token\n",
    "        if distance.cosine(output_embedding, unk_vector) < tol:\n",
    "            sampled_word = unk_vector\n",
    "        else:\n",
    "            sampled_word = w2v.most_similar(positive=[output_embedding], topn=1)\n",
    "        decoded_sentence += sampled_word[0][0] + ' '     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, vocab_dim))\n",
    "        target_seq[0, 0] = output_embedding\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = 'every muscle in your body strains, and you feel the grinding of faraway pulleys as the portcullis slowly lifts open. at last the heavy machinery catches, and you relax. east'\n",
    "input_seq = np.array(prepare_input(test_line, word2embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 300)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = np.reshape(input_seq, (1, 30, 300))\n",
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniele\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\scipy\\spatial\\distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2657773540215731 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Perrine_Bridge butterflyer butterflyer butterflyer UniCredit UniCredit UniCredit UniCredit UniCredit UniCredit Woodbourne_Correctional_Facility Woodbourne_Correctional_Facility Woodbourne_Correctional_Facility Woodbourne_Correctional_Facility unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon unsuspecting_sockeye_salmon replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers replacing_eager_earmarkers Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett Nicole_Haislett '"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(input_seq, w2v.vector_size, eos_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml (3.6)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
