{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the followings:\n",
    "* http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "* http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/farizrahman4u/seq2seq\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "## TODO\n",
    "* ~~look into categorical representation~~\n",
    "* ~~look into the number of missing words over the total~~\n",
    "* look into different training data generators (e.g. simple sentence2sentence)\n",
    "* look into different models (attention, hierachical, etc.)\n",
    "* look into character-level representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, LSTM, GRU, Dense, Masking, Embedding, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "OUTPUT_PATH = 'output'\n",
    "punct = set(punctuation)\n",
    "file_list = sorted(glob.glob('data/parsed/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'GoogleNews-vectors-negative300.bin.gz'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data_20k.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5f46202f10a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# re-load params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_20k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'params_20k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparams_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/data_20k.pkl'"
     ]
    }
   ],
   "source": [
    "# re-load params\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'rb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'rb') as params_file:\n",
    "    data = pkl.load(data_file)\n",
    "    params = pkl.load(params_file)\n",
    "    tokenizer = params['tokenizer']\n",
    "    index_word = params['index_word']\n",
    "    word2embeddings = params['w2e']\n",
    "    embedding_matrix = params['W']\n",
    "    missing_words = params['missing_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300#w2v.vector_size\n",
    "eos_token = 'EOS'\n",
    "unk_token = 'UNK'\n",
    "eos_vector = np.ones((embedding_dim))\n",
    "unk_vector = np.zeros((embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(repl='', string=text, pattern='^> ') # remove starting caret, if any\n",
    "    text = re.sub(repl='\\g<1> \\g<2>', string=text, pattern='(\\w+)-(\\w+)') # compound words    \n",
    "    text = re.sub(repl=' ', string=text, pattern='-{2,}|\\s{2,}|[%s\\t\\n/]' % (''.join(punctuation)))\n",
    "#     text = re.sub(repl=' digits ', string=text, pattern='^\\d+$| \\d+| \\d+ ') # replace digits with a standard 'digits' word\n",
    "    return text\n",
    "\n",
    "def read_corpus(file_list):\n",
    "    corpus = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            print('read_corpus: processing [{}]'.format(file))\n",
    "            corpus.append(f.read())\n",
    "            \n",
    "    return corpus\n",
    "            \n",
    "def build_vocabulary(corpus, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words=num_words+1, oov_token=oov_token) # +1 for the oov token\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Fix keras' nasty behaviour. See https://github.com/keras-team/keras/issues/8092\n",
    "    # Only include words found in w2v\n",
    "    tokenizer.word_index = {w:i for  w,i in tokenizer.word_index.items() \n",
    "                                if   i <= num_words} # <= because tokenizer is 1 indexed\n",
    "    tokenizer.num_words = num_words + 1\n",
    "    tokenizer.word_index[tokenizer.oov_token] = len(tokenizer.word_index) + 1   \n",
    "    index_word = [None for i in range(len(tokenizer.word_index)+1)]\n",
    "    for w,i in tokenizer.word_index.items():\n",
    "        index_word[i] = w\n",
    "        \n",
    "    return tokenizer, index_word\n",
    "\n",
    "def prepare_data(corpus, tokenizer):\n",
    "    # Still go through the files line by line, as we want to predict the next scene, \n",
    "    # not just the next sentence\n",
    "    data = []\n",
    "    for i, doc in enumerate(corpus):\n",
    "        doc_data = []\n",
    "        print('prepare_data: processing [{}]'.format(file_list[i]))\n",
    "        \n",
    "        for j, line in enumerate(doc.split('\\n')):\n",
    "            if len(line) == 0:\n",
    "                print('Line {} is empty. Replacing with \"empty line\".'.format(j+1))\n",
    "                line = 'empty line'\n",
    "\n",
    "            doc_data.append(tokenizer.texts_to_sequences([line])[0])\n",
    "\n",
    "        if len(doc_data) == 0:\n",
    "            print('File {} has no data'.format(file_list[i]))\n",
    "        else:\n",
    "            data.append(doc_data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def get_embeddings(word_index, w2v, unk_vector):\n",
    "    embedding_matrix=np.zeros(shape=(len(word_index)+2, w2v.vector_size))  # +2 as keras' tokenizer is 1-based\n",
    "    missing_words = []\n",
    "    for word,i in word_index.items():\n",
    "        if word not in w2v:\n",
    "            # Try to capitalize it\n",
    "            if word.capitalize() not in w2v:\n",
    "                missing_words.append(word)\n",
    "                embedding_matrix[i] = unk_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = w2v[word.capitalize()]\n",
    "        else:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "    \n",
    "    # add EOS token\n",
    "    embedding_matrix[-1] = eos_vector # keras' index the vocab starting from 1\n",
    "    embedding_matrix[-2] = unk_vector\n",
    "    return embedding_matrix, missing_words\n",
    "\n",
    "def get_embedding_matrix(word2embeddings):\n",
    "    embedding_dim = len(list(word2embeddings.values())[0])\n",
    "    embedding_matrix = np.zeros(shape=(len(word2embeddings)+2, embedding_dim)) # +2 as keras tokenizer is 1-based\n",
    "    for i, w in enumerate(word2embeddings): # keras' tokenizer index is 1-based\n",
    "        embedding_matrix[i+1] = word2embeddings[w]\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_corpus: processing [data/parsed/parsed-12heads.txt]\n",
      "read_corpus: processing [data/parsed/parsed-1893.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "read_corpus: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "read_corpus: processing [data/parsed/parsed-abno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acitw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-actofmurder.txt]\n",
      "read_corpus: processing [data/parsed/parsed-adverbum.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afdfr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afflicted.txt]\n",
      "read_corpus: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "read_corpus: processing [data/parsed/parsed-aotearoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-awakening.txt]\n",
      "read_corpus: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bellwater.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bestman.txt]\n",
      "read_corpus: processing [data/parsed/parsed-blindhouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bonaventure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bookvol.txt]\n",
      "read_corpus: processing [data/parsed/parsed-broadsides.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bryant.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-buddha.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cacophony.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "read_corpus: processing [data/parsed/parsed-childsplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chineseroom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cove.txt]\n",
      "read_corpus: processing [data/parsed/parsed-crescent.txt]\n",
      "read_corpus: processing [data/parsed/parsed-csbb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cull.txt]\n",
      "read_corpus: processing [data/parsed/parsed-death.txt]\n",
      "read_corpus: processing [data/parsed/parsed-defra.txt]\n",
      "read_corpus: processing [data/parsed/parsed-degeneracy.txt]\n",
      "read_corpus: processing [data/parsed/parsed-demoparty.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "read_corpus: processing [data/parsed/parsed-divis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-djinni.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dualtransform.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eatme.txt]\n",
      "read_corpus: processing [data/parsed/parsed-edifice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-electric.txt]\n",
      "read_corpus: processing [data/parsed/parsed-elysium.txt]\n",
      "read_corpus: processing [data/parsed/parsed-envcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eurydice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodydies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finetuned.txt]\n",
      "read_corpus: processing [data/parsed/parsed-firebird.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-floatpoint.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foofoo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-forachange.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foth.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fragileshells.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-galatea.txt]\n",
      "read_corpus: processing [data/parsed/parsed-gdc09.txt]\n",
      "read_corpus: processing [data/parsed/parsed-glowgrass.txt]\n",
      "read_corpus: processing [data/parsed/parsed-goldilocks.txt]\n",
      "read_corpus: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ground.txt]\n",
      "read_corpus: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-halothane.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hamper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-heroes.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hoosegow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1701.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1702.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1703.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1704.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-indigo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-inls.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp11.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-invisargo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacket4.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacqissick.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jfw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ka.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lethe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lmwh.txt]\n",
      "read_corpus: processing [data/parsed/parsed-loose.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lostpig.txt]\n",
      "read_corpus: processing [data/parsed/parsed-luminous.txt]\n",
      "read_corpus: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-marika.txt]\n",
      "read_corpus: processing [data/parsed/parsed-measure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mingsheng.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mite.txt]\n",
      "read_corpus: processing [data/parsed/parsed-monkfish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-moonlittower.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newernewyear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nordandbert.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oad.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-onehalf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-orevore.txt]\n",
      "read_corpus: processing [data/parsed/parsed-park.txt]\n",
      "read_corpus: processing [data/parsed/parsed-partyfoul.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pathway.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pepper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "read_corpus: processing [data/parsed/parsed-progressive1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-punkpoints.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rameses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-recluse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-represso.txt]\n",
      "read_corpus: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "read_corpus: processing [data/parsed/parsed-robot.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rogue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rover.txt]\n",
      "read_corpus: processing [data/parsed/parsed-samfortune.txt]\n",
      "read_corpus: processing [data/parsed/parsed-santaland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scavenger.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sequitur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shelter.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sherbet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-simplethefts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-six.txt]\n",
      "read_corpus: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "read_corpus: processing [data/parsed/parsed-snacktime.txt]\n",
      "read_corpus: processing [data/parsed/parsed-softfood.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sorcerer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spring.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssi.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssos.txt]\n",
      "read_corpus: processing [data/parsed/parsed-starborn.txt]\n",
      "read_corpus: processing [data/parsed/parsed-statue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suspended.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suvehnux.txt]\n",
      "read_corpus: processing [data/parsed/parsed-swigian.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tacofiction.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tapestry.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theone.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theoracle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thread.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "read_corpus: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tryst.txt]\n",
      "read_corpus: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unclezeb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-undertow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unipool.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unscientific.txt]\n",
      "read_corpus: processing [data/parsed/parsed-vagueness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-varkana.txt]\n",
      "read_corpus: processing [data/parsed/parsed-violet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wand.txt]\n",
      "read_corpus: processing [data/parsed/parsed-weapon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wedding.txt]\n",
      "read_corpus: processing [data/parsed/parsed-windjack.txt]\n",
      "read_corpus: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wishbringer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wizard.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wof-sa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "read_corpus: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yakshaving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yetifail.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(file_list)\n",
    "tokenizer, index_word = build_vocabulary(corpus, num_words=20000, oov_token=unk_token)\n",
    "embedding_matrix, missing_words = get_embeddings(tokenizer.word_index, w2v, unk_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20001\n",
      "OOV token index: 20001\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', tokenizer.num_words)\n",
    "print('OOV token index:', tokenizer.word_index[unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size: (20003, 300)\n",
      "Total triples: 131807\n",
      "Unique words found (UNK, EOS + vocab): 20002\n",
      "Of which missing words (no embeddings): 1557\n"
     ]
    }
   ],
   "source": [
    "# text = 'Sample sentence with a possible balabiut token and some 1984 plus sentry'\n",
    "# print(preprocess(text))\n",
    "# print(prepare_input(text, tokenizer))\n",
    "vocab_size = len(embedding_matrix)\n",
    "unk_index = tokenizer.word_index[unk_token]\n",
    "eos_index = unk_index+1\n",
    "print('Embedding matrix size:', embedding_matrix.shape)\n",
    "print('Total triples:', sum([(len(f)-3)//2 for f in data]))\n",
    "print('Unique words found (UNK, EOS + vocab):', len(tokenizer.word_index)+1)\n",
    "print('Of which missing words (no embeddings):', len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-12heads.txt]\n",
      "prepare_data: processing [data/parsed/parsed-1893.txt]\n",
      "Line 1197 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "Line 1445 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "prepare_data: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "prepare_data: processing [data/parsed/parsed-abno.txt]\n",
      "Line 1217 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "Line 1825 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acitw.txt]\n",
      "prepare_data: processing [data/parsed/parsed-actofmurder.txt]\n",
      "Line 567 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-adverbum.txt]\n",
      "Line 1085 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-afdfr.txt]\n",
      "prepare_data: processing [data/parsed/parsed-afflicted.txt]\n",
      "prepare_data: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "prepare_data: processing [data/parsed/parsed-aotearoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-awakening.txt]\n",
      "prepare_data: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "Line 833 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bellwater.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bestman.txt]\n",
      "Line 1509 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-blindhouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bonaventure.txt]\n",
      "Line 759 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bookvol.txt]\n",
      "Line 393 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-broadsides.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bryant.txt]\n",
      "Line 995 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-buddha.txt]\n",
      "Line 661 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-cacophony.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "prepare_data: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "Line 871 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-childsplay.txt]\n",
      "Line 2191 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-chineseroom.txt]\n",
      "prepare_data: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cove.txt]\n",
      "prepare_data: processing [data/parsed/parsed-crescent.txt]\n",
      "prepare_data: processing [data/parsed/parsed-csbb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cull.txt]\n",
      "Line 2031 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-death.txt]\n",
      "prepare_data: processing [data/parsed/parsed-defra.txt]\n",
      "prepare_data: processing [data/parsed/parsed-degeneracy.txt]\n",
      "prepare_data: processing [data/parsed/parsed-demoparty.txt]\n",
      "Line 665 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "prepare_data: processing [data/parsed/parsed-divis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-djinni.txt]\n",
      "Line 891 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-dualtransform.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eas.txt]\n",
      "Line 363 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-eas2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eatme.txt]\n",
      "prepare_data: processing [data/parsed/parsed-edifice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-electric.txt]\n",
      "prepare_data: processing [data/parsed/parsed-elysium.txt]\n",
      "prepare_data: processing [data/parsed/parsed-envcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eurydice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-everybodydies.txt]\n",
      "Line 459 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fear.txt]\n",
      "Line 1897 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "prepare_data: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "Line 1723 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-finetuned.txt]\n",
      "prepare_data: processing [data/parsed/parsed-firebird.txt]\n",
      "Line 1877 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fish.txt]\n",
      "Line 2983 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-floatpoint.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foofoo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-forachange.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foth.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fragileshells.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "Line 1875 is empty. Replacing with \"empty line\".\n",
      "Line 2021 is empty. Replacing with \"empty line\".\n",
      "Line 2325 is empty. Replacing with \"empty line\".\n",
      "Line 2545 is empty. Replacing with \"empty line\".\n",
      "Line 3249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-galatea.txt]\n",
      "prepare_data: processing [data/parsed/parsed-gdc09.txt]\n",
      "prepare_data: processing [data/parsed/parsed-glowgrass.txt]\n",
      "prepare_data: processing [data/parsed/parsed-goldilocks.txt]\n",
      "prepare_data: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "Line 675 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ground.txt]\n",
      "prepare_data: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-halothane.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hamper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-heroes.txt]\n",
      "Line 2197 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hoosegow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1701.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1702.txt]\n",
      "Line 135 is empty. Replacing with \"empty line\".\n",
      "Line 139 is empty. Replacing with \"empty line\".\n",
      "Line 165 is empty. Replacing with \"empty line\".\n",
      "Line 201 is empty. Replacing with \"empty line\".\n",
      "Line 205 is empty. Replacing with \"empty line\".\n",
      "Line 209 is empty. Replacing with \"empty line\".\n",
      "Line 213 is empty. Replacing with \"empty line\".\n",
      "Line 217 is empty. Replacing with \"empty line\".\n",
      "Line 221 is empty. Replacing with \"empty line\".\n",
      "Line 225 is empty. Replacing with \"empty line\".\n",
      "Line 229 is empty. Replacing with \"empty line\".\n",
      "Line 233 is empty. Replacing with \"empty line\".\n",
      "Line 237 is empty. Replacing with \"empty line\".\n",
      "Line 241 is empty. Replacing with \"empty line\".\n",
      "Line 249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ic1703.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1704.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-indigo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-inls.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp11.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-invisargo.txt]\n",
      "Line 89 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacket4.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacqissick.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-jfw.txt]\n",
      "Line 27 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ka.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "Line 539 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lethe.txt]\n",
      "prepare_data: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-lmwh.txt]\n",
      "prepare_data: processing [data/parsed/parsed-loose.txt]\n",
      "Line 675 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lostpig.txt]\n",
      "prepare_data: processing [data/parsed/parsed-luminous.txt]\n",
      "prepare_data: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "Line 843 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-marika.txt]\n",
      "prepare_data: processing [data/parsed/parsed-measure.txt]\n",
      "prepare_data: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mingsheng.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mite.txt]\n",
      "Line 495 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-monkfish.txt]\n",
      "Line 1249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-moonlittower.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "Line 249 is empty. Replacing with \"empty line\".\n",
      "Line 385 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-newernewyear.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nordandbert.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oad.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-onehalf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-orevore.txt]\n",
      "prepare_data: processing [data/parsed/parsed-park.txt]\n",
      "Line 791 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-partyfoul.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pathway.txt]\n",
      "Line 411 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pax.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2011.txt]\n",
      "Line 1563 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pepper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "Line 1057 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "Line 1727 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "Line 1091 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-progressive1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-punkpoints.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rameses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-recluse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-represso.txt]\n",
      "Line 1485 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "prepare_data: processing [data/parsed/parsed-robot.txt]\n",
      "Line 581 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-rogue.txt]\n",
      "Line 1007 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rover.txt]\n",
      "Line 1341 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-samfortune.txt]\n",
      "prepare_data: processing [data/parsed/parsed-santaland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scavenger.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sequitur.txt]\n",
      "prepare_data: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "Line 661 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-shelter.txt]\n",
      "Line 1353 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sherbet.txt]\n",
      "Line 4491 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-simplethefts.txt]\n",
      "Line 791 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-six.txt]\n",
      "Line 887 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "prepare_data: processing [data/parsed/parsed-snacktime.txt]\n",
      "Line 413 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-softfood.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sorcerer.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spring.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spur.txt]\n",
      "Line 2639 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ssi.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ssos.txt]\n",
      "prepare_data: processing [data/parsed/parsed-starborn.txt]\n",
      "Line 103 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-statue.txt]\n",
      "Line 503 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-stf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suspended.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suvehnux.txt]\n",
      "Line 1083 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-swigian.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tacofiction.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tangle.txt]\n",
      "Line 2267 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tangle2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tapestry.txt]\n",
      "Line 1267 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "Line 1235 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "Line 651 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theone.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theoracle.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theplay.txt]\n",
      "Line 1 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thohc1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thohc2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thread.txt]\n",
      "Line 1153 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "Line 939 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "Line 1421 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tryst.txt]\n",
      "prepare_data: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "Line 1271 is empty. Replacing with \"empty line\".\n",
      "Line 1575 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-unclezeb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-undertow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unipool.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unscientific.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 4597 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-vagueness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-varkana.txt]\n",
      "prepare_data: processing [data/parsed/parsed-violet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wand.txt]\n",
      "prepare_data: processing [data/parsed/parsed-weapon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wedding.txt]\n",
      "Line 4547 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-windjack.txt]\n",
      "prepare_data: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wishbringer.txt]\n",
      "Line 651 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-wizard.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wof-sa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "prepare_data: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yakshaving.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yetifail.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save various objects for later reuse\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'wb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'wb') as params_file:\n",
    "    params = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'index_word': index_word,\n",
    "        'W': embedding_matrix,\n",
    "        'w2e': word2embeddings,\n",
    "        'missing_words': missing_words\n",
    "    }\n",
    "    pkl.dump(data, data_file)\n",
    "    pkl.dump(params, params_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(seq, n=3, step=1):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s[0],...s[n-1]), (s[0+skip_n],...,s[n-1+skip_n]), ...   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "\n",
    "    result = result[step:]\n",
    "    for elem in it:\n",
    "        result = result + (elem,)\n",
    "        if len(result) == n:\n",
    "            yield result\n",
    "            result = result[step:]\n",
    "\n",
    "def create_samples(data, test_split=0.1, shuffle=False, max_seq_length=None):    \n",
    "    samples = []\n",
    "    for i, play in enumerate(data):\n",
    "        if max_seq_length is not None:\n",
    "            chunks = [line[offset:offset+max_seq_length] \n",
    "                      for line in play \n",
    "                      for offset in range(0, len(line), max_seq_length)]\n",
    "        else:\n",
    "            chunks = play\n",
    "            \n",
    "        for scene, command, reply in window(chunks, n=3, step=2):\n",
    "#             if max_seq_length is not None:\n",
    "#                 sub_scenes  = [scene[offset:offset+max_seq_length]   for offset in range(0, len(scene),   max_seq_length)]\n",
    "#                 sub_cmds    = [command[offset:offset+max_seq_length] for offset in range(0, len(command), max_seq_length)]\n",
    "#                 sub_replies = [reply[offset:offset+max_seq_length]   for offset in range(0, len(reply),   max_seq_length)]\n",
    "                \n",
    "#                 nb_samples = \n",
    "#                 # sample a number of contextual sequences\n",
    "#                 scenes   = sub_scenes[np.random.choice(range(len(sub_scenes)), len(sub_scenes)//max_seq_length)]\n",
    "#                 commands = sub_cmds[np.random.choice(range(len(sub_cmds)), len(sub_cmds)//max_seq_length)]\n",
    "#                 replies   = sub_replies[np.random.choice(range(len(sub_replies)), len(sub_replies)//max_seq_length)]\n",
    "                \n",
    "                \n",
    "#             if len(command) > 10:\n",
    "#                 command_line = ' '.join([index_word[idx] for idx in command])\n",
    "#                 print('Found anomalous command for play {} [{}] with length {}: [{}]'.format(\n",
    "#                     i, os.path.basename(file_list[i]), len(command), command_line))\n",
    "                \n",
    "            samples.append((scene, command, reply))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(samples)\n",
    "        \n",
    "    if test_split is not None:\n",
    "        split = int((1-test_split) * len(samples))\n",
    "        train_samples = samples[:split]\n",
    "        test_samples = samples[split:]\n",
    "        return train_samples, test_samples\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch generator\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, vocab_size, batch_size=1, reverse_input=True, shuffle=True):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.vocab_size = vocab_size\n",
    "        self.reverse_input = reverse_input\n",
    "        self.UNK = unk_index\n",
    "        self.EOS = eos_index\n",
    "        self.PAD = 0\n",
    "        \n",
    "    def generate_batch(self): \n",
    "        # every three lines comprise a sample sequence where the first two items\n",
    "        # are the input and the last one is the output\n",
    "        i  = 1 # batch counter        \n",
    "        x_enc = []\n",
    "        x_dec = []\n",
    "        y  = []\n",
    "            \n",
    "        while True:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.data)\n",
    "            \n",
    "            for j, (scene, command, reply) in enumerate(self.data):\n",
    "                if self.reverse_input:\n",
    "                    scene = scene[::-1]\n",
    "                    \n",
    "                encoder_input  = np.array(scene + command)\n",
    "                decoder_input  = np.array([self.EOS] + reply)\n",
    "                decoder_output = np.array(to_categorical(reply + [self.EOS], self.vocab_size))\n",
    "                    \n",
    "                x_enc.append(encoder_input)\n",
    "                x_dec.append(decoder_input)\n",
    "                y.append(decoder_output)\n",
    "                \n",
    "                if i == self.batch_size or j == len(data):\n",
    "                    if self.batch_size > 1:\n",
    "                        # pad and return the batch\n",
    "                        x_enc = sequence.pad_sequences(x_enc, padding='post', value=self.PAD)\n",
    "                        x_dec = sequence.pad_sequences(x_dec, padding='post', value=self.PAD)    \n",
    "                        y     = sequence.pad_sequences(y, padding='post', value=self.PAD)\n",
    "\n",
    "                    x_out, y_out = [np.array(x_enc), np.array(x_dec)], np.array(y)\n",
    "                    \n",
    "                    i = 1\n",
    "                    x_enc = []\n",
    "                    x_dec = []\n",
    "                    y = []\n",
    "\n",
    "                    yield (x_out, y_out)\n",
    "                else:\n",
    "                    i += 1 # next sample per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models_lstm(src_vocab_size, embedding_matrix, dst_vocab_size=None, embedding_dim=300, latent_dim=128, \n",
    "                       mask_value=0, trainable_embeddings=False, encoder_depth=1, decoder_depth=1):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_size is None:\n",
    "        dst_vocab_size = src_vocab_size\n",
    "        \n",
    "    encoder_inputs = Input(shape=(None,)) # timesteps, features (integer)\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    inputs = [encoder_inputs, decoder_inputs]\n",
    "    \n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    encoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(encoder_masking)\n",
    "    decoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(decoder_masking)\n",
    "    \n",
    "    ######## ENCODER ########\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    for _ in range(encoder_depth-1):  # DEPTH\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    \n",
    "    ######## DECODER ########\n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_lstm = LSTM(units=latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    for _ in range(decoder_depth-1):  # DEPTH\n",
    "        decoder_outputs, _, _ = decoder_lstm(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "    \n",
    "    decoder_dense = Dense(dst_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "    model = Model(inputs, decoder_outputs)\n",
    "    \n",
    "    ####### INFERENCE ENCODER #######\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    ####### INFERENCE DECODER #######\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]    \n",
    "    for _ in range(decoder_depth-1):  # DEPTH\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models_gru(src_vocab_size, embedding_matrix, dst_vocab_size=None, embedding_dim=300, latent_dim=128, \n",
    "                       mask_value=0, trainable_embeddings=False, encoder_depth=1, decoder_depth=1):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_size is None:\n",
    "        dst_vocab_size = src_vocab_size\n",
    "        \n",
    "    encoder_inputs = Input(shape=(None,)) # timesteps, features (integer)\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    inputs = [encoder_inputs, decoder_inputs]\n",
    "    \n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    encoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(encoder_masking)\n",
    "    decoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(decoder_masking)\n",
    "    \n",
    "    ######## ENCODER ########\n",
    "    encoder_gru = GRU(latent_dim, return_state=True, return_sequences=True)\n",
    "    encoder_outputs, state_h = encoder_gru(encoder_embedding)\n",
    "    encoder_states = [state_h]\n",
    "    for _ in range(encoder_depth-1):  # DEPTH\n",
    "        encoder_outputs, state_h = encoder_gru(encoder_outputs)\n",
    "        encoder_states = [state_h]\n",
    "    \n",
    "    ######## DECODER ########\n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_gru = GRU(units=latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=encoder_states)\n",
    "    for _ in range(decoder_depth-1):  # DEPTH\n",
    "        decoder_outputs, _ = decoder_gru(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "    \n",
    "    decoder_dense = Dense(dst_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "    model = Model(inputs, decoder_outputs)\n",
    "    \n",
    "    ####### INFERENCE ENCODER #######\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    ####### INFERENCE DECODER #######\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h]\n",
    "    \n",
    "    decoder_outputs, state_h = decoder_gru(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h]    \n",
    "    for _ in range(decoder_depth-1):  # DEPTH\n",
    "        decoder_outputs, state_h = decoder_gru(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "        decoder_states = [state_h]\n",
    "    \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, val_loss, color=None, fname=None, legend=False):\n",
    "        N = len(loss)\n",
    "        train_loss_plt, = plt.plot(range(0, N), loss)\n",
    "        val_loss_plt, = plt.plot(range(0, N), val_loss)\n",
    "        \n",
    "        if color is not None:\n",
    "            train_loss_plt.plt.setp(lines, color=color, linestyle='-')\n",
    "            val_loss_plt.plt.setp(lines, color=color, linestyle='--')\n",
    "            \n",
    "        if legend:\n",
    "            plt.legend((train_loss_plt, val_loss_plt), ('train loss', 'val loss'))\n",
    "        \n",
    "        if fname is not None:\n",
    "            plt.savefig(fname)\n",
    "        \n",
    "        return [train_loss_plt, val_loss_plt]\n",
    "\n",
    "def plot(losses, fname=None):        \n",
    "    lines = []\n",
    "    names = []\n",
    "    colors = [plt.cm.gist_ncar(i) for i in np.linspace(0, 1, len(losses))]\n",
    "    for i, (loss, val_loss) in enumerate(losses):\n",
    "        lines.extend(plot_loss(loss, val_loss, colors[i]))\n",
    "        names.extend(['{} loss'.format(i+1), '{} val loss'.format(i+1)])\n",
    "    \n",
    "    plt.legend(lines, names)\n",
    "    \n",
    "    if fname is not None:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_model(model, train_samples, batch_size, epochs=10, shuffle=True, n_folds=None, train_split=None, model_name=None):\n",
    "    assert not (n_folds is not None and train_split is not None), ValueError('Either n_folds or train_split should be specified, but not both.')\n",
    "    assert not (n_folds is None and train_split is None), ValueError('Either n_folds or train_split must be specified.')   \n",
    "    \n",
    "    def _run_model(train, val, model_file):\n",
    "        train_generator = BatchGenerator(train, batch_size=batch_size, vocab_size=vocab_size, reverse_input=True, shuffle=shuffle)\n",
    "        val_generator = BatchGenerator(val, batch_size=batch_size, vocab_size=vocab_size, reverse_input=True, shuffle=shuffle)\n",
    "        \n",
    "        # utils callbacks\n",
    "        checkpointer = ModelCheckpoint(filepath=model_file, verbose=1, save_best_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, mode='auto', \n",
    "                                      min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "        early_stop = EarlyStopping(patience=1, min_delta=0.0001, verbose=1)\n",
    "        callbacks = [checkpointer, reduce_lr, early_stop]\n",
    "        \n",
    "        # actual train\n",
    "        history = model.fit_generator(train_generator.generate_batch(), steps_per_epoch=len(train)//batch_size, epochs=epochs, \n",
    "                            validation_data=val_generator.generate_batch(), validation_steps=len(val)//batch_size,\n",
    "                            callbacks=callbacks)\n",
    "        \n",
    "        # plot current losses\n",
    "        plot([(history.history['loss'], history.history['val_loss'])], fname=model_name + '.png')\n",
    "        \n",
    "    train_samples = np.array(train_samples)\n",
    "    losses = []  # keep track of train and val loss for each fold\n",
    "    \n",
    "    if n_folds is None:\n",
    "        train, val = train_test_split(train_samples, train_size=train_split, shuffle=shuffle)\n",
    "        model_file = model_name + '.h5'\n",
    "        \n",
    "        history = _run_model(train, val, model_file=model_file)\n",
    "    else:  \n",
    "        kfold = KFold(n_folds, shuffle=shuffle)\n",
    "        for i, (train, val) in enumerate(kfold.split(train_samples)):\n",
    "            print(\"Running fold {}/{}\".format(i+1, n_folds))\n",
    "\n",
    "            model_file = model_name + '-fold-{}.h5'.format(i+1)\n",
    "            history = _run_model(train_samples[train], train_samples[val], model_file=model_file)\n",
    "\n",
    "            # record losses for the final plot\n",
    "            losses.append((history.history['loss'], history.history['val_loss']))\n",
    "\n",
    "        # plot losses for all folds\n",
    "        plot(losses, model_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'basic_seq2seq_20k_200_300d_1-1_LSTM'\n",
    "model_file = model_name + '.h5'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_lstm(src_vocab_size=vocab_size, latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=1, decoder_depth=1, trainable_embeddings=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_84 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_85 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_49 (Masking)            (None, None)         0           input_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_50 (Masking)            (None, None)         0           input_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_49 (Embedding)        (None, None, 300)    6000900     masking_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_50 (Embedding)        (None, None, 300)    6000900     masking_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  [(None, None, 300),  721200      embedding_49[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  [(None, None, 300),  721200      embedding_50[0][0]               \n",
      "                                                                 lstm_24[0][1]                    \n",
      "                                                                 lstm_24[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, None, 20003)  6020903     lstm_25[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,465,103\n",
      "Trainable params: 7,463,303\n",
      "Non-trainable params: 12,001,800\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "masking_49 (Masking)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_49 (Embedding)     (None, None, 300)         6000900   \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               [(None, None, 300), (None 721200    \n",
      "=================================================================\n",
      "Total params: 6,722,100\n",
      "Trainable params: 721,200\n",
      "Non-trainable params: 6,000,900\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_85 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_50 (Masking)            (None, None)         0           input_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_50 (Embedding)        (None, None, 300)    6000900     masking_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_86 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_87 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  [(None, None, 300),  721200      embedding_50[0][0]               \n",
      "                                                                 input_86[0][0]                   \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, None, 20003)  6020903     lstm_25[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,743,003\n",
      "Trainable params: 6,742,103\n",
      "Non-trainable params: 6,000,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "encinf.summary()\n",
    "decinf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 132060\n",
      "Test samples: 1334\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = create_samples(data, max_seq_length=200, test_split=0.01)\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/sw/python-3.5.2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3259/3920 [=======================>......] - ETA: 16:42 - loss: 0.8847 - categorical_accuracy: 0.0319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3920/3920 [==============================] - 6111s 2s/step - loss: 0.8682 - categorical_accuracy: 0.0332 - val_loss: 0.7862 - val_categorical_accuracy: 0.0400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78624, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/keras/engine/topology.py:2379: UserWarning: Layer lstm_25 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_24/while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'lstm_24/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "3920/3920 [==============================] - 6139s 2s/step - loss: 0.7569 - categorical_accuracy: 0.0451 - val_loss: 0.7433 - val_categorical_accuracy: 0.0484\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78624 to 0.74331, saving model to basic_seq2seq_20k_200_300d_1-1_LSTM.h5\n",
      "Epoch 3/5\n",
      "1266/3920 [========>.....................] - ETA: 1:06:36 - loss: 0.7269 - categorical_accuracy: 0.0504"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "train_model(model, train_samples, batch_size=batch_size, epochs=epochs, \n",
    "            train_split=0.95, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encinf.save(model_name + '_encinf.h5')\n",
    "decinf.save(model_name + '_decinf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(input_text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([preprocess(input_text)])[0]\n",
    "\n",
    "def decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "    print(states_value)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0, 0] = eos_index\n",
    "    \n",
    "    decoder_inputs = [target_seq, states_value]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 1 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output, h = decinf.predict(decoder_inputs)\n",
    "        sampled_word_index = np.argmax(output[0, -1, :])  # batch, time, features\n",
    "        sampled_word = index_word[sampled_word_index]\n",
    "\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if sampled_word == eos_token or i > max_output_len:\n",
    "            stop_condition = True     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_word_index\n",
    "        i += 1\n",
    "        \n",
    "        # Update states\n",
    "        decoder_inputs = [target_seq, h]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = \"\"\"When you enter the room, you can only see a pair of red spots on the other end. Before you can\n",
    "make sense of them, they're gone. East.\"\"\"\n",
    "input_seq = prepare_input(test_line, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence encoded\n",
      "[[0.05334514 0.04006272 0.04207818 ... 0.05511576 0.05311017 0.04898664]\n",
      " [0.03029856 0.04405143 0.02400631 ... 0.03485937 0.03231867 0.02948697]\n",
      " [0.05602306 0.03670164 0.04189364 ... 0.05776625 0.05400322 0.06077509]\n",
      " ...\n",
      " [0.0461299  0.03069659 0.03379175 ... 0.04781977 0.04690003 0.04870632]\n",
      " [0.04026349 0.04083458 0.02838288 ... 0.03982588 0.03837633 0.03897776]\n",
      " [0.05105275 0.04323575 0.03928686 ... 0.07083659 0.06055689 0.04348134]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'story credentials halted ruminate ironwood acres entertaining rumours peyton ashtray indignantly assessment motorbikes curb heated eucalypt misgivings drowsy teller egress davies blowgun juffannazlo flinches piano knapsack origins clenched daunting flank commissar injuries erratically retreats leveritt ranks receptionist pauper elementary ross prunes dollars gnats expired attached streak malcolm unbecoming turquoise booby white '"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights('basic_seq2seq2.h5')\n",
    "# decinf.summary(line_length=100)\n",
    "decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decoder_gru(encinf, decinf, input_seq, k, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0,0] = eos_index\n",
    "    \n",
    "    decoder_inputs = [target_seq, states_value]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''    \n",
    "    decoded_sequences = []\n",
    "    \n",
    "    # Init Beam Array\n",
    "    beams = [[target_seq, states_value, 1.0]]\n",
    "    \n",
    "    output_len = 1 # number of sampled words\n",
    "    while not stop_condition:        \n",
    "        all_candidates = []\n",
    "        for i in range(len(beams)):\n",
    "            # Get the values of the beam\n",
    "            target_seq, h, score = beams[i]\n",
    "            decoder_inputs = [target_seq, h]\n",
    "            \n",
    "            # predict the next word and create candidate list\n",
    "            output, h = decinf.predict(decoder_inputs)            \n",
    "            candidate_score_list = output[0][0]\n",
    "            \n",
    "            for j in range(len(candidate_score_list)):                \n",
    "                # Add each candidate to the target sequence.\n",
    "                candidate_seq = np.insert(target_seq, [1], j, axis=1)\n",
    "                candidate_beam = [candidate_seq, h,  score*-log(candidate_score_list[j])]\n",
    "                all_candidates.append(candidate_beam)\n",
    "\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[2])\n",
    "        # select k best\n",
    "        beams = ordered[:k]\n",
    "        \n",
    "        # if a beam has the index of the eos, \n",
    "        # 1. Add the beam to decoded_sequences, \n",
    "        # 2. Remove it from beams \n",
    "        # 3. Lower number of beams (k)\n",
    "        for bindex, beam in enumerate(beams):\n",
    "            if beam[0][0][-1] == eos_index:\n",
    "                decoded_sequences.append(beam)\n",
    "                del beams[bindex]\n",
    "                k -= 1\n",
    "                   \n",
    "        # Exit condition: either each beam got eos or hit max length\n",
    "        if len(beams) == 0 or output_len > max_output_len:\n",
    "            # Add the beams that did not have eos so far to the decoded_sequences\n",
    "            decoded_sequences += beams\n",
    "            stop_condition = True \n",
    "\n",
    "        output_len += 1\n",
    "        \n",
    "    decoded_sentences = [' '.join([index_word[int(wi)-1] \n",
    "                                   for wi in seq[0]]) \n",
    "                         for seq, state, score in decoded_sequences]\n",
    "\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decoder_lstm(encinf, decinf, input_seq, k, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0,0] = eos_index\n",
    "    \n",
    "    decoder_inputs = [target_seq, states_value]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''    \n",
    "    decoded_sequences = []\n",
    "    \n",
    "    # Init Beam Array\n",
    "    beams = [[target_seq, states_value, 1.0]]\n",
    "    \n",
    "    output_len = 1 # number of sampled words\n",
    "    while not stop_condition:        \n",
    "        all_candidates = []\n",
    "        for i in range(len(beams)):\n",
    "            # Get the values of the beam\n",
    "            target_seq, states, score = beams[i]\n",
    "            decoder_inputs = [target_seq] + states\n",
    "            \n",
    "            # predict the next word and create candidate list\n",
    "            output, h, c = decinf.predict(decoder_inputs)            \n",
    "            candidate_score_list = output[0][0]\n",
    "            \n",
    "            for j in range(len(candidate_score_list)):                \n",
    "                # Add each candidate to the target sequence.\n",
    "                candidate_seq = np.insert(target_seq, [1], j, axis=1)\n",
    "                candidate_beam = [candidate_seq, [h, c],  score*-log(candidate_score_list[j])]\n",
    "                all_candidates.append(candidate_beam)\n",
    "\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[2])\n",
    "        # select k best\n",
    "        beams = ordered[:k]\n",
    "        \n",
    "        # if a beam has the index of the eos, \n",
    "        # 1. Add the beam to decoded_sequences, \n",
    "        # 2. Remove it from beams \n",
    "        # 3. Lower number of beams (k)\n",
    "        for bindex, beam in enumerate(beams):\n",
    "            if beam[0][0][-1] == eos_index:\n",
    "                decoded_sequences.append(beam)\n",
    "                del beams[bindex]\n",
    "                k -= 1\n",
    "                   \n",
    "        # Exit condition: either each beam got eos or hit max length\n",
    "        if len(beams) == 0 or output_len > max_output_len:\n",
    "            # Add the beams that did not have eos so far to the decoded_sequences\n",
    "            decoded_sequences += beams\n",
    "            stop_condition = True \n",
    "\n",
    "        output_len += 1\n",
    "        \n",
    "    decoded_sentences = [' '.join([index_word[int(wi)-1] \n",
    "                                   for wi in seq[0]]) \n",
    "                         for seq, state, score in decoded_sequences]\n",
    "\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
