{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the followings:\n",
    "* http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "* http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "* https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://github.com/farizrahman4u/seq2seq\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "## TODO\n",
    "* ~~look into categorical representation~~\n",
    "* ~~look into the number of missing words over the total~~\n",
    "* look into different training data generators (e.g. simple sentence2sentence)\n",
    "* look into different models (attention, hierachical, etc.)\n",
    "* look into character-level representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruc0029/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from string import punctuation\n",
    "from itertools import islice\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, LSTM, GRU, Dense, Masking, Embedding, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "OUTPUT_PATH = 'output'\n",
    "punct = set(punctuation)\n",
    "file_list = sorted(glob.glob('data/parsed/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'GoogleNews-vectors-negative300.bin.gz'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data_20k.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5f46202f10a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# re-load params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_20k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'params_20k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparams_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/data_20k.pkl'"
     ]
    }
   ],
   "source": [
    "# re-load params\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'rb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'rb') as params_file:\n",
    "    data = pkl.load(data_file)\n",
    "    params = pkl.load(params_file)\n",
    "    tokenizer = params['tokenizer']\n",
    "    index_word = params['index_word']\n",
    "    word2embeddings = params['w2e']\n",
    "    embedding_matrix = params['W']\n",
    "    missing_words = params['missing_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300#w2v.vector_size\n",
    "eos_token = 'EOS'\n",
    "unk_token = 'UNK'\n",
    "eos_vector = np.ones((embedding_dim))\n",
    "unk_vector = np.zeros((embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(repl='', string=text, pattern='^> ') # remove starting caret, if any\n",
    "    text = re.sub(repl='\\g<1> \\g<2>', string=text, pattern='(\\w+)-(\\w+)') # compound words    \n",
    "    text = re.sub(repl=' ', string=text, pattern='-{2,}|\\s{2,}|[%s\\t\\n/]' % (''.join(punctuation)))\n",
    "#     text = re.sub(repl=' digits ', string=text, pattern='^\\d+$| \\d+| \\d+ ') # replace digits with a standard 'digits' word\n",
    "    return text\n",
    "\n",
    "def read_corpus(file_list):\n",
    "    corpus = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            print('read_corpus: processing [{}]'.format(file))\n",
    "            corpus.append(f.read())\n",
    "            \n",
    "    return corpus\n",
    "            \n",
    "def build_vocabulary(corpus, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words=num_words+1, oov_token=oov_token) # +1 for the oov token\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Fix keras' nasty behaviour. See https://github.com/keras-team/keras/issues/8092\n",
    "    # Only include words found in w2v\n",
    "    tokenizer.word_index = {w:i for  w,i in tokenizer.word_index.items() \n",
    "                                if   i <= num_words} # <= because tokenizer is 1 indexed\n",
    "    tokenizer.num_words = num_words + 1\n",
    "    tokenizer.word_index[tokenizer.oov_token] = len(tokenizer.word_index) + 1   \n",
    "    index_word = [None for i in range(len(tokenizer.word_index)+1)]\n",
    "    for w,i in tokenizer.word_index.items():\n",
    "        index_word[i] = w\n",
    "        \n",
    "    return tokenizer, index_word\n",
    "\n",
    "def prepare_data(corpus, tokenizer):\n",
    "    # Still go through the files line by line, as we want to predict the next scene, \n",
    "    # not just the next sentence\n",
    "    data = []\n",
    "    for i, doc in enumerate(corpus):\n",
    "        doc_data = []\n",
    "        print('prepare_data: processing [{}]'.format(file_list[i]))\n",
    "        \n",
    "        for j, line in enumerate(doc.split('\\n')):\n",
    "            if len(line) == 0:\n",
    "                print('Line {} is empty. Replacing with \"empty line\".'.format(j+1))\n",
    "                line = 'empty line'\n",
    "\n",
    "            doc_data.append(tokenizer.texts_to_sequences([line])[0])\n",
    "\n",
    "        if len(doc_data) == 0:\n",
    "            print('File {} has no data'.format(file_list[i]))\n",
    "        else:\n",
    "            data.append(doc_data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def get_embeddings(word_index, w2v, unk_vector):\n",
    "    embedding_matrix=np.zeros(shape=(len(word_index)+2, w2v.vector_size))  # +2 as keras' tokenizer is 1-based\n",
    "    missing_words = []\n",
    "    for word,i in word_index.items():\n",
    "        if word not in w2v:\n",
    "            # Try to capitalize it\n",
    "            if word.capitalize() not in w2v:\n",
    "                missing_words.append(word)\n",
    "                embedding_matrix[i] = unk_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = w2v[word.capitalize()]\n",
    "        else:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "    \n",
    "    # add <EOS> token\n",
    "    embedding_matrix[-1] = eos_vector # keras' index the vocab starting from 1\n",
    "    return embedding_matrix, missing_words\n",
    "\n",
    "def get_embedding_matrix(word2embeddings):\n",
    "    embedding_dim = len(list(word2embeddings.values())[0])\n",
    "    embedding_matrix = np.zeros(shape=(len(word2embeddings)+2, embedding_dim)) # +2 as keras tokenizer is 1-based\n",
    "    for i, w in enumerate(word2embeddings): # keras' tokenizer index is 1-based\n",
    "        embedding_matrix[i+1] = word2embeddings[w]\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_corpus: processing [data/parsed/parsed-12heads.txt]\n",
      "read_corpus: processing [data/parsed/parsed-1893.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "read_corpus: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "read_corpus: processing [data/parsed/parsed-abno.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-acitw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-actofmurder.txt]\n",
      "read_corpus: processing [data/parsed/parsed-adverbum.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afdfr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-afflicted.txt]\n",
      "read_corpus: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "read_corpus: processing [data/parsed/parsed-aotearoa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-awakening.txt]\n",
      "read_corpus: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bellwater.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bestman.txt]\n",
      "read_corpus: processing [data/parsed/parsed-blindhouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bonaventure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bookvol.txt]\n",
      "read_corpus: processing [data/parsed/parsed-broadsides.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bryant.txt]\n",
      "read_corpus: processing [data/parsed/parsed-bse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-buddha.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cacophony.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "read_corpus: processing [data/parsed/parsed-childsplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-chineseroom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cove.txt]\n",
      "read_corpus: processing [data/parsed/parsed-crescent.txt]\n",
      "read_corpus: processing [data/parsed/parsed-csbb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-cull.txt]\n",
      "read_corpus: processing [data/parsed/parsed-death.txt]\n",
      "read_corpus: processing [data/parsed/parsed-defra.txt]\n",
      "read_corpus: processing [data/parsed/parsed-degeneracy.txt]\n",
      "read_corpus: processing [data/parsed/parsed-demoparty.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "read_corpus: processing [data/parsed/parsed-divis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-djinni.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-dualtransform.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eas2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eatme.txt]\n",
      "read_corpus: processing [data/parsed/parsed-edifice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-electric.txt]\n",
      "read_corpus: processing [data/parsed/parsed-elysium.txt]\n",
      "read_corpus: processing [data/parsed/parsed-envcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-eurydice.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodydies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "read_corpus: processing [data/parsed/parsed-finetuned.txt]\n",
      "read_corpus: processing [data/parsed/parsed-firebird.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-floatpoint.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foofoo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-forachange.txt]\n",
      "read_corpus: processing [data/parsed/parsed-foth.txt]\n",
      "read_corpus: processing [data/parsed/parsed-fragileshells.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-galatea.txt]\n",
      "read_corpus: processing [data/parsed/parsed-gdc09.txt]\n",
      "read_corpus: processing [data/parsed/parsed-glowgrass.txt]\n",
      "read_corpus: processing [data/parsed/parsed-goldilocks.txt]\n",
      "read_corpus: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ground.txt]\n",
      "read_corpus: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-halothane.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hamper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-heroes.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hoosegow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1701.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1702.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1703.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic1704.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-indigo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-inls.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp11.txt]\n",
      "read_corpus: processing [data/parsed/parsed-introcomp2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-invisargo.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacket4.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jacqissick.txt]\n",
      "read_corpus: processing [data/parsed/parsed-jfw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ka.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lethe.txt]\n",
      "read_corpus: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lmwh.txt]\n",
      "read_corpus: processing [data/parsed/parsed-loose.txt]\n",
      "read_corpus: processing [data/parsed/parsed-lostpig.txt]\n",
      "read_corpus: processing [data/parsed/parsed-luminous.txt]\n",
      "read_corpus: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "read_corpus: processing [data/parsed/parsed-marika.txt]\n",
      "read_corpus: processing [data/parsed/parsed-measure.txt]\n",
      "read_corpus: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mingsheng.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mite.txt]\n",
      "read_corpus: processing [data/parsed/parsed-monkfish.txt]\n",
      "read_corpus: processing [data/parsed/parsed-moonlittower.txt]\n",
      "read_corpus: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newernewyear.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "read_corpus: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nightfall2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-nordandbert.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oad.txt]\n",
      "read_corpus: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "read_corpus: processing [data/parsed/parsed-onehalf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-orevore.txt]\n",
      "read_corpus: processing [data/parsed/parsed-park.txt]\n",
      "read_corpus: processing [data/parsed/parsed-partyfoul.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_corpus: processing [data/parsed/parsed-pathway.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pax2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pepper.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "read_corpus: processing [data/parsed/parsed-photograph.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "read_corpus: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "read_corpus: processing [data/parsed/parsed-progressive1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-punkpoints.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rameses.txt]\n",
      "read_corpus: processing [data/parsed/parsed-recluse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-represso.txt]\n",
      "read_corpus: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "read_corpus: processing [data/parsed/parsed-robot.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rogue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "read_corpus: processing [data/parsed/parsed-rover.txt]\n",
      "read_corpus: processing [data/parsed/parsed-samfortune.txt]\n",
      "read_corpus: processing [data/parsed/parsed-santaland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "read_corpus: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-scavenger.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sequitur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "read_corpus: processing [data/parsed/parsed-shelter.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sherbet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-simplethefts.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "read_corpus: processing [data/parsed/parsed-six.txt]\n",
      "read_corpus: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "read_corpus: processing [data/parsed/parsed-snacktime.txt]\n",
      "read_corpus: processing [data/parsed/parsed-softfood.txt]\n",
      "read_corpus: processing [data/parsed/parsed-sorcerer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spring.txt]\n",
      "read_corpus: processing [data/parsed/parsed-spur.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssi.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ssos.txt]\n",
      "read_corpus: processing [data/parsed/parsed-starborn.txt]\n",
      "read_corpus: processing [data/parsed/parsed-statue.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "read_corpus: processing [data/parsed/parsed-stf.txt]\n",
      "read_corpus: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suspended.txt]\n",
      "read_corpus: processing [data/parsed/parsed-suvehnux.txt]\n",
      "read_corpus: processing [data/parsed/parsed-swigian.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tacofiction.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tangle2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tapestry.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theone.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theoracle.txt]\n",
      "read_corpus: processing [data/parsed/parsed-theplay.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc1.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thohc2.txt]\n",
      "read_corpus: processing [data/parsed/parsed-thread.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "read_corpus: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "read_corpus: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "read_corpus: processing [data/parsed/parsed-tryst.txt]\n",
      "read_corpus: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unclezeb.txt]\n",
      "read_corpus: processing [data/parsed/parsed-undertow.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unipool.txt]\n",
      "read_corpus: processing [data/parsed/parsed-unscientific.txt]\n",
      "read_corpus: processing [data/parsed/parsed-vagueness.txt]\n",
      "read_corpus: processing [data/parsed/parsed-varkana.txt]\n",
      "read_corpus: processing [data/parsed/parsed-violet.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wand.txt]\n",
      "read_corpus: processing [data/parsed/parsed-weapon.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wedding.txt]\n",
      "read_corpus: processing [data/parsed/parsed-windjack.txt]\n",
      "read_corpus: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wishbringer.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wizard.txt]\n",
      "read_corpus: processing [data/parsed/parsed-wof-sa.txt]\n",
      "read_corpus: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "read_corpus: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yakshaving.txt]\n",
      "read_corpus: processing [data/parsed/parsed-yetifail.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "read_corpus: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(file_list)\n",
    "tokenizer, index_word = build_vocabulary(corpus, num_words=20000, oov_token=unk_token)\n",
    "embedding_matrix, missing_words = get_embeddings(tokenizer.word_index, w2v, unk_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20001\n",
      "OOV token index: 20001\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', tokenizer.num_words)\n",
    "print('OOV token index:', tokenizer.word_index[unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size: (20003, 300)\n",
      "Total triples: 131807\n",
      "Unique words found (<UNK>, <EOS> + vocab): 20002\n",
      "Of which missing words (no embeddings): 1557\n"
     ]
    }
   ],
   "source": [
    "# text = 'Sample sentence with a possible balabiut token and some 1984 plus sentry'\n",
    "# print(preprocess(text))\n",
    "# print(prepare_input(text, tokenizer))\n",
    "vocab_size = len(embedding_matrix)\n",
    "print('Embedding matrix size:', embedding_matrix.shape)\n",
    "print('Total triples:', sum([(len(f)-3)//2 for f in data]))\n",
    "print('Unique words found (<UNK>, <EOS> + vocab):', len(tokenizer.word_index)+1)\n",
    "print('Of which missing words (no embeddings):', len(missing_words))\n",
    "# print('Corpus size:', corpus_size)\n",
    "# missing_words[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-12heads.txt]\n",
      "prepare_data: processing [data/parsed/parsed-1893.txt]\n",
      "Line 1197 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-20160221-thesueno.txt]\n",
      "Line 1445 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-3card-deadmanshill-2016Ap24.txt]\n",
      "prepare_data: processing [data/parsed/parsed-69krakatoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-905-shrapnel.txt]\n",
      "prepare_data: processing [data/parsed/parsed-abno.txt]\n",
      "Line 1217 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acg-crossbow.txt]\n",
      "Line 1825 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-acitw.txt]\n",
      "prepare_data: processing [data/parsed/parsed-actofmurder.txt]\n",
      "Line 567 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-adverbum.txt]\n",
      "Line 1085 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-afdfr.txt]\n",
      "prepare_data: processing [data/parsed/parsed-afflicted.txt]\n",
      "prepare_data: processing [data/parsed/parsed-allthingsdevours.txt]\n",
      "prepare_data: processing [data/parsed/parsed-aotearoa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-awakening.txt]\n",
      "prepare_data: processing [data/parsed/parsed-beingandrewplotkin.txt]\n",
      "Line 833 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bellwater.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bestman.txt]\n",
      "Line 1509 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-blindhouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bonaventure.txt]\n",
      "Line 759 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bookvol.txt]\n",
      "Line 393 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-broadsides.txt]\n",
      "prepare_data: processing [data/parsed/parsed-bryant.txt]\n",
      "Line 995 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-bse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-buddha.txt]\n",
      "Line 661 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-cacophony.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cc-fangvclaw-flooby.txt]\n",
      "prepare_data: processing [data/parsed/parsed-chefjanitor.txt]\n",
      "Line 871 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-childsplay.txt]\n",
      "Line 2191 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-chineseroom.txt]\n",
      "prepare_data: processing [data/parsed/parsed-clipperbeta.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cokeandspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cove.txt]\n",
      "prepare_data: processing [data/parsed/parsed-crescent.txt]\n",
      "prepare_data: processing [data/parsed/parsed-csbb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-cull.txt]\n",
      "Line 2031 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-death.txt]\n",
      "prepare_data: processing [data/parsed/parsed-defra.txt]\n",
      "prepare_data: processing [data/parsed/parsed-degeneracy.txt]\n",
      "prepare_data: processing [data/parsed/parsed-demoparty.txt]\n",
      "Line 665 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-dialcforcupcakes-103014.txt]\n",
      "prepare_data: processing [data/parsed/parsed-divis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-djinni.txt]\n",
      "Line 891 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-dramaqueen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-dualtransform.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eas.txt]\n",
      "Line 363 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-eas2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eatme.txt]\n",
      "prepare_data: processing [data/parsed/parsed-edifice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-electric.txt]\n",
      "prepare_data: processing [data/parsed/parsed-elysium.txt]\n",
      "prepare_data: processing [data/parsed/parsed-envcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-eurydice.txt]\n",
      "prepare_data: processing [data/parsed/parsed-everybodydies.txt]\n",
      "Line 459 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-everybodylovesaparade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fdb-tin-folkar.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fear.txt]\n",
      "Line 1897 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fifteenminutes-100214.txt]\n",
      "prepare_data: processing [data/parsed/parsed-finalexam20160124.txt]\n",
      "Line 1723 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-finetuned.txt]\n",
      "prepare_data: processing [data/parsed/parsed-firebird.txt]\n",
      "Line 1877 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-fish.txt]\n",
      "Line 2983 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-floatpoint.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foofoo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-forachange.txt]\n",
      "prepare_data: processing [data/parsed/parsed-foth.txt]\n",
      "prepare_data: processing [data/parsed/parsed-fragileshells.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ft-n-awe.txt]\n",
      "Line 1875 is empty. Replacing with \"empty line\".\n",
      "Line 2021 is empty. Replacing with \"empty line\".\n",
      "Line 2325 is empty. Replacing with \"empty line\".\n",
      "Line 2545 is empty. Replacing with \"empty line\".\n",
      "Line 3249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-galatea.txt]\n",
      "prepare_data: processing [data/parsed/parsed-gdc09.txt]\n",
      "prepare_data: processing [data/parsed/parsed-glowgrass.txt]\n",
      "prepare_data: processing [data/parsed/parsed-goldilocks.txt]\n",
      "prepare_data: processing [data/parsed/parsed-groovebillygoat.txt]\n",
      "Line 675 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ground.txt]\n",
      "prepare_data: processing [data/parsed/parsed-guesstheverb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-halothane.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hamper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-heroes.txt]\n",
      "Line 2197 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2-utf8.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hollywoodvisionary-part2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hoosegow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-houseofdreamofmoon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-hunterindarkness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1701.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1702.txt]\n",
      "Line 135 is empty. Replacing with \"empty line\".\n",
      "Line 139 is empty. Replacing with \"empty line\".\n",
      "Line 165 is empty. Replacing with \"empty line\".\n",
      "Line 201 is empty. Replacing with \"empty line\".\n",
      "Line 205 is empty. Replacing with \"empty line\".\n",
      "Line 209 is empty. Replacing with \"empty line\".\n",
      "Line 213 is empty. Replacing with \"empty line\".\n",
      "Line 217 is empty. Replacing with \"empty line\".\n",
      "Line 221 is empty. Replacing with \"empty line\".\n",
      "Line 225 is empty. Replacing with \"empty line\".\n",
      "Line 229 is empty. Replacing with \"empty line\".\n",
      "Line 233 is empty. Replacing with \"empty line\".\n",
      "Line 237 is empty. Replacing with \"empty line\".\n",
      "Line 241 is empty. Replacing with \"empty line\".\n",
      "Line 249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ic1703.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic1704.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ic2010-1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-indigo.txt]\n",
      "prepare_data: processing [data/parsed/parsed-inls.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp08a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp11.txt]\n",
      "prepare_data: processing [data/parsed/parsed-introcomp2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-invisargo.txt]\n",
      "Line 89 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-jabberwocky.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacket4.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jacqissick.txt]\n",
      "prepare_data: processing [data/parsed/parsed-jfw.txt]\n",
      "Line 27 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ka.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-1May2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-laidoff-subrosa-1May2016.txt]\n",
      "Line 539 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lethe.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data: processing [data/parsed/parsed-littlebluemen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-lmwh.txt]\n",
      "prepare_data: processing [data/parsed/parsed-loose.txt]\n",
      "Line 675 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-lostpig.txt]\n",
      "prepare_data: processing [data/parsed/parsed-luminous.txt]\n",
      "prepare_data: processing [data/parsed/parsed-maincourse-iamthelaw.txt]\n",
      "Line 843 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-marika.txt]\n",
      "prepare_data: processing [data/parsed/parsed-measure.txt]\n",
      "prepare_data: processing [data/parsed/parsed-metamorphoses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mingsheng.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mite.txt]\n",
      "Line 495 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-monkfish.txt]\n",
      "Line 1249 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-moonlittower.txt]\n",
      "prepare_data: processing [data/parsed/parsed-mugglestudies.txt]\n",
      "Line 249 is empty. Replacing with \"empty line\".\n",
      "Line 385 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-newernewyear.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan16b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed-jan9.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed.txt]\n",
      "prepare_data: processing [data/parsed/parsed-newyearsspeed08.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nightfall2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-nordandbert.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oad.txt]\n",
      "prepare_data: processing [data/parsed/parsed-oneeyeopen.txt]\n",
      "prepare_data: processing [data/parsed/parsed-onehalf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-orevore.txt]\n",
      "prepare_data: processing [data/parsed/parsed-park.txt]\n",
      "Line 791 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-partyfoul.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pathway.txt]\n",
      "Line 411 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pax.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pax2011.txt]\n",
      "Line 1563 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-pepper.txt]\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txr.txt]\n",
      "Line 1057 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-photograph.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plan6-waker.txt]\n",
      "prepare_data: processing [data/parsed/parsed-plunderedhearts.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-pnnsi2.txt]\n",
      "Line 1727 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-primrose-edited.txt]\n",
      "Line 1091 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-progressive1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-punkpoints.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rameses.txt]\n",
      "prepare_data: processing [data/parsed/parsed-recluse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-represso.txt]\n",
      "Line 1485 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-revolution-buny.txt]\n",
      "prepare_data: processing [data/parsed/parsed-robot.txt]\n",
      "Line 581 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-rogue.txt]\n",
      "Line 1007 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-roofed-alien.txt]\n",
      "prepare_data: processing [data/parsed/parsed-rover.txt]\n",
      "Line 1341 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-samfortune.txt]\n",
      "prepare_data: processing [data/parsed/parsed-santaland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-a.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-b.txt]\n",
      "prepare_data: processing [data/parsed/parsed-saugusnet-c.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scaryhouseamulet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-scavenger.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sequitur.txt]\n",
      "prepare_data: processing [data/parsed/parsed-shadowsonthemirror.txt]\n",
      "Line 661 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-shelter.txt]\n",
      "Line 1353 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sherbet.txt]\n",
      "Line 4491 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-simplethefts.txt]\n",
      "Line 791 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-sinsagainstmimesis.txt]\n",
      "prepare_data: processing [data/parsed/parsed-six.txt]\n",
      "Line 887 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-smittenkittens.txt]\n",
      "prepare_data: processing [data/parsed/parsed-snacktime.txt]\n",
      "Line 413 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-softfood.txt]\n",
      "prepare_data: processing [data/parsed/parsed-sorcerer.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spring.txt]\n",
      "prepare_data: processing [data/parsed/parsed-spur.txt]\n",
      "Line 2639 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-ssi.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ssos.txt]\n",
      "prepare_data: processing [data/parsed/parsed-starborn.txt]\n",
      "Line 103 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-statue.txt]\n",
      "Line 503 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-stewie-escapade.txt]\n",
      "prepare_data: processing [data/parsed/parsed-stf.txt]\n",
      "prepare_data: processing [data/parsed/parsed-subrosa-1and8may2016.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suspended.txt]\n",
      "prepare_data: processing [data/parsed/parsed-suvehnux.txt]\n",
      "Line 1083 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-swigian.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tacofiction.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tangle.txt]\n",
      "Line 2267 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tangle2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-tapestry.txt]\n",
      "Line 1267 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tdmamoom.txt]\n",
      "Line 1235 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thanksgiving.txt]\n",
      "Line 651 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-themultidimensionalthief.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theone.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theoracle.txt]\n",
      "prepare_data: processing [data/parsed/parsed-theplay.txt]\n",
      "Line 1 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-thohc1.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thohc2.txt]\n",
      "prepare_data: processing [data/parsed/parsed-thread.txt]\n",
      "Line 1153 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tokyo-mouse.txt]\n",
      "prepare_data: processing [data/parsed/parsed-toonesiabandit.txt]\n",
      "Line 939 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-transparent-100914.txt]\n",
      "Line 1421 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-tryst.txt]\n",
      "prepare_data: processing [data/parsed/parsed-turkeyspeeds.txt]\n",
      "Line 1271 is empty. Replacing with \"empty line\".\n",
      "Line 1575 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-unclezeb.txt]\n",
      "prepare_data: processing [data/parsed/parsed-undertow.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unipool.txt]\n",
      "prepare_data: processing [data/parsed/parsed-unscientific.txt]\n",
      "Line 4597 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-vagueness.txt]\n",
      "prepare_data: processing [data/parsed/parsed-varkana.txt]\n",
      "prepare_data: processing [data/parsed/parsed-violet.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wand.txt]\n",
      "prepare_data: processing [data/parsed/parsed-weapon.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wedding.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 4547 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-windjack.txt]\n",
      "prepare_data: processing [data/parsed/parsed-winterwonderland.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wishbringer.txt]\n",
      "Line 651 is empty. Replacing with \"empty line\".\n",
      "prepare_data: processing [data/parsed/parsed-wizard.txt]\n",
      "prepare_data: processing [data/parsed/parsed-wof-sa.txt]\n",
      "prepare_data: processing [data/parsed/parsed-ww-jingo-madrigals.txt]\n",
      "prepare_data: processing [data/parsed/parsed-xyzzy2011.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yakshaving.txt]\n",
      "prepare_data: processing [data/parsed/parsed-yetifail.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork-i-2016-04-0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zork1+troll-2016Ap0310.txt]\n",
      "prepare_data: processing [data/parsed/parsed-zorkII.txt]\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save various objects for later reuse\n",
    "with open(os.path.join(DATA_PATH, 'data_20k.pkl'), 'wb') as data_file, open(os.path.join(DATA_PATH, 'params_20k.pkl'), 'wb') as params_file:\n",
    "    params = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'index_word': index_word,\n",
    "        'W': embedding_matrix,\n",
    "        'w2e': word2embeddings,\n",
    "        'missing_words': missing_words\n",
    "    }\n",
    "    pkl.dump(data, data_file)\n",
    "    pkl.dump(params, params_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(seq, n=3, step=1):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s[0],...s[n-1]), (s[0+skip_n],...,s[n-1+skip_n]), ...   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "\n",
    "    result = result[step:]\n",
    "    for elem in it:\n",
    "        result = result + (elem,)\n",
    "        if len(result) == n:\n",
    "            yield result\n",
    "            result = result[step:]\n",
    "\n",
    "def create_samples(data, test_split=0.1, shuffle=False, max_seq_length=None):    \n",
    "    samples = []\n",
    "    for i, play in enumerate(data):\n",
    "        if max_seq_length is not None:\n",
    "            chunks = [line[offset:offset+max_seq_length] \n",
    "                      for line in play \n",
    "                      for offset in range(0, len(line), max_seq_length)]\n",
    "        else:\n",
    "            chunks = play\n",
    "            \n",
    "        for scene, command, reply in window(chunks, n=3, step=2):\n",
    "#             if max_seq_length is not None:\n",
    "#                 sub_scenes  = [scene[offset:offset+max_seq_length]   for offset in range(0, len(scene),   max_seq_length)]\n",
    "#                 sub_cmds    = [command[offset:offset+max_seq_length] for offset in range(0, len(command), max_seq_length)]\n",
    "#                 sub_replies = [reply[offset:offset+max_seq_length]   for offset in range(0, len(reply),   max_seq_length)]\n",
    "                \n",
    "#                 nb_samples = \n",
    "#                 # sample a number of contextual sequences\n",
    "#                 scenes   = sub_scenes[np.random.choice(range(len(sub_scenes)), len(sub_scenes)//max_seq_length)]\n",
    "#                 commands = sub_cmds[np.random.choice(range(len(sub_cmds)), len(sub_cmds)//max_seq_length)]\n",
    "#                 replies   = sub_replies[np.random.choice(range(len(sub_replies)), len(sub_replies)//max_seq_length)]\n",
    "                \n",
    "                \n",
    "#             if len(command) > 10:\n",
    "#                 command_line = ' '.join([index_word[idx] for idx in command])\n",
    "#                 print('Found anomalous command for play {} [{}] with length {}: [{}]'.format(\n",
    "#                     i, os.path.basename(file_list[i]), len(command), command_line))\n",
    "                \n",
    "            samples.append((scene, command, reply))\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(samples)\n",
    "        \n",
    "    if test_split is not None:\n",
    "        split = int((1-test_split) * len(samples))\n",
    "        train_samples = samples[:split]\n",
    "        test_samples = samples[split:]\n",
    "        return train_samples, test_samples\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_index = tokenizer.word_index[unk_token]\n",
    "eos_index = unk_index+1\n",
    "\n",
    "# Define a batch generator\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, vocab_size, batch_size=1, reverse_input=True, shuffle=True):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.vocab_size = vocab_size\n",
    "        self.reverse_input = reverse_input\n",
    "        self.UNK = unk_index\n",
    "        self.EOS = eos_index\n",
    "        self.PAD = 0\n",
    "        \n",
    "    def generate_batch(self): \n",
    "        # every three lines comprise a sample sequence where the first two items\n",
    "        # are the input and the last one is the output\n",
    "        i  = 1 # batch counter        \n",
    "        x_enc = []\n",
    "        x_dec = []\n",
    "        y  = []\n",
    "            \n",
    "        while True:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.data)\n",
    "            \n",
    "            for j, (scene, command, reply) in enumerate(self.data):\n",
    "                if self.reverse_input:\n",
    "                    scene = scene[::-1]\n",
    "                    \n",
    "                encoder_input  = np.array(scene + command)\n",
    "                decoder_input  = np.array([self.EOS] + reply)\n",
    "                decoder_output = np.array(to_categorical(reply + [self.EOS], self.vocab_size))\n",
    "                    \n",
    "                x_enc.append(encoder_input)\n",
    "                x_dec.append(decoder_input)\n",
    "                y.append(decoder_output)\n",
    "                \n",
    "                if i == self.batch_size or j == len(data):\n",
    "                    if self.batch_size > 1:\n",
    "                        # pad and return the batch\n",
    "                        x_enc = sequence.pad_sequences(x_enc, padding='post', value=self.PAD)\n",
    "                        x_dec = sequence.pad_sequences(x_dec, padding='post', value=self.PAD)    \n",
    "                        y     = sequence.pad_sequences(y, padding='post', value=self.PAD)\n",
    "\n",
    "                    x_out, y_out = [np.array(x_enc), np.array(x_dec)], np.array(y)\n",
    "                    \n",
    "                    i = 1\n",
    "                    x_enc = []\n",
    "                    x_dec = []\n",
    "                    y = []\n",
    "\n",
    "                    yield (x_out, y_out)\n",
    "                else:\n",
    "                    i += 1 # next sample per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models_lstm(src_vocab_size, embedding_matrix, dst_vocab_size=None, embedding_dim=300, latent_dim=128, \n",
    "                       mask_value=0, trainable_embeddings=False, encoder_depth=1, decoder_depth=1):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_size is None:\n",
    "        dst_vocab_size = src_vocab_size\n",
    "        \n",
    "    encoder_inputs = Input(shape=(None,)) # timesteps, features (integer)\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    inputs = [encoder_inputs, decoder_inputs]\n",
    "    \n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    encoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(encoder_masking)\n",
    "    decoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(decoder_masking)\n",
    "    encoder_outputs = encoder_embedding\n",
    "    decoder_outputs = decoder_embedding\n",
    "    \n",
    "    ######## ENCODER ########\n",
    "    for _ in range(encoder_depth):\n",
    "        encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True, return_sequences=True)(encoder_outputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    \n",
    "    ######## DECODER ########\n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_outputs, _, _ = LSTM(units=latent_dim, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    for _ in range(decoder_depth-1):\n",
    "        decoder_outputs, _, _ = LSTM(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "    \n",
    "    decoder_dense = Dense(dst_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "    model = Model(inputs, decoder_outputs)\n",
    "    \n",
    "    ####### INFERENCE ENCODER #######\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    ####### INFERENCE DECODER #######\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_sequences=True, return_state=True)(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    for _ in range(decoder_depth-1):\n",
    "        decoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_outputs = Dense(dst_vocab_size, activation='softmax')(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models_gru(src_vocab_size, embedding_matrix, dst_vocab_size=None, embedding_dim=300, latent_dim=128, \n",
    "                       mask_value=0, trainable_embeddings=False, encoder_depth=1, decoder_depth=1):\n",
    "    # define training encoder. We use return_state to retrieve the hidden states for the encoder and\n",
    "    # provide them as input to the decoder\n",
    "    if dst_vocab_size is None:\n",
    "        dst_vocab_size = src_vocab_size\n",
    "        \n",
    "    encoder_inputs = Input(shape=(None,)) # timesteps, features (integer)\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    inputs = [encoder_inputs, decoder_inputs]\n",
    "    \n",
    "    encoder_masking = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    decoder_masking = Masking(mask_value=mask_value)(decoder_inputs)\n",
    "    \n",
    "    encoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(encoder_masking)\n",
    "    decoder_embedding = Embedding(input_dim=src_vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], \n",
    "                               trainable=trainable_embeddings)(decoder_masking)\n",
    "    encoder_outputs = encoder_embedding\n",
    "    decoder_outputs = decoder_embedding\n",
    "    \n",
    "    ######## ENCODER ########\n",
    "    for _ in range(encoder_depth):\n",
    "        encoder_outputs, state_h = GRU(latent_dim, return_state=True, return_sequences=True)(encoder_outputs)\n",
    "        encoder_states = [state_h]\n",
    "    \n",
    "    ######## DECODER ########\n",
    "    # define training decoder. It is initialized with the encoder hidden states\n",
    "    decoder_outputs, _ = GRU(units=latent_dim, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    for _ in range(decoder_depth-1):\n",
    "        decoder_outputs, _ = GRU(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "    \n",
    "    decoder_dense = Dense(dst_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "    model = Model(inputs, decoder_outputs)\n",
    "    \n",
    "    ####### INFERENCE ENCODER #######\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    ####### INFERENCE DECODER #######\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h]\n",
    "    \n",
    "    decoder_outputs, state_h = GRU(units=latent_dim, return_sequences=True, return_state=True)(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h]\n",
    "    \n",
    "    for _ in range(decoder_depth-1):\n",
    "        decoder_outputs, state_h = GRU(units=latent_dim, return_sequences=True, return_state=True)(decoder_outputs)\n",
    "        decoder_states = [state_h]\n",
    "    \n",
    "    decoder_outputs = Dense(dst_vocab_size, activation='softmax')(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(losses, fname=None):\n",
    "    def _plot(loss, val_loss, color):\n",
    "        N = len(loss)\n",
    "        train_loss_plt, = plt.plot(range(0, N), losses)\n",
    "        val_loss_plt, = plt.plot(range(0, N), val_losses)\n",
    "        \n",
    "        train_loss_plt.plt.setp(lines, color=color, linestyle='-')\n",
    "        val_loss_plt.plt.setp(lines, color=color, linestyle='--')\n",
    "        \n",
    "        return [train_loss_plt, val_loss_plt]\n",
    "        \n",
    "    lines = []\n",
    "    names = []\n",
    "    colors = [plt.cm.gist_ncar(i) for i in np.linspace(0, 1, len(losses))]\n",
    "    for i, (loss, val_loss) in enumerate(losses):\n",
    "        lines.extend(_plot(loss, val_loss, colors[i]))\n",
    "        names.extend(['{} loss'.format(i+1), '{} val loss'.format(i+1)])\n",
    "    \n",
    "    plt.legend(lines, names)\n",
    "    \n",
    "    if fname is not None:\n",
    "        plt.savefig(fname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_model(model, train_samples, batch_size, shuffle=True, n_folds=None, train_split=None, epochs=10, model_name=None):\n",
    "    assert not (n_folds is not None and train_split is not None), ValueError('Either n_folds or train_split should be specified, but not both.')\n",
    "    assert not (n_folds is None and train_split is None), ValueError('Either n_folds or train_split must be specified.')   \n",
    "    \n",
    "    def _run_model(train, val, model_file, plot=True):\n",
    "        train_generator = BatchGenerator(train, batch_size=batch_size, vocab_size=vocab_size, reverse_input=False)\n",
    "        val_generator = BatchGenerator(val, batch_size=batch_size, vocab_size=vocab_size, reverse_input=False)\n",
    "        \n",
    "        # utils callbacks\n",
    "        checkpointer = ModelCheckpoint(filepath=model_file, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, mode='auto', \n",
    "                                      min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "        early_stop = EarlyStopping(patience=1, min_delta=0.0001, verbose=1)\n",
    "        callbacks = [checkpointer, reduce_lr, early_stop]\n",
    "        \n",
    "        # actual train\n",
    "        history = model.fit_generator(train_generator.generate_batch(), steps_per_epoch=len(train)//batch_size, epochs=epochs, \n",
    "                            validation_data=val_generator.generate_batch(), validation_steps=len(val)//batch_size,\n",
    "                            callbacks=callbacks)\n",
    "        \n",
    "        # plot current losses\n",
    "        plot([(history.history['loss'], history.history['val_loss'])], fname=model_name + '.png')\n",
    "        \n",
    "    train_samples = np.array(train_samples)\n",
    "    losses = []  # keep track of train and val loss for each fold\n",
    "    \n",
    "    if n_folds is None:\n",
    "        train, val = train_test_split(train_samples, train_size=train_split, shuffle=shuffle)\n",
    "        model_file = model_name + '.h5'\n",
    "        \n",
    "        history = _run_model(train, val, model_file=model_file, load_model=load_model)\n",
    "    else:  \n",
    "        kfold = KFold(n_folds, shuffle=shuffle)\n",
    "        for i, (train, val) in enumerate(kfold.split(train_samples)):\n",
    "            print(\"Running fold {}/{}\".format(i+1, n_folds))\n",
    "\n",
    "            model_file += '-fold-{}'.format(i+1)\n",
    "            history = _run_model(train_samples[train], train_samples[val], model_file=model_file, load_model=load_model)\n",
    "\n",
    "            # record losses for the final plot\n",
    "            losses.append((history.history['loss'], history.history['val_loss']))\n",
    "\n",
    "        # plot losses for all folds\n",
    "        plot(losses, model_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'basic_seq2seq_20k_200_300d_3-3_GRU'\n",
    "model_file = model_name + '.h5'\n",
    "# create the model\n",
    "model, encinf, decinf = define_models_gru(src_vocab_size=vocab_size, latent_dim=300, embedding_matrix=embedding_matrix,\n",
    "                             encoder_depth=3, decoder_depth=3, trainable_embeddings=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 132060\n",
      "Test samples: 1334\n"
     ]
    }
   ],
   "source": [
    "train_samples, test_samples = create_samples(data, max_seq_length=200, test_split=0.01)\n",
    "print('Train samples:', len(train_samples))\n",
    "print('Test samples:', len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() got an unexpected keyword argument 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e213b6e4cd4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_model(model, train_samples, batch_size=batch_size, epochs=0, \n\u001b[0;32m----> 4\u001b[0;31m             train_split=0.95, model_name=model_name, load_model=True)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'load_model'"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "train_model(model, train_samples, batch_size=batch_size, epochs=0, \n",
    "            train_split=0.95, model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(input_text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([preprocess(input_text)])[0]\n",
    "\n",
    "def decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encinf.predict(input_seq)\n",
    "    print('Sequence encoded')\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))  # time, features\n",
    "    # Populate the first word with the eos index (.\n",
    "    target_seq[0,0] = eos_index\n",
    "    \n",
    "    decoder_inputs = [target_seq, states_value]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 1 # number of sampled words\n",
    "    while not stop_condition:\n",
    "        output, h = decinf.predict(decoder_inputs, batch_size=1)\n",
    "        sampled_word_index = np.argmax(output[0, -1, :])\n",
    "        sampled_word = index_word[sampled_word_index]\n",
    "\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if sampled_word == eos_token or i > max_output_len:\n",
    "            stop_condition = True     \n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_word_index\n",
    "        i += 1\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = \"\"\"you find yourself in a dimly lit room. You can see nothing but a pair or red dots seemingly staring at you. \n",
    "               Before you can realize what they are, they are gone. On the north wall you can spot a glowing panel of some sort.\n",
    "               Examine panel\"\"\"\n",
    "# test_line = \"you are\"\n",
    "input_seq = prepare_input(test_line, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence encoded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle tangle '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights('basic_seq2seq2.h5')\n",
    "# decinf.summary(line_length=100)\n",
    "decode_sequence(encinf, decinf, input_seq, vocab_size, max_output_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
